{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# **Low-resource NLP**\n",
        "\n",
        "<img src=\"https://github.com/stevenkolawole/indaba-low-resource-nlp-prac/blob/c426a3e79c3a06d38562f49d7fc57ec0c751622c/content/lr_llm_header.png?raw=1\" width=\"60%\" allign =\"center\"/>\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2024/blob/main/practicals/Indaba_2024_Prac_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [Change colab link to point to prac.]\n",
        "\n",
        "Â© Deep Learning Indaba 2024. Apache License 2.0.\n",
        "\n",
        "**Authors:**\n",
        "- Ali Zaidi\n",
        "- Aya Salama\n",
        "- Khalil Mrini\n",
        "- Steven Kolawole\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "Low-resource NLP (Natural Language Processing) refers to the study and development of NLP models and systems for languages, tasks, or domains that have limited data and resources available. These can include languages with fewer digital text corpora, limited computational tools, or less-developed linguistic research.\n",
        "\n",
        "**Key Challenges in Low-Resource NLP**\n",
        "\n",
        "1. **Data Scarcity:**\n",
        "   - **Limited Training Data:** Many languages lack large annotated corpora necessary for training NLP models.\n",
        "   - **Lack of Pre-trained Models:** Popular NLP models like BERT, GPT, and others are often not available for low-resource languages.\n",
        "\n",
        "2. **Linguistic Diversity:**\n",
        "   - **Morphological Complexity:** Some languages have complex grammatical structures and morphological richness.\n",
        "   - **Dialectal Variations:** A lack of standardized versions can complicate NLP tasks.\n",
        "\n",
        "3. **Resource Limitations:**\n",
        "   - **Computational Constraints:** Low-resource scenarios often involve limited access to computational power and storage.\n",
        "   - **Expertise and Tools:** Fewer linguistic experts and fewer NLP tools are tailored for these languages.\n",
        "\n",
        "**Topics:**\n",
        "\n",
        "Content: [Natural Language Processing, Large Language Models,Parameter Efficient Finetuning, Adaptation]  \n",
        "Level: [Intermediate]\n",
        "\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "- Exploring data scarcity challenges\n",
        "- Exploring Compute resource limitations\n",
        "- Comparing SOTA LLM Performance on low-resource languages/tasks (depending on which dataset we will end up using )\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "[Knowledge required for this prac. You can link a relevant parallel track session, blogs, papers, courses, topics etc.]\n",
        "\n",
        "**Outline:**\n",
        "\n",
        "[Points that link to each section. Auto-generate following the instructions [here](https://stackoverflow.com/questions/67458990/how-to-automatically-generate-a-table-of-contents-in-colab-notebook).]\n",
        "\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "[Tasks just before starting.]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcvDuWXdveJL"
      },
      "source": [
        "Storyline: working on a task with scarce data which is summarization in Moroccan Darija.\n",
        "this task pose resources constrainst as the Moroccan Darija can be considered a low resource dialect of Arabic, we will set this task in a coumputational resource poor environment so our training should be able to run on a commodity GPU\n",
        "we will be using parameter efficient fine tuning technique (LORA) to optimize the training procedure in order to make it feasible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installation and Imports [should download any needed resources]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vhZZkHAveJM"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install arabert\n",
        "!pip install accelerate -U\n",
        "!pip install transformers[torch]\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQSnu5R9veJM"
      },
      "outputs": [],
      "source": [
        "#download the datset\n",
        "#download the model checkpoints\n",
        "#download the GPT outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qHz3BlWveJM"
      },
      "source": [
        "# Task and Dataset Overiew"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3lptfGKveJN"
      },
      "source": [
        "In this practical we are interested in generating headlines for news articles featured on the news website [Goud.ma](www.Gound.ma).\n",
        "\n",
        "We will frame this as a summarization task where the input is the body of a news article and the output is an appropriate headline. The [Goud dataset](https://github.com/issam9/goud-summarization-dataset) contains 158k articles and their headlines. All headlines are in Moroccan Darija, while articles may be in Moroccan Darija, in Modern Standard Arabic, or a mix of both (code-switched Moroccan Darija).\n",
        "\n",
        "**Data Fields**\n",
        "- *article*: a string containing the body of the news article\n",
        "- *headline*: a string containing the article's headline\n",
        "- *categories*: a list of string of article categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6_dAxvAveJN"
      },
      "source": [
        "## What we will do:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9sCB3IiveJN"
      },
      "source": [
        "<img src=\"https://github.com/stevenkolawole/indaba-low-resource-nlp-prac/blob/c426a3e79c3a06d38562f49d7fc57ec0c751622c/content/DLI_LR_llm_prac_1.png?raw=1\" width=\"40%\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYsi9rboveJN"
      },
      "source": [
        "## Evaluation Metric: ROUGE\n",
        "\n",
        "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate the quality of summaries by comparing them to reference (or ground truth) summaries. ROUGE is widely used in Natural Language Processing (NLP) tasks, particularly for evaluating the performance of text summarization models.\n",
        "\n",
        "![ROUGE-Base](https://i0.wp.com/blog.uptrain.ai/wp-content/uploads/2024/01/rouge-n.webp?resize=700%2C228&ssl=1)\n",
        "\n",
        "### Key ROUGE Variants\n",
        "\n",
        "1. **ROUGE-N**: Measures the overlap of n-grams between the candidate summary and the reference summary.\n",
        "\n",
        "![ROUGE-1](https://clementbm.github.io/assets/2021-12-23/rouge-unigrams.png)\n",
        "\n",
        "*caption:*\n",
        "$ROUGE_1 = \\frac{7}{10} = 0.7$\n",
        "\n",
        "   - **ROUGE-1**: Overlap of unigrams (1-gram).\n",
        "   - **ROUGE-2**: Overlap of bigrams (2-grams).\n",
        "   - **ROUGE-L**: Measures the longest common subsequence (LCS) between the candidate and reference summaries.\n",
        "\n",
        "2. **ROUGE-L**: Measures the longest common subsequence (LCS) between the candidate summary and the reference summary. Unlike ROUGE-N, ROUGE-L considers sentence-level structure similarity by identifying the longest co-occurring sequence of words in both summaries.\n",
        "\n",
        "\n",
        "\n",
        "3. **ROUGE-W**: A weighted version of ROUGE-L that gives more importance to the contiguous LCS.\n",
        "\n",
        "4. **ROUGE-S**: Measures the overlap of skip-bigrams, which are pairs of words in their order of appearance that can have any number of gaps between them.\n",
        "\n",
        "### How ROUGE is Computed\n",
        "\n",
        "ROUGE metrics can be calculated in terms of three measures:\n",
        "\n",
        "- **Recall**: The ratio of overlapping units (n-grams, LCS, or skip-bigrams) between the candidate summary and the reference summary to the total units in the reference summary. It answers, \"How much of the reference summary is captured by the candidate summary?\"\n",
        "\n",
        "- **Precision**: The ratio of overlapping units between the candidate summary and the reference summary to the total units in the candidate summary. It answers, \"How much of the candidate summary is relevant to the reference summary?\"\n",
        "\n",
        "- **F1-Score**: The harmonic mean of Precision and Recall. This gives a balanced measure that considers both precision and recall.\n",
        "\n",
        "### Importance of ROUGE\n",
        "\n",
        "ROUGE is essential for summarization tasks because it provides a standardized way to evaluate and compare different summarization models. Higher ROUGE scores generally indicate that the candidate summary is more similar to the reference summary, meaning the model is likely performing well.\n",
        "\n",
        "#### NOTE: Caveat\n",
        "<div style=\"display: flex; justify-content: space-between;\">\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*8ZNpaag-Nr2GLs3A-sz0aQ.png\" alt=\"limitation 1\" width=\"250\"/>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*CLIKeyKYiR6sNA4yjIkCWg.png\" alt=\"limitation 2\" width=\"250\"/>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*667HMbjSLJhwR_xqBau3JQ.png\" alt=\"limitation 3\" width=\"250\"/>\n",
        "</div>\n",
        "\n",
        "While ROUGE and other evaluation metrics (e.g., BLEU, METEOR, etc) serve as valuable tools for quick and straightforward evaluation of language models, they have certain limitations that render them less than ideal. To begin with, they fall short when it comes to assessing the fluency, coherence, and overall meaning of passages. They are also relatively insensitive to word order. ROUGE primarily measures lexical overlap and may not fully capture the semantic meaning or quality of a summary. For these reasons, researchers are still trying to find improved metrics.\n",
        "\n",
        "Therefore, these metrics are not shoe-in replacements for human evaluation, but are best used in conjunction with human evaluations for a more comprehensive assessment of summary quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LxSz2ebveJO"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('Goud/Goud-sum')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS53FEVPveJO"
      },
      "outputs": [],
      "source": [
        "#Data Exploration\n",
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AABm2WlJveJO"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA2PQeZ_veJO"
      },
      "source": [
        "# Section1: Efficiently Fine-Tune Seq2Seq Models with Low Rank Adaptation (LoRA)\n",
        "We are going to leverage Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), and [PEFT](https://github.com/huggingface/peft).\n",
        "\n",
        "You will learn how to:\n",
        "\n",
        "1. Setup Development Environment\n",
        "2. Load and prepare the dataset\n",
        "3. Fine-Tune Multilingual BERT with LoRA and bnb int-8\n",
        "4. Evaluate & run Inference\n",
        "5. Cost performance comparison\n",
        "\n",
        "### Quick intro to PEFT or Parameter Efficient Fine-tuning\n",
        "<img src=\"https://github.com/stevenkolawole/indaba-low-resource-nlp-prac/blob/c426a3e79c3a06d38562f49d7fc57ec0c751622c/content/PEFT_method.png?raw=1\" width=\"60%\" />\n",
        "\n",
        "[PEFT](https://github.com/huggingface/peft), or Parameter Efficient Fine-tuning, is a new open-source library from Hugging Face to enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. PEFT currently includes techniques for:\n",
        "\n",
        "- LoRA:Â [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
        "- Prefix Tuning:Â [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
        "- P-Tuning:Â [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
        "- Prompt Tuning:Â [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNAcjTSLveJO"
      },
      "source": [
        "## Summarization flow using BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3WZz7S4veJO",
        "outputId": "50a96bdd-dee4-4a2a-d6a3-7fe9b29faeaf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, Dataset, load_metric\n",
        "from transformers import BertTokenizer, EncoderDecoderModel, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "\n",
        "\n",
        "# Load the tokenizer and model\n",
        "model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = EncoderDecoderModel.from_encoder_decoder_pretrained(model_name, model_name)\n",
        "\n",
        "# Set decoder_start_token_id\n",
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "8320199d383145e382058c1f6edee467",
            "895cd8a7287c4f968e40e06cd8331d1a"
          ]
        },
        "id": "ca1zyqXJveJP",
        "outputId": "1274ba12-9022-41f1-ede0-60a77e39ad87"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8320199d383145e382058c1f6edee467",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/148785 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max source length: 512\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "895cd8a7287c4f968e40e06cd8331d1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/148785 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max target length: 16\n"
          ]
        }
      ],
      "source": [
        "from datasets import concatenate_datasets\n",
        "import numpy as np\n",
        "# The maximum total input sequence length after tokenization.\n",
        "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
        "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"article\"], truncation=True), batched=True, remove_columns=[\"article\", \"categories\"])\n",
        "input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
        "# take 85 percentile of max length for better utilization\n",
        "max_source_length = int(np.percentile(input_lenghts, 85))\n",
        "print(f\"Max source length: {max_source_length}\")\n",
        "\n",
        "# The maximum total sequence length for target text after tokenization.\n",
        "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
        "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"categories\"], truncation=True), batched=True, remove_columns=[\"article\", \"categories\"])\n",
        "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
        "# take 90 percentile of max length for better utilization\n",
        "max_target_length = int(np.percentile(target_lenghts, 90))\n",
        "print(f\"Max target length: {max_target_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_Uzk866veJP"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data\n",
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(examples[\"article\"], max_length=512, truncation=True, padding=\"max_length\")\n",
        "    outputs = tokenizer(examples[\"headline\"], max_length=150, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    inputs[\"decoder_input_ids\"] = outputs[\"input_ids\"]\n",
        "    inputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
        "\n",
        "    # replace padding token id's of the labels by -100 so it's ignored by the loss\n",
        "    inputs[\"labels\"] = [[(label if label != tokenizer.pad_token_id else -100) for label in labels] for labels in inputs[\"labels\"]]\n",
        "\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c99a7211424a4d858507351a13d68efd",
            "f4b8ca5d233540768c29d1b755cfe6da",
            "ed4918c5701e4c48a31a03348553ad51",
            "daf8b3596b8041f6a1d0a590e61a119f",
            "61068863224d4140a3913f175a552d78"
          ]
        },
        "id": "MMVMAvcIveJP",
        "outputId": "d243340a-d73b-4aa6-998b-924966904195"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c99a7211424a4d858507351a13d68efd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/139288 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4b8ca5d233540768c29d1b755cfe6da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/9497 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed4918c5701e4c48a31a03348553ad51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/9497 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys of tokenized dataset: ['input_ids', 'token_type_ids', 'attention_mask', 'decoder_input_ids', 'labels']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "daf8b3596b8041f6a1d0a590e61a119f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/2 shards):   0%|          | 0/139288 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61068863224d4140a3913f175a552d78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/9497 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"article\", \"categories\", \"headline\"])\n",
        "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
        "# save datasets to disk for later easy loading\n",
        "tokenized_dataset[\"train\"].save_to_disk(\"data/train\")\n",
        "tokenized_dataset[\"test\"].save_to_disk(\"data/eval\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbip_2vAveJP",
        "outputId": "51296c9a-d12c-4a8a-dd58-6d8f62c8e97b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/alizaidi/micromamba/envs/peft/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "\n",
        "# Define data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    save_total_limit=3,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jvjb2KwbveJP",
        "outputId": "53507027-bc68-4c3b-d0f2-d00696e863d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,769,472 || all params: 385,964,283 || trainable%: 0.4585\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "\n",
        "# Define LoRA Config\n",
        "lora_config = LoraConfig(\n",
        " r=16,\n",
        " lora_alpha=32,\n",
        " target_modules=[\"query\", \"value\"],\n",
        " lora_dropout=0.05,\n",
        " bias=\"none\",\n",
        " task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")\n",
        "# prepare int-8 model for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# add LoRA adaptor\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# trainable params: 18874368 || all params: 11154206720 || trainable%: 0.16921300163961817"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBug5VkZveJP",
        "outputId": "33f46ae2-438b-4ad8-a7e9-0a621788c74c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2722794/1378865303.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  rouge = load_metric(\"rouge\")\n"
          ]
        }
      ],
      "source": [
        "# Define ROUGE metric\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them\n",
        "    labels = [[(label if label != -100 else tokenizer.pad_token_id) for label in labels] for labels in labels]\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    # Extract the ROUGE scores\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zZHg-xHveJP"
      },
      "outputs": [],
      "source": [
        "# Sample a subset of the tokenized training data\n",
        "subset_fraction = 0.05  # 5% of the training data\n",
        "train_subset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(int(subset_fraction * len(tokenized_datasets[\"train\"]))))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtaDeCbzveJP"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "output_dir=\"goud-bert\"\n",
        "\n",
        "# Define training args\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "\t\tauto_find_batch_size=True,\n",
        "    learning_rate=1e-3, # higher learning rate\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=500,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qeXi_IAveJQ"
      },
      "source": [
        "## Training [we will not be running training during the practical time]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5AQIvxBveJQ",
        "outputId": "5d541fbc-b627-4416-e60c-622f73226559"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
            "/home/alizaidi/micromamba/envs/peft/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='136' max='52233' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  136/52233 00:30 < 3:17:20, 4.40 it/s, Epoch 0.01/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E33-QuSKveJQ"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save_pretrained(\"./fine_tuned_bert2bert_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_bert2bert_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRR4scsyveJQ"
      },
      "source": [
        "## Evaluation1: early checkpoint of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8hNN5H_veJQ"
      },
      "outputs": [],
      "source": [
        "#load an early checkpoint\n",
        "#run evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kssoYoVbveJQ"
      },
      "source": [
        "## Evaluation2: final trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggLzfzIaveJQ"
      },
      "source": [
        "# Section2: Summarization using GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mM92286bveJQ"
      },
      "outputs": [],
      "source": [
        "from openai import AzureOpenAI\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "import openai\n",
        "import time\n",
        "\n",
        "DATASET = \"Goud\"\n",
        "MAX_TRAIN = 20\n",
        "\n",
        "goud_data = load_dataset(\"Goud/Goud-sum\")\n",
        "\n",
        "train_source = goud_data[\"train\"][\"article\"]\n",
        "train_target = goud_data[\"train\"][\"headline\"]\n",
        "train_len = len(train_source)\n",
        "\n",
        "test_source = goud_data[\"test\"][\"article\"]\n",
        "\n",
        "output_filename = f\"./{DATASET}_test_generated_{MAX_TRAIN}.csv\"\n",
        "\n",
        "def summarize_news_article():\n",
        "    rewritten_prompt_count = 0\n",
        "    line_count = 0\n",
        "    wait_time = 1\n",
        "    df_lines = []\n",
        "    client = AzureOpenAI(\n",
        "        azure_endpoint=\"\",\n",
        "        api_version=\"\",\n",
        "        api_key=\"\",\n",
        "    )\n",
        "    existing_len = 0\n",
        "    if os.path.exists(output_filename):\n",
        "        existing_df = pd.read_csv(output_filename)\n",
        "        existing_len = existing_df.shape[0]\n",
        "        rewritten_prompt_count = existing_len\n",
        "        line_count = existing_len\n",
        "        df_lines = existing_df.to_dict('records')\n",
        "    for data in tqdm(test_source[existing_len:], desc=f\"Lines processed from {existing_len}-th line\"):\n",
        "        news_article = data.strip()\n",
        "        line_count += 1\n",
        "        made_error = True\n",
        "        num_error = 0\n",
        "        while made_error:\n",
        "            messages = [{\"role\": \"system\", \"content\": \"You are asked to summarize a news article written in Modern Standard Arabic and Moroccan Darija, and write that summary as a clickbait headline, in Moroccan Darija only.\\n\"}]\n",
        "            if MAX_TRAIN - num_error > 0:\n",
        "                for _ in range(MAX_TRAIN-num_error):\n",
        "                    idx = random.choice(range(train_len))\n",
        "                    train_src = train_source[idx]\n",
        "                    train_tgt = train_target[idx]\n",
        "                    messages.append({\"role\": \"user\", \"content\": f\"Summarize the following news article into a headline in Moroccan Darija only:\\n\\\"{train_src}\\\"\"})\n",
        "                    messages.append({\"role\": \"assistant\", \"content\": f\"Absolutely! Here is the headline summarizing your news article:\\n\\\"{train_tgt}\\\"\"})\n",
        "            messages.append({\"role\": \"user\", \"content\": f\"Summarize the following news article into a headline in Moroccan Darija only:\\n\\\"{news_article}\\\"\"})\n",
        "            try:\n",
        "                response = client.chat.completions.create(\n",
        "                    messages=messages,\n",
        "                    model=\"gpt-4-32k-0613\", #Must fill in, optional: gpt-35-turboã€gpt-4ã€gpt-4-32k\n",
        "                )\n",
        "                rewritten_prompt = response.choices[0].message.content\n",
        "                df_lines.append({\"article\": news_article, \"generated_headline\": rewritten_prompt})\n",
        "                rewritten_prompt_count += 1\n",
        "                made_error = False\n",
        "            except Exception as e:\n",
        "                if type(e) is openai.RateLimitError:\n",
        "                    print(\"Rate limit error\")\n",
        "                    print(f\"Wait for {wait_time} seconds because all calls failed: \", flush=True)\n",
        "                    time.sleep(wait_time)\n",
        "                    wait_time *= 2\n",
        "                else:\n",
        "                    print(e)\n",
        "                    num_error += 1\n",
        "                    print(\"May be too long, reducing context to:\", MAX_TRAIN-num_error)\n",
        "\n",
        "\n",
        "    df = pd.DataFrame.from_dict(df_lines)\n",
        "    df.to_csv(output_filename)\n",
        "\n",
        "summarize_news_article()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpX3i4dfveJQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from rouge_metric import PyRouge\n",
        "\n",
        "def evaluate_rouge(hypotheses, references):\n",
        "    these_refs = [[ref.strip().lower()] for ref in references]\n",
        "    rouge = PyRouge(rouge_n=(1, 2), rouge_l=True)\n",
        "    scores = rouge.evaluate(hypotheses, these_refs)\n",
        "    print(scores)\n",
        "\n",
        "def substring_after_colon(input_string):\n",
        "    # Find the index of the first colon\n",
        "    colon_index = input_string.find(':')\n",
        "\n",
        "    # If a colon is found, return the substring starting just after it\n",
        "    if colon_index != -1:\n",
        "        return input_string[colon_index + 1:]\n",
        "    else:\n",
        "        # If no colon is found, return an empty string\n",
        "        return input_string\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    DATASET = \"Goud\"\n",
        "    hypotheses = pd.read_csv(f\"./{DATASET}_test_generated_0.csv\")[\"generated_headline\"].tolist()\n",
        "    hypotheses = [substring_after_colon(hypo).replace(\"\\\"\", \"\").strip() for hypo in hypotheses]\n",
        "    goud_data = load_dataset(\"Goud/Goud-sum\")\n",
        "    references = goud_data[\"test\"][\"headline\"]\n",
        "    evaluate_rouge(hypotheses, references)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhIkaajyaYGl"
      },
      "source": [
        "**Group Task:**\n",
        "\n",
        "Task that involves asking your neighbour or a group a question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L03B3HKwhAhK"
      },
      "outputs": [],
      "source": [
        "# @title Generate Quiz Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/zbJoTSz3nfYq1VrY6\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "## Conclusion\n",
        "**Summary:**\n",
        "\n",
        "[Summary of the main points/takeaways from the prac.]\n",
        "\n",
        "**Next Steps:**\n",
        "\n",
        "[Next steps for people who have completed the prac, like optional reading (e.g. blogs, papers, courses, youtube videos). This could also link to other pracs.]\n",
        "\n",
        "**Appendix:**\n",
        "\n",
        "[Anything (probably math heavy stuff) we don't have space for in the main practical sections.]\n",
        "\n",
        "**References:**\n",
        "\n",
        "[References for any content used in the notebook.]\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2022)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "## Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/WUpRupqfhFtbLXtN6\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "WILOYJH4gCnD"
      ],
      "name": "Indaba_2022_Prac_Template.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "145833166d986a8417df3c7acb65d917d84b716b5a452e57fcacdc66f1a168c9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}