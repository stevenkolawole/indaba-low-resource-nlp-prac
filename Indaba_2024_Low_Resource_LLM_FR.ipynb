{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NLP à ressources limitées**\n",
    "\n",
    "<img src=\"https://dli2024prac.blob.core.windows.net/content/lr_llm_header.png\" width=\"60%\" allign =\"center\"/>\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2024/blob/main/practicals/Indaba_2024_Prac_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [Change colab link to point to prac.]\n",
    "\n",
    "© Deep Learning Indaba 2024. Licence Apache 2.0.\n",
    "\n",
    "**Auteurs :**\n",
    "- Ali Zaidi\n",
    "- Aya Salama\n",
    "- Khalil Mrini\n",
    "- Steven Kolawole \n",
    "\n",
    "**Introduction :**\n",
    "\n",
    "Le TAL à ressources limitées (Traitement Automatique des Langues, ou NLP en anglais, Natural Language Processing) fait référence à l'étude et au développement de modèles et de systèmes TAL pour des langues, des tâches ou des domaines qui disposent de peu de données et de ressources. Cela peut inclure des langues avec moins de corpus textes numériques, d'outils informatiques limités ou de recherches linguistiques moins développées.\n",
    "\n",
    "**Principaux défis du TAL à ressources limitées**\n",
    "\n",
    "1. **Rareté des données :**\n",
    "   - **Données d'entraînement limitées :** De nombreuses langues manquent de grands corpus annotés nécessaires pour entraîner les modèles TAL.\n",
    "   - **Absence de modèles pré-entraînés :** Les modèles TAL populaires comme BERT, GPT et autres ne sont souvent pas disponibles pour les langues à ressources limitées.\n",
    "\n",
    "2. **Diversité linguistique :**\n",
    "   - **Complexité morphologique :** Certaines langues ont des structures grammaticales complexes et une richesse morphologique.\n",
    "   - **Variations dialectales :** Un manque de versions standardisées peut compliquer les tâches TAL.\n",
    "\n",
    "3. **Limitations des ressources :**\n",
    "   - **Contraintes computationnelles :** Les scénarios à faibles ressources impliquent souvent un accès limité à la puissance de calcul et au stockage.\n",
    "   - **Expertise et outils :** Moins d'experts linguistiques et moins d'outils TAL sont adaptés à ces langues.\n",
    "\n",
    "**Sujets :**\n",
    "\n",
    "Contenu : [Traitement Automatique des Langues, Ressources Limitées, Modèles de Langages, Réglage Fin Efficace en Paramètres, Adaptation]  \n",
    "Niveau : [Intermédiaire]\n",
    "\n",
    "\n",
    "**Objectifs/Compétences visées :**\n",
    "\n",
    "- Explorer les défis de la rareté des données\n",
    "- Explorer les limitations des ressources de calcul et les aborder avec un finetuning efficace des paramètres\n",
    "- Comparer les performances entre les petits (BERT) et les grands (GPT) modèles de langage sur des langues/tâches à faibles ressources.\n",
    "  \n",
    "\n",
    "**Prérequis :**\n",
    "\n",
    "[Connaissances requises pour cette pratique. Vous pouvez lier une session de track parallèle pertinente, des blogs, des articles, des cours, des sujets, etc.]\n",
    "_ lier des ressources sur les LLMs, BERT, articles de Masakhane sur les conversations à faibles ressources\n",
    "\n",
    "**Plan :**\n",
    "\n",
    "[Points qui lient chaque section. Auto-générer en suivant les instructions [ici](https://stackoverflow.com/questions/67458990/how-to-automatically-generate-a-table-of-contents-in-colab-notebook).]\n",
    "\n",
    "**Avant de commencer :**\n",
    "\n",
    "[Tâches juste avant de commencer.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exécutez la cellule pour configurer les packages Python et ressources nécessaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dossiers de ressources** :\n",
    "\n",
    "Les ressources dont vous aurez besoin pour réaliser ce pratique seront téléchargées lorsque vous exécuterez la prochaine cellule.\n",
    "Une fois le téléchargement et l'extraction terminés, vous aurez les dossiers suivants présents dans le dossier \"resources\" dans le répertoire parent :\n",
    "\n",
    "- *models* : ce dossier contient les modèles pré-entraînés qui seront utilisés dans le pratique\n",
    "- *dataset* : ce dossier contient le jeu de données Goud-sum que nous utiliserons dans le pratique\n",
    "- *generated_responses* : ce dossier contient des résumés pré-générés qui seront utilisés dans la Section 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# Installer les packages requis\n",
    "utils.install_requirements()\n",
    "\n",
    "# Téléchargez et extrayez le fichier zip contenant les ressources\n",
    "utils.download_and_extract_zip(\"https://dli2024prac.blob.core.windows.net/resources/resources.zip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets, load_metric, load_from_disk\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PeftConfig, PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from huggingface_hub import login\n",
    "from rouge_metric import PyRouge\n",
    "import wandb\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/alizaidi/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malizaidi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "login(token=os.getenv(\"HF_HUB_TOKEN\"))\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce pratique, nous nous intéressons à générer des titres pour les articles de presse présentés sur le site d'actualités [Goud.ma](www.Gound.ma).\n",
    "\n",
    "Nous aborderons cela comme une tâche de résumé où l'entrée est le corps d'un article de presse et la sortie est un titre approprié. L'[ensemble de données Goud](https://github.com/issam9/goud-summarization-dataset) contient 158k articles et leurs titres. Tous les titres sont en Darija marocaine, tandis que les articles peuvent être en Darija marocaine, en arabe standard moderne, ou un mélange des deux (Darija marocaine en alternance de code).\n",
    "\n",
    "**Champs de données**\n",
    "- *article* : une chaîne de caractères contenant le corps de l'article de presse\n",
    "- *headline* : une chaîne de caractères contenant le titre de l'article\n",
    "- *categories* : une liste de chaînes de caractères des catégories d'articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ce que nous allons faire :\n",
    "<img src=\"./content/DLI_LR_llm_prac_1.png\" width=\"40%\" />\n",
    "\n",
    "**Dossiers de ressources** :\n",
    "\n",
    "Les ressources dont vous aurez besoin pour réaliser cet exercice font partie du dépôt, vous les trouverez dans le dossier parent :\n",
    "\n",
    "- Dossier *Data* : ce dossier contient le dataset Goud-sum que nous utiliserons dans la pratique\n",
    "- Dossier *genrated_responses* : ce dossier contient des résumés pré-générés qui seront utilisés dans la Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métrique d'évaluation : ROUGE\n",
    "\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) est un ensemble de métriques utilisé pour évaluer la qualité des résumés en les comparant aux résumés de référence (ou ground truth). ROUGE est largement utilisé dans les tâches de Traitement Automatique du Langage Naturel (NLP), en particulier pour évaluer la performance des modèles de résumé de texte.\n",
    "\n",
    "![ROUGE-Base](https://i0.wp.com/blog.uptrain.ai/wp-content/uploads/2024/01/rouge-n.webp?resize=700%2C228&ssl=1)\n",
    "\n",
    "### Variants clés de ROUGE\n",
    "\n",
    "1. **ROUGE-N** : Mesure le chevauchement des n-grams entre le résumé candidat et le résumé de référence.\n",
    "\n",
    "![ROUGE-1](https://clementbm.github.io/assets/2021-12-23/rouge-unigrams.png)\n",
    "\n",
    "*caption:*\n",
    "$ROUGE_1 = \\frac{7}{10} = 0.7$\n",
    "\n",
    "   - **ROUGE-1** : Chevauchement des unigrams (1-gram).\n",
    "   - **ROUGE-2** : Chevauchement des bigrams (2-grams).\n",
    "   - **ROUGE-L** : Mesure la plus longue sous-séquence commune (LCS) entre les résumés candidat et de référence.\n",
    "\n",
    "2. **ROUGE-L** : Mesure la plus longue sous-séquence commune (LCS) entre le résumé candidat et le résumé de référence. Contrairement à ROUGE-N, ROUGE-L considère la similarité de la structure au niveau de la phrase en identifiant la plus longue séquence de mots co-occurrences dans les deux résumés.\n",
    "\n",
    "3. **ROUGE-W** : Une version pondérée de ROUGE-L qui donne plus d'importance à la LCS contiguë.\n",
    "\n",
    "4. **ROUGE-S** : Mesure le chevauchement des skip-bigrams, qui sont des paires de mots dans leur ordre d'apparition pouvant avoir n'importe quel nombre d'écarts entre eux.\n",
    "\n",
    "### Comment ROUGE est calculé\n",
    "\n",
    "Les métriques ROUGE peuvent être calculées en termes de trois mesures :\n",
    "\n",
    "- **Recall** : Le ratio des unités chevauchantes (n-grams, LCS, ou skip-bigrams) entre le résumé candidat et le résumé de référence par rapport au nombre total d'unités dans le résumé de référence. Cela répond à la question : \"Quelle part du résumé de référence est capturée par le résumé candidat ?\".\n",
    "\n",
    "- **Précision** : Le ratio des unités chevauchantes entre le résumé candidat et le résumé de référence par rapport au nombre total d'unités dans le résumé candidat. Cela répond à la question : \"Quelle part du résumé candidat est pertinente par rapport au résumé de référence ?\".\n",
    "\n",
    "- **F1-Score** : La moyenne harmonique de Précision et Recall. Cela donne une mesure équilibrée qui considère à la fois la précision et le recall.\n",
    "\n",
    "### Importance de ROUGE\n",
    "\n",
    "ROUGE est essentiel pour les tâches de résumé car il fournit un moyen normalisé pour évaluer et comparer différents modèles de résumé. Des scores ROUGE plus élevés indiquent généralement que le résumé candidat est plus similaire au résumé de référence, ce qui signifie que le modèle fonctionne probablement bien.\n",
    "\n",
    "#### NOTE: Mise en garde\n",
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*8ZNpaag-Nr2GLs3A-sz0aQ.png\" alt=\"limitation 1\" width=\"250\"/>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*CLIKeyKYiR6sNA4yjIkCWg.png\" alt=\"limitation 2\" width=\"250\"/>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*667HMbjSLJhwR_xqBau3JQ.png\" alt=\"limitation 3\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "Bien que ROUGE et d'autres métriques d'évaluation (par exemple, BLEU, METEOR, etc.) servent d'outils précieux pour une évaluation rapide et simple des modèles de langue, elles présentent certaines limitations qui les rendent moins idéales. Pour commencer, elles ne parviennent pas à évaluer la fluidité, la cohérence et le sens général des passages. Elles sont également relativement insensibles à l'ordre des mots. ROUGE mesure principalement le chevauchement lexical et peut ne pas capturer pleinement le sens sémantique ou la qualité d'un résumé. Pour ces raisons, les chercheurs cherchent encore à trouver des métriques améliorées.\n",
    "\n",
    "Par conséquent, ces métriques ne remplacent pas totalement l'évaluation humaine, mais elles sont mieux utilisées conjointement avec les évaluations humaines pour une évaluation plus complète de la qualité des résumés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "# Load the dataset from disk\n",
    "dataset = load_from_disk(\"./data/Goud-sum/Goud-sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérifier le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'headline', 'categories'],\n",
      "        num_rows: 139288\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'headline', 'categories'],\n",
      "        num_rows: 9497\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'headline', 'categories'],\n",
      "        num_rows: 9497\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': 'منير العلمي من مراكش: تحول فضاء مقر الغرفة الفلاحية بمدينة مراكش، الذي يحتضن في هذه الأثناء، انتخاب رئيس وأعضاء المكتب المسير للغرفة الفلاحية بجهة مراكش آسفي، إلى حلبة للاشتباكات والملاسنات، بعد اشتداد الخلاف بين البرلمانيين حميد العكرود وعمر خفيف، اللذين ينتميان إلى حزب التجمع الوطني للأحرار، ما كاد يعصف بالاجتماع بعد انطلاق شرارة الاشتباك بالأيادي التي أجهضت في مهدها بتدخل بعض الحاضرين. وحسب شهود عيان، فإن عمر خفيف، الذي يشغل رئيس جماعة أكفاي، ومدعم الحبيب بن الطالب المنسق الاقليمي لحزب الأصالة والمعاصر الذي يتجه لتولي رئاسة الغرفة لولاية تانية، رفض دخول حميد العكرود للمنافسة على رئاسة الغرفة، واصفا إياه بـ “الأمي الذي لايفقه شيئا”، ليدخل الطرفان في ملاسنات كلامية قبل أن يتحول الصراع إلى تشابك بالأيدي. ',\n",
       " 'headline': 'برلمانيين من حزب الحمامة قلبوها بونيا قبل انتخاب رئيس وأعضاء غرفة الفلاحة بجهة مراكش آسفي (صور)',\n",
       " 'categories': \"['آش واقع', 'الرئيسية']\"}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section1: Affiner efficacement les modèles Seq2Seq avec Low Rank Adaptation (LoRA)\n",
    "\n",
    "Nous allons utiliser Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), et [PEFT](https://github.com/huggingface/peft). \n",
    "\n",
    "Vous apprendrez à :\n",
    "\n",
    "1. Configurer l'environnement de développement\n",
    "2. Charger et préparer le jeu de données\n",
    "3. Affiner Multilingual BERT avec LoRA et bnb int-8\n",
    "4. Évaluer et exécuter une inférence\n",
    "5. Comparaison des performances de coût\n",
    "\n",
    "### Introduction rapide à PEFT ou Affinage de paramètres efficace\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: flex-start;\">    <figure style=\"text-align: center;\">\n",
    "        <a href=\"https://arxiv.org/abs/2303.15647#\" target=\"_blank\">\n",
    "            <img src=\"./content/PEFT_method.png\" width=\"90%\" />\n",
    "        </a>\n",
    "        <figcaption><a href=\"https://arxiv.org/abs/2303.15647#\" target=\"_blank\">Méthodes PEFT, de l'article \"Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning\"\n",
    "</a></figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "[PEFT](https://github.com/huggingface/peft), ou Affinage de paramètres efficace, est une nouvelle bibliothèque open-source de Hugging Face permettant l'adaptation efficace des modèles de langue pré-entrainés (PLMs) à diverses applications en aval sans affiner tous les paramètres du modèle. PEFT inclut actuellement des techniques pour :\n",
    "\n",
    "- LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "- Prefix Tuning: [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
    "- P-Tuning: [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "- Prompt Tuning: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptation à Basse Rangée (LoRA)\n",
    "\n",
    "Bien que les grands modèles de langage (LLMs) aient montré des performances remarquables dans une large gamme de tâches de NLP, ils nécessitent des ressources de calcul importantes pour l'entraînement, le fine-tuning et le déploiement. De plus, de nombreux cas d'utilisation du monde réel nécessitent l'adaptation des LLMs disponibles à leur tâche cible afin d'atteindre les performances souhaitées.\n",
    "\n",
    "Alors que le fine-tuning d'un LLM complet est prohibitivement coûteux, même sur de petits ensembles de données. Par exemple, le fine-tuning complet du modèle Llama7B nécessite 112 Go de VRAM, soit au moins deux GPU A100 de 80 Go. Heureusement, des méthodes de fine-tuning efficaces en termes de paramètres comme LoRA permettent aux utilisateurs avec des ressources limitées d'adapter efficacement et efficacement un LLM à leur tâche cible.\n",
    "\n",
    "Dans ce tutoriel, nous explorons QLoRA, qui est une technique de fine-tuning efficace en termes de paramètres qui réduit le nombre de paramètres ajustés pendant le processus d'adaptation, et introduit en plus une quantification pour réduire encore l'empreinte mémoire du modèle adapté.\n",
    "\n",
    "### Comment Fonctionne LoRA ?\n",
    "\n",
    "L'article [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) s'inspire de la conjecture selon laquelle les modèles surparamétrés couvrent une dimension intrinsèque à basse rangée. Une basse dimension intrinsèque signifie que les données peuvent être efficacement représentées ou approchées par un espace de dimension inférieure tout en conservant la plupart de leurs informations ou structures essentielles. En d'autres termes, cela signifie que nous pouvons décomposer la nouvelle matrice de poids pour la tâche adaptée en matrices de dimension inférieure (plus petites) sans perdre d'informations significatives.\n",
    "\n",
    "Concrètement, supposons que $\\delta W$ soit la mise à jour des poids pour une matrice de poids de $A\\times B$. Alors, une décomposition à basse rangée de $\\delta W$ peut être exprimée comme : $\\delta W = W_A W_B$, où $W_A$ est une matrice de $A\\times k$ et $W_B$ est une matrice de $k\\times B$. Ici, $k$ est le rang de la décomposition, et est généralement beaucoup plus petit que $A$ et $B$.\n",
    "\n",
    "![Image courtoisie du tutoriel de Sebastian Raschka sur LoRA de Lightning.AI](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/lora-4-300x226@2x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé en utilisant mT5\n",
    "\n",
    "Avant l'affinage de notre modèle, nous devons sélectionner le modèle que nous utiliserons comme modèle de base. Dans ce cas, nous utiliserons le modèle [mT5](https://huggingface.co/google/mt5-small), qui est une variante multilingue du modèle T5. Le modèle mT5 est entraîné sur un large corpus multilingue et est capable de réaliser une vaste gamme de tâches en PNL, y compris le résumé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alizaidi/micromamba/envs/peft/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous préparons nos jeux de données pour l'entraînement. Cela nécessite de tokenizer les séquences d'entrée et de sortie, de les compléter jusqu'à la longueur souhaitée, puis de les convertir en objets Dataset de PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(\n",
    "    lambda x: tokenizer(x[\"article\"], truncation=True),\n",
    "    batched=True,\n",
    "    remove_columns=[\"categories\", \"headline\"],\n",
    ")\n",
    "input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 571\n"
     ]
    }
   ],
   "source": [
    "# take 85 percentile of max length for better utilization\n",
    "max_source_length = int(np.percentile(input_lenghts, 85))\n",
    "print(f\"Max source length: {max_source_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 50\n"
     ]
    }
   ],
   "source": [
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(\n",
    "    lambda x: tokenizer(x[\"headline\"], truncation=True),\n",
    "    batched=True,\n",
    "    remove_columns=[\"article\", \"categories\"],\n",
    ")\n",
    "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "# take 90 percentile of max length for better utilization\n",
    "max_target_length = int(np.percentile(target_lenghts, 90))\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    # # add prefix to the input for t5\n",
    "    # inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(sample['article'], max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `summary` keyword argument\n",
    "    labels = tokenizer(sample[\"headline\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"headline\", \"article\", \"categories\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9239cc1fc6694dd1b0edd5613e98d9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/9497 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset[\"test\"].save_to_disk(\"arabic-goud-data/eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, nous devons définir notre configuration pour LoRA. Les principaux paramètres pour LoRA sont :\n",
    "\n",
    "* `r` : il s'agit du rang des matrices décomposées $A$ et $B$ à apprendre pendant le fine-tuning. Un nombre plus petit économisera plus de mémoire GPU mais pourrait diminuer les performances.\n",
    "* `lora_alpha` : il s'agit du poids de la perte de bas-rang dans la fonction de perte totale, ou du coefficient pour le facteur $\\Delta W$ appris. Un nombre plus grand entraînera généralement un changement de comportement plus significatif après le fine-tuning.\n",
    "* `lora_dropout` : le ratio de dropout pour les couches dans les adaptateurs LoRA $A$ et $B$.\n",
    "* `target_modules` : les modules pour lesquels apprendre la décomposition de bas-rang. Cela pourrait être toutes les couches linéaires, par exemple, ou des modules spécifiques du réseau de base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 688,128 || all params: 300,864,896 || trainable%: 0.2287\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "  r=16,\n",
    "  lora_alpha=32,\n",
    "  target_modules=[\"q\", \"v\"],\n",
    "  lora_dropout=0.05,\n",
    "  bias=\"none\",\n",
    "  task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "# prepare int-8 model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "output_dir = \"lora-goud-mt5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"lora-mt5-goud\",\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=[\"tensorboard\", \"wandb\"],\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset = tokenized_dataset[\"train\"],\n",
    "    eval_dataset = tokenized_dataset[\"validation\"].select(range(20)),\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/alizaidi/dev/nlp/llms/indaba/indaba-low-resource-nlp-prac/wandb/run-20240816_012514-brfaca9p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora-mt5-goud\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/alizaidi/huggingface\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/alizaidi/huggingface/runs/brfaca9p\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='174110' max='174110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [174110/174110 4:54:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.364100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.733600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.573900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.487000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.375900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.370600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.309900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.276800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.278400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.174400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>4.174500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>4.145700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>4.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>4.122800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>4.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>4.096600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>4.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>4.070700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>4.075500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>4.048700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>4.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>4.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>4.032200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>4.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>4.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>4.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>4.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>4.054900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.983700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.977800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>4.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.962000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.950300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.963800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.948300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.954600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>4.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.944200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.942800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.938300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.926900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.943500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.933100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>3.914700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>3.971900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.906500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>3.913700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.909400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.934100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>3.904400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.883700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>3.890200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.901500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>3.899800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.869100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>3.912900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.917400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>3.887100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>3.892100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>3.874800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>3.897100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>3.874900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>3.846300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>3.858400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>3.864700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>3.873700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>3.855400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>3.860700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>3.864000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>3.822500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>3.871400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>3.873800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>3.864700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>3.871200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>3.824300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>3.836300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>3.822100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>3.809800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>3.870400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>3.828100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>3.858900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>3.821300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>3.850200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>3.808800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>3.836400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>3.848400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>3.831800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>3.844700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>3.852400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>3.810800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>3.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>3.826900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>3.818600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>3.808900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>3.829400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>3.811100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>3.810500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>3.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>3.779800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>3.802900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>3.798900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>3.799100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>3.812200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>3.799300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>3.808400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>3.793900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>3.777100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>3.790800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>3.783400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>3.804400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>3.785300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>3.806300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>3.804200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>3.801200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>3.806000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>3.808200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>3.785500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>3.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>3.795900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>3.775200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>3.803400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>3.763200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>3.774300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>3.775800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>3.811800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>3.761400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>3.747300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>3.763600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>3.779900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>3.755300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>3.745700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>3.752600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>3.764000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>3.755300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>3.731200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>3.735600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>3.761100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74500</td>\n",
       "      <td>3.765700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>3.735500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75500</td>\n",
       "      <td>3.749600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>3.758200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76500</td>\n",
       "      <td>3.745700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>3.735900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>3.738800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>3.742200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78500</td>\n",
       "      <td>3.770900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>3.771400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79500</td>\n",
       "      <td>3.741600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>3.754100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80500</td>\n",
       "      <td>3.754300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>3.753500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81500</td>\n",
       "      <td>3.734200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>3.761200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82500</td>\n",
       "      <td>3.732100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>3.752900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83500</td>\n",
       "      <td>3.728700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>3.766300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84500</td>\n",
       "      <td>3.738100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>3.760300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85500</td>\n",
       "      <td>3.742100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>3.735100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86500</td>\n",
       "      <td>3.719900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>3.751800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87500</td>\n",
       "      <td>3.702000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>3.728800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88500</td>\n",
       "      <td>3.727000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>3.711700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89500</td>\n",
       "      <td>3.710300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>3.736100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90500</td>\n",
       "      <td>3.745400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>3.706500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91500</td>\n",
       "      <td>3.698400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>3.720200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92500</td>\n",
       "      <td>3.753100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93000</td>\n",
       "      <td>3.712200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93500</td>\n",
       "      <td>3.714400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>3.725300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94500</td>\n",
       "      <td>3.681700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>3.720100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95500</td>\n",
       "      <td>3.714700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>3.709200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96500</td>\n",
       "      <td>3.732100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97000</td>\n",
       "      <td>3.673100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97500</td>\n",
       "      <td>3.695300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>3.707700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98500</td>\n",
       "      <td>3.690500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99000</td>\n",
       "      <td>3.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99500</td>\n",
       "      <td>3.737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>3.713200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100500</td>\n",
       "      <td>3.707300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101000</td>\n",
       "      <td>3.723000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101500</td>\n",
       "      <td>3.688400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>3.682600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102500</td>\n",
       "      <td>3.701200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103000</td>\n",
       "      <td>3.667900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103500</td>\n",
       "      <td>3.709200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>3.710600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104500</td>\n",
       "      <td>3.678600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105000</td>\n",
       "      <td>3.696300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105500</td>\n",
       "      <td>3.695500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>3.697400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106500</td>\n",
       "      <td>3.683600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107000</td>\n",
       "      <td>3.691600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107500</td>\n",
       "      <td>3.684500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>3.657400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108500</td>\n",
       "      <td>3.670300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109000</td>\n",
       "      <td>3.665800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109500</td>\n",
       "      <td>3.687900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>3.693500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110500</td>\n",
       "      <td>3.682900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111000</td>\n",
       "      <td>3.691300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111500</td>\n",
       "      <td>3.666200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>3.695800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112500</td>\n",
       "      <td>3.669500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113000</td>\n",
       "      <td>3.689800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113500</td>\n",
       "      <td>3.674900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>3.685900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114500</td>\n",
       "      <td>3.649200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115000</td>\n",
       "      <td>3.652300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115500</td>\n",
       "      <td>3.663400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>3.672100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116500</td>\n",
       "      <td>3.641500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117000</td>\n",
       "      <td>3.679800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117500</td>\n",
       "      <td>3.649500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>3.673100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118500</td>\n",
       "      <td>3.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119000</td>\n",
       "      <td>3.678800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119500</td>\n",
       "      <td>3.679000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>3.672200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120500</td>\n",
       "      <td>3.664500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121000</td>\n",
       "      <td>3.643500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121500</td>\n",
       "      <td>3.676300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>3.649900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122500</td>\n",
       "      <td>3.636700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123000</td>\n",
       "      <td>3.652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123500</td>\n",
       "      <td>3.651200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>3.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124500</td>\n",
       "      <td>3.672800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125000</td>\n",
       "      <td>3.664500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125500</td>\n",
       "      <td>3.633600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>3.634400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126500</td>\n",
       "      <td>3.642600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127000</td>\n",
       "      <td>3.645700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127500</td>\n",
       "      <td>3.658100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>3.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128500</td>\n",
       "      <td>3.650900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129000</td>\n",
       "      <td>3.620500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129500</td>\n",
       "      <td>3.672300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>3.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130500</td>\n",
       "      <td>3.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131000</td>\n",
       "      <td>3.615800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131500</td>\n",
       "      <td>3.667500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>3.650500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132500</td>\n",
       "      <td>3.629600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133000</td>\n",
       "      <td>3.642500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133500</td>\n",
       "      <td>3.636100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>3.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134500</td>\n",
       "      <td>3.647200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135000</td>\n",
       "      <td>3.633500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135500</td>\n",
       "      <td>3.648700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>3.672900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136500</td>\n",
       "      <td>3.644700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137000</td>\n",
       "      <td>3.627500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137500</td>\n",
       "      <td>3.612800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>3.642700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138500</td>\n",
       "      <td>3.635900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139000</td>\n",
       "      <td>3.625600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139500</td>\n",
       "      <td>3.611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>3.620800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140500</td>\n",
       "      <td>3.616100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141000</td>\n",
       "      <td>3.616600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141500</td>\n",
       "      <td>3.617400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142000</td>\n",
       "      <td>3.597200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142500</td>\n",
       "      <td>3.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143000</td>\n",
       "      <td>3.609700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143500</td>\n",
       "      <td>3.622200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144000</td>\n",
       "      <td>3.604300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144500</td>\n",
       "      <td>3.599500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145000</td>\n",
       "      <td>3.595100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145500</td>\n",
       "      <td>3.586200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146000</td>\n",
       "      <td>3.594400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146500</td>\n",
       "      <td>3.657200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147000</td>\n",
       "      <td>3.630300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147500</td>\n",
       "      <td>3.614800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148000</td>\n",
       "      <td>3.636200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148500</td>\n",
       "      <td>3.605800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149000</td>\n",
       "      <td>3.612300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149500</td>\n",
       "      <td>3.623400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>3.620400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150500</td>\n",
       "      <td>3.590200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151000</td>\n",
       "      <td>3.609600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151500</td>\n",
       "      <td>3.600400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152000</td>\n",
       "      <td>3.583000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152500</td>\n",
       "      <td>3.626200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153000</td>\n",
       "      <td>3.603100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153500</td>\n",
       "      <td>3.619900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154000</td>\n",
       "      <td>3.641200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154500</td>\n",
       "      <td>3.616100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155000</td>\n",
       "      <td>3.620600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155500</td>\n",
       "      <td>3.623900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156000</td>\n",
       "      <td>3.607400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156500</td>\n",
       "      <td>3.605200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157000</td>\n",
       "      <td>3.587200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157500</td>\n",
       "      <td>3.599800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158000</td>\n",
       "      <td>3.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158500</td>\n",
       "      <td>3.575400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159000</td>\n",
       "      <td>3.574400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159500</td>\n",
       "      <td>3.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>3.603000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160500</td>\n",
       "      <td>3.611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161000</td>\n",
       "      <td>3.566600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161500</td>\n",
       "      <td>3.615100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162000</td>\n",
       "      <td>3.571300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162500</td>\n",
       "      <td>3.581900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163000</td>\n",
       "      <td>3.581500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163500</td>\n",
       "      <td>3.574000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164000</td>\n",
       "      <td>3.578700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164500</td>\n",
       "      <td>3.566000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165000</td>\n",
       "      <td>3.599700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165500</td>\n",
       "      <td>3.584300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166000</td>\n",
       "      <td>3.587500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166500</td>\n",
       "      <td>3.632200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167000</td>\n",
       "      <td>3.598800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167500</td>\n",
       "      <td>3.570100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168000</td>\n",
       "      <td>3.575700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168500</td>\n",
       "      <td>3.608400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169000</td>\n",
       "      <td>3.607100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169500</td>\n",
       "      <td>3.590300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170000</td>\n",
       "      <td>3.575900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170500</td>\n",
       "      <td>3.604300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171000</td>\n",
       "      <td>3.600400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171500</td>\n",
       "      <td>3.592300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172000</td>\n",
       "      <td>3.559900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172500</td>\n",
       "      <td>3.577000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173000</td>\n",
       "      <td>3.594700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173500</td>\n",
       "      <td>3.576800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174000</td>\n",
       "      <td>3.613900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=174110, training_loss=3.7825945418129274, metrics={'train_runtime': 17642.8655, 'train_samples_per_second': 78.949, 'train_steps_per_second': 9.869, 'total_flos': 8.31857984054231e+17, 'train_loss': 3.7825945418129274, 'epoch': 10.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id=\"peft-lora-mt5-goud-results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('peft-lora-mt5-goud-results/tokenizer_config.json',\n",
       " 'peft-lora-mt5-goud-results/special_tokens_map.json',\n",
       " 'peft-lora-mt5-goud-results/spiece.model',\n",
       " 'peft-lora-mt5-goud-results/added_tokens.json',\n",
       " 'peft-lora-mt5-goud-results/tokenizer.json')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8860c96231496a83995df93aa5b870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d8f0fc17824720a34d4e75b778b1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/2.77M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e2bd214de94322ac57fa72945058aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77fa9fe51ed4b2f9b73db57b14eeecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4aec675d46d456d9826e38d3879f121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/alizaidi/lora-mt5-goud/commit/fee52adbeda959a97d9b839e0b8f0a5f315b0713', commit_message='alizaidi/lora-mt5-goud-ar', commit_description='', oid='fee52adbeda959a97d9b839e0b8f0a5f315b0713', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.push_to_hub(\"alizaidi/lora-mt5-goud-ar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alizaidi/micromamba/envs/peft/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  load_in_8bit=True,  device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بالفيديو. طالبة كليات فاس خرجات للاحتجاج على فتح الأحياء والجامعات\n"
     ]
    }
   ],
   "source": [
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "\n",
    "text = dataset[\"test\"][0][\"article\"]\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=128)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'روبورطاج.. البياتة بالليل على برا شكل احتجاجي جديد للمطالبة بفتح الأحياء الجامعية بفاس'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[\"test\"][0][\"headline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
    "    \"\"\"\n",
    "    split the dataset into smaller batches that we can process simultaneously\n",
    "    Yield successive batch-sized chunks from list_of_elements.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "\n",
    "def calculate_metric_on_test_ds(\n",
    "    dataset,\n",
    "    metric,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    batch_size=16,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    column_text=\"article\",\n",
    "    column_summary=\"highlights\",\n",
    "):\n",
    "    article_batches = list(\n",
    "        generate_batch_sized_chunks(dataset[column_text], batch_size)\n",
    "    )\n",
    "    target_batches = list(\n",
    "        generate_batch_sized_chunks(dataset[column_summary], batch_size)\n",
    "    )\n",
    "\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches), total=len(article_batches)\n",
    "    ):\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            article_batch,\n",
    "            max_length=1024,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        summaries = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"].to(device),\n",
    "            attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "            length_penalty=0.8,\n",
    "            num_beams=8,\n",
    "            max_length=128,\n",
    "        )\n",
    "        \"\"\" parameter for length penalty ensures that the model does not generate sequences that are too long. \"\"\"\n",
    "\n",
    "        # Finally, we decode the generated texts,\n",
    "        # replace the  token, and add the decoded texts with the references to the metric.\n",
    "        decoded_summaries = [\n",
    "            tokenizer.decode(\n",
    "                s, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "            )\n",
    "            for s in summaries\n",
    "        ]\n",
    "\n",
    "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "    #  Finally compute and return the ROUGE scores.\n",
    "    score = metric.compute()\n",
    "    return score\n",
    "\n",
    "\n",
    "def evaluation(tokenizer, model, dataset):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # loading data\n",
    "    rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    score = calculate_metric_on_test_ds(\n",
    "        dataset[\"test\"][0:10],\n",
    "        rouge_metric,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        batch_size=2,\n",
    "        column_text=\"article\",\n",
    "        column_summary=\"headline\",\n",
    "    )\n",
    "\n",
    "    rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "\n",
    "    return rouge_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_498243/3742215907.py:70: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge_metric = load_metric(\"rouge\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|                                                                    | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 20%|████████████                                                | 1/5 [00:02<00:08,  2.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 40%|████████████████████████                                    | 2/5 [00:04<00:06,  2.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 60%|████████████████████████████████████                        | 3/5 [00:06<00:04,  2.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 80%|████████████████████████████████████████████████            | 4/5 [00:10<00:02,  2.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "100%|████████████████████████████████████████████████████████████| 5/5 [00:13<00:00,  2.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "100%|████████████████████████████████████████████████████████████| 5/5 [00:13<00:00,  2.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.1, 'rouge2': 0.0, 'rougeL': 0.1, 'rougeLsum': 0.1}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation(tokenizer, model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation1 : point de contrôle précoce du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load an early checkpoint\n",
    "#run evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation2 : modèle final entraîné AraBERT, DarijaBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activité : Résumé d'Article en Langue Maternelle\n",
    "\n",
    "**Tâche :** Rechercher un article dans votre langue maternelle et évaluer les capacités de résumé et de génération de titres de ChatGPT.\n",
    "\n",
    "**Étapes :**\n",
    "\n",
    "1. **Sélectionner un Article :** Choisissez un article pertinent et récent rédigé dans votre langue maternelle. Assurez-vous qu'il soit de longueur courte ou moyenne.\n",
    "\n",
    "2. **Résumer avec ChatGPT :** Utilisez ChatGPT pour générer un résumé de l'article sélectionné.\n",
    "\n",
    "3. **Évaluer la Qualité du Résumé :**\n",
    "    - **Impression :** Partagez votre impression sur la qualité du résumé. Considérez les points suivants :\n",
    "        - **Exactitude :** Le résumé capture-t-il les points principaux et l'essence de l'article ?\n",
    "        - **Clarté :** Le résumé est-il clair et facile à comprendre ?\n",
    "        - **Couverture :** Le résumé inclut-il toutes les informations cruciales de l'article ?\n",
    "\n",
    "4. **Fournir un Retour :** Offrez un retour constructif sur le résumé. Soulignez les divergences éventuelles ou les domaines à améliorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe\n",
       "\tsrc=\"https://forms.gle/sggLJWMFQ4JQCmHL8\",\n",
       "  width=\"80%\"\n",
       "\theight=\"320px\" >\n",
       "\tLoading...\n",
       "</iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Generate Quiz Form. (Run Cell)\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<iframe\n",
    "\tsrc=\"https://forms.gle/sggLJWMFQ4JQCmHL8\",\n",
    "  width=\"80%\"\n",
    "\theight=\"320px\" >\n",
    "\tLoading...\n",
    "</iframe>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section2: Résumé en utilisant GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dans cette section, nous allons utiliser à la fois les grands et petits modèles de langue d'OpenAI pour accomplir la même tâche de résumé textuel abstractive. Nous évaluerons ensuite leurs performances et comparerons les résultats avec ceux obtenus à partir des modèles discutés dans la Section 1.\n",
    "\n",
    "Contrairement aux approches traditionnelles de fine-tuning qui impliquent la mise à jour des poids du modèle, la première étape de l'adaptation d'un modèle basé sur GPT pour une tâche spécifique est l'ingénierie des prompts, qui ne nécessite pas de mise à jour des poids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: flex-start;\">\n",
    "    <figure style=\"margin-right: 10px; text-align: center;\">\n",
    "        <a href=\"https://arxiv.org/pdf/2005.14165\" target=\"_blank\">\n",
    "            <img src=\"./content/traditional-finetuning.png\" width=\"80%\" />\n",
    "        </a>\n",
    "        <figcaption><a href=\"https://arxiv.org/pdf/2005.14165\" target=\"_blank\">Traditional Fine-Tuning</a></figcaption>\n",
    "    </figure>\n",
    "    <figure style= \"text-align: center;\">\n",
    "        <a href=\"https://arxiv.org/pdf/2005.14165\" target=\"_blank\">\n",
    "            <img src=\"./content/prompting.png\" width=\"80%\" />\n",
    "        </a>\n",
    "        <figcaption><a href=\"https://arxiv.org/pdf/2005.14165\" target=\"_blank\">Prompting</a></figcaption>\n",
    "    </figure>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rouge_metric import PyRouge\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import openai\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import os \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions Utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rouge(hypotheses, references):\n",
    "    these_refs = [[ref.strip().lower()] for ref in references]\n",
    "    rouge = PyRouge(rouge_n=(1, 2), rouge_l=True)\n",
    "    scores = rouge.evaluate(hypotheses, these_refs)\n",
    "    print(scores)\n",
    "\n",
    "def substring_after_colon(input_string):\n",
    "    colon_index = input_string.find(':')\n",
    "    if colon_index != -1:\n",
    "        return input_string[colon_index + 1:]\n",
    "    else:\n",
    "        return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset and paths\n",
    "DATASET = \"Goud\"\n",
    "MAX_TRAIN = 0\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "output_filename = f\"./{DATASET}_{model_name}_test_generated_{MAX_TRAIN}.csv\"\n",
    "\n",
    "# Load dataset\n",
    "goud_data = dataset\n",
    "train_source = goud_data[\"train\"][\"article\"]\n",
    "train_target = goud_data[\"train\"][\"headline\"]\n",
    "test_source = goud_data[\"test\"][\"article\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction pour résumer des articles de presse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_news_article(MAX_TRAIN=20):\n",
    "       \n",
    "    client = OpenAI(api_key=key)\n",
    "    rewritten_prompt_count = 0\n",
    "    line_count = 0\n",
    "    wait_time = 1\n",
    "    df_lines = []\n",
    "    tokens_consumption = 0\n",
    "    existing_len = 0\n",
    "    if os.path.exists(output_filename):\n",
    "        existing_df = pd.read_csv(output_filename)\n",
    "        existing_len = existing_df.shape[0]\n",
    "        rewritten_prompt_count = existing_len\n",
    "        line_count = existing_len\n",
    "        df_lines = existing_df.to_dict('records')\n",
    "    \n",
    "    for data in tqdm(test_source[existing_len:], desc=f\"Lines processed from {existing_len}-th line\"):\n",
    "        news_article = data.strip()\n",
    "        line_count += 1\n",
    "        made_error = True\n",
    "        num_error = 0\n",
    "        while made_error:\n",
    "            messages = [{\"role\": \"system\", \"content\": \"You are asked to summarize a news article written in Modern Standard Arabic and Moroccan Darija, and write that summary as a clickbait headline, in Moroccan Darija only.\\n\"}]\n",
    "            if MAX_TRAIN - num_error > 0:\n",
    "                for _ in range(MAX_TRAIN - num_error):\n",
    "                    idx = random.choice(range(len(train_source)))\n",
    "                    train_src = train_source[idx]\n",
    "                    train_tgt = train_target[idx]\n",
    "                    messages.append({\"role\": \"user\", \"content\": f\"Summarize the following news article into a headline in Moroccan Darija only:\\n\\\"{train_src}\\\"\"})\n",
    "                    messages.append({\"role\": \"assistant\", \"content\": f\"Absolutely! Here is the headline summarizing your news article:\\n\\\"{train_tgt}\\\"\"})\n",
    "            messages.append({\"role\": \"user\", \"content\": f\"Summarize the following news article into a headline in Moroccan Darija only:\\n\\\"{news_article}\\\"\"})\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    messages=messages,\n",
    "                    model=model_name,\n",
    "                )\n",
    "                headline = response.choices[0].message.content\n",
    "                df_lines.append({\"article\": news_article,\"generated_headline\": headline,\"prompt_messages\":messages})\n",
    "\n",
    "                rewritten_prompt_count += 1\n",
    "                made_error = False\n",
    "            except Exception as e:\n",
    "                if isinstance(e, openai.RateLimitError):\n",
    "                    print(\"Rate limit error\")\n",
    "                    print(f\"Wait for {wait_time} seconds because all calls failed: \", flush=True)\n",
    "                    time.sleep(wait_time)\n",
    "                    wait_time *= 2\n",
    "                else:\n",
    "                    print(e)\n",
    "                    num_error += 1\n",
    "                    print(\"May be too long, reducing context to:\", MAX_TRAIN - num_error)\n",
    "            #time.sleep(1)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(df_lines)\n",
    "    df.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exécuter la synthèse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lines processed from 0-th line: 100%|██████████| 9497/9497 [2:34:22<00:00,  1.03it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 9263.82370686531 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#record cell running time\n",
    "import time\n",
    "start_time = time.time()\n",
    "summarize_news_article(0)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger le résultat généré et évaluer le ROUGE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le dossier \"generated_responses\", vous trouverez les réponses gpt correspondant aux invites de 0, 1, 5, 20 coups.\n",
    "\n",
    "Évaluez les résumés de titres générés en exécutant l'évaluation ROUGE et ajoutez les résultats au tableau des résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résultats du Métrique ROUGE : 0 Shot\n",
    "\n",
    "| Métrique | Rappel (r)         | Précision (p)      | Score F1 (f)      |\n",
    "|----------|-------------------|-------------------|-------------------|\n",
    "| ROUGE-1  | 0.1228            | 0.1069            | 0.1143            |\n",
    "| ROUGE-2  | 0.0282            | 0.0235            | 0.0256            |\n",
    "| ROUGE-L  | 0.1128            | 0.0980            | 0.1049            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'r': 0.11884854492859614, 'p': 0.12977184865821972, 'f': 0.12407023545586798}, 'rouge-2': {'r': 0.0325784137233658, 'p': 0.03389945881811894, 'f': 0.03322581039836068}, 'rouge-l': {'r': 0.11128523451125509, 'p': 0.12142149826298465, 'f': 0.11613260817866634}}\n"
     ]
    }
   ],
   "source": [
    "shot_count = 0\n",
    "hypotheses = pd.read_csv(f\".\\generated_responses\\{model_name}\\Goud_{model_name}_test_generated_{str(shot_count)}.csv\", encoding = \"UTF-8\")[\"generated_headline\"].tolist()\n",
    "hypotheses = [substring_after_colon(hypo).replace(\"\\\"\", \"\").strip() for hypo in hypotheses]\n",
    "references = goud_data[\"test\"][\"headline\"]\n",
    "evaluate_rouge(hypotheses, references)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résultats du Metric ROUGE: 20 Exemple\n",
    "Choisissez un ou plusieurs des fichiers contenant les réponses GPT N-shot précédemment générées, présents dans le dossier \"generated_responses\", exécutez l'évaluation, puis remplissez le tableau ci-dessous\n",
    "\n",
    "| Metric   | Rappel (r)        | Précision (p)     | Score-F (f)       |\n",
    "|----------|-------------------|-------------------|-------------------|\n",
    "| ROUGE-1  |                   |                   |                   |\n",
    "| ROUGE-2  |                   |                   |                   |\n",
    "| ROUGE-L  |                   |                   |                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'r': 0.13750397869020445, 'p': 0.1306205269799418, 'f': 0.13397389480280544}, 'rouge-2': {'r': 0.03387859852289136, 'p': 0.03194819653833152, 'f': 0.032885092553751126}, 'rouge-l': {'r': 0.1264128630910928, 'p': 0.11986076103165903, 'f': 0.12304965282629712}}\n"
     ]
    }
   ],
   "source": [
    "shot_count = 20\n",
    "model_name  = \"gpt4\"  #\"C:\\Users\\salamaaya\\OneDrive - Microsoft\\Desktop\\DLI\\Indaba2024-practical\\indaba-low-resource-nlp-prac\\generated_responses\\gpt4\\Goud_test_generated_5.csv\"\n",
    "hypotheses = pd.read_csv(f\".\\generated_responses\\{model_name}\\Goud_test_generated_{str(shot_count)}.csv\", encoding = \"UTF-8\")[\"generated_headline\"].tolist()\n",
    "hypotheses = [substring_after_colon(hypo).replace(\"\\\"\", \"\").strip() for hypo in hypotheses]\n",
    "references = goud_data[\"test\"][\"headline\"]\n",
    "evaluate_rouge(hypotheses, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Generate Quiz Form. (Run Cell)\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<iframe\n",
    "\tsrc=\"https://forms.gle/zbJoTSz3nfYq1VrY6\",\n",
    "  width=\"80%\"\n",
    "\theight=\"1200px\" >\n",
    "\tLoading...\n",
    "</iframe>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "**Résumé :**\n",
    "\n",
    "[Résumé des points principaux/à retenir de la pratique.]\n",
    "\n",
    "**Prochaines Étapes :**\n",
    "\n",
    "[Prochaines étapes pour les personnes ayant terminé la pratique, comme des lectures optionnelles (par exemple, blogs, articles, cours, vidéos YouTube). Cela pourrait également renvoyer à d'autres pratiques.]\n",
    "\n",
    "**Annexe :**\n",
    "\n",
    "[Tout ce qui (probablement des trucs mathématiques lourds) ne trouve pas de place dans les sections pratiques principales.]\n",
    "\n",
    "**Références :**\n",
    "\n",
    "[Références pour tout contenu utilisé dans le notebook.]\n",
    "\n",
    "Pour d'autres pratiques du Deep Learning Indaba, veuillez visiter [ici](https://github.com/deep-learning-indaba/indaba-pracs-2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Donnez-nous votre avis sur notre session!\n",
    "\n",
    "Veuillez fournir des retours que nous pouvons utiliser pour améliorer nos travaux pratiques à l'avenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe\n",
       "\tsrc=\"https://forms.gle/WUpRupqfhFtbLXtN6\",\n",
       "  width=\"80%\"\n",
       "\theight=\"1200px\" >\n",
       "\tLoading...\n",
       "</iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Generate Feedback Form. (Run Cell)\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<iframe\n",
    "\tsrc=\"https://forms.gle/WUpRupqfhFtbLXtN6\",\n",
    "  width=\"80%\"\n",
    "\theight=\"1200px\" >\n",
    "\tLoading...\n",
    "</iframe>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "WILOYJH4gCnD"
   ],
   "name": "Indaba_2022_Prac_Template.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "145833166d986a8417df3c7acb65d917d84b716b5a452e57fcacdc66f1a168c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
