{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# **Low-resource NLP**\n",
        "\n",
        "<img src=\"./content/lr_llm_header.png\" width=\"60%\" allign =\"center\"/>\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2024/blob/main/practicals/Indaba_2024_Prac_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [Change colab link to point to prac.]\n",
        "\n",
        "Â© Deep Learning Indaba 2024. Apache License 2.0.\n",
        "\n",
        "**Authors:**\n",
        "- Ali Zaidi\n",
        "- Aya Salama\n",
        "- Khalil Mrini\n",
        "- Steven Kolawole \n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "Low-resource NLP (Natural Language Processing) refers to the study and development of NLP models and systems for languages, tasks, or domains that have limited data and resources available. These can include languages with fewer digital text corpora, limited computational tools, or less-developed linguistic research.\n",
        "\n",
        "**Key Challenges in Low-Resource NLP**\n",
        "\n",
        "1. **Data Scarcity:**\n",
        "   - **Limited Training Data:** Many languages lack large annotated corpora necessary for training NLP models.\n",
        "   - **Lack of Pre-trained Models:** Popular NLP models like BERT, GPT, and others are often not available for low-resource languages.\n",
        "\n",
        "2. **Linguistic Diversity:**\n",
        "   - **Morphological Complexity:** Some languages have complex grammatical structures and morphological richness.\n",
        "   - **Dialectal Variations:** A lack of standardized versions can complicate NLP tasks.\n",
        "\n",
        "3. **Resource Limitations:**\n",
        "   - **Computational Constraints:** Low-resource scenarios often involve limited access to computational power and storage.\n",
        "   - **Expertise and Tools:** Fewer linguistic experts and fewer NLP tools are tailored for these languages.\n",
        "\n",
        "**Topics:**\n",
        "\n",
        "Content: [Natural Language Processing, Large Language Models,Parameter Efficient Finetuning, Adaptation]  \n",
        "Level: [Intermediate]\n",
        "\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "- Exploring data scarcity challenges\n",
        "- Exploring Compute resource limitations\n",
        "- Comparing SOTA LLM Performance on low-resource languages/tasks (depending on which dataset we will end up using )\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "[Knowledge required for this prac. You can link a relevant parallel track session, blogs, papers, courses, topics etc.]\n",
        "\n",
        "**Outline:**\n",
        "\n",
        "[Points that link to each section. Auto-generate following the instructions [here](https://stackoverflow.com/questions/67458990/how-to-automatically-generate-a-table-of-contents-in-colab-notebook).]\n",
        "\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "[Tasks just before starting.]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Storyline: working on a task with scarce data which is summarization in Moroccan Darija.\n",
        "this task pose resources constrainst as the Moroccan Darija can be considered a low resource dialect of Arabic, we will set this task in a coumputational resource poor environment so our training should be able to run on a commodity GPU \n",
        "we will be using parameter efficient fine tuning technique (LORA) to optimize the training procedure in order to make it feasible "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installation and Imports [should download any needed resources]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install arabert\n",
        "!pip install accelerate -U\n",
        "!pip install transformers[torch]\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#download the datset\n",
        "#download the model checkpoints\n",
        "#download the GPT outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task and Dataset Overiew"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this practical we are interested in generating headlines for news articles featured on the news website [Goud.ma](www.Gound.ma).\n",
        "\n",
        "We will frame this as a summarization task where the input is the body of a news article and the output is an appropriate headline. The [Goud dataset](https://github.com/issam9/goud-summarization-dataset) contains 158k articles and their headlines. All headlines are in Moroccan Darija, while articles may be in Moroccan Darija, in Modern Standard Arabic, or a mix of both (code-switched Moroccan Darija).\n",
        "\n",
        "**Data Fields**\n",
        "- *article*: a string containing the body of the news article\n",
        "- *headline*: a string containing the article's headline\n",
        "- *categories*: a list of string of article categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What we will do:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./content/DLI_LR_llm_prac_1.png\" width=\"40%\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Metric: ROUGE\n",
        "\n",
        "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate the quality of summaries by comparing them to reference (or ground truth) summaries. ROUGE is widely used in Natural Language Processing (NLP) tasks, particularly for evaluating the performance of text summarization models.\n",
        "\n",
        "### Key ROUGE Variants\n",
        "\n",
        "1. **ROUGE-N**: Measures the overlap of n-grams between the candidate summary and the reference summary.\n",
        "   - **ROUGE-1**: Overlap of unigrams (1-gram).\n",
        "   - **ROUGE-2**: Overlap of bigrams (2-grams).\n",
        "   - **ROUGE-L**: Measures the longest common subsequence (LCS) between the candidate and reference summaries.\n",
        "\n",
        "2. **ROUGE-L**: Measures the longest common subsequence (LCS) between the candidate summary and the reference summary. Unlike ROUGE-N, ROUGE-L considers sentence-level structure similarity by identifying the longest co-occurring sequence of words in both summaries.\n",
        "\n",
        "3. **ROUGE-W**: A weighted version of ROUGE-L that gives more importance to the contiguous LCS.\n",
        "\n",
        "4. **ROUGE-S**: Measures the overlap of skip-bigrams, which are pairs of words in their order of appearance that can have any number of gaps between them.\n",
        "\n",
        "### How ROUGE is Computed\n",
        "\n",
        "ROUGE metrics can be calculated in terms of three measures:\n",
        "\n",
        "- **Recall**: The ratio of overlapping units (n-grams, LCS, or skip-bigrams) between the candidate summary and the reference summary to the total units in the reference summary. It answers, \"How much of the reference summary is captured by the candidate summary?\"\n",
        "\n",
        "- **Precision**: The ratio of overlapping units between the candidate summary and the reference summary to the total units in the candidate summary. It answers, \"How much of the candidate summary is relevant to the reference summary?\"\n",
        "\n",
        "- **F1-Score**: The harmonic mean of Precision and Recall. This gives a balanced measure that considers both precision and recall.\n",
        "\n",
        "### Importance of ROUGE\n",
        "\n",
        "ROUGE is essential for summarization tasks because it provides a standardized way to evaluate and compare different summarization models. Higher ROUGE scores generally indicate that the candidate summary is more similar to the reference summary, meaning the model is likely performing well.\n",
        "\n",
        "However, while ROUGE is widely used, it's not without its limitations. It primarily measures lexical overlap and may not fully capture the semantic meaning or quality of a summary. As such, it's often used in conjunction with human evaluations for a more comprehensive assessment of summary quality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('Goud/Goud-sum')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Data Exploration\n",
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section1: Efficiently Fine-Tune Seq2Seq Models with Low Rank Adaptation (LoRA)\n",
        "We are going to leverage Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), and [PEFT](https://github.com/huggingface/peft). \n",
        "\n",
        "You will learn how to:\n",
        "\n",
        "1. Setup Development Environment\n",
        "2. Load and prepare the dataset\n",
        "3. Fine-Tune Multilingual BERT with LoRA and bnb int-8\n",
        "4. Evaluate & run Inference\n",
        "5. Cost performance comparison\n",
        "\n",
        "### Quick intro to PEFT or Parameter Efficient Fine-tuning\n",
        "<img src=\"./content/PEFT_method.png\" width=\"60%\" />\n",
        "\n",
        "[PEFT](https://github.com/huggingface/peft), or Parameter Efficient Fine-tuning, is a new open-source library from Hugging Face to enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. PEFT currently includes techniques for:\n",
        "\n",
        "- LoRA:Â [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
        "- Prefix Tuning:Â [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
        "- P-Tuning:Â [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
        "- Prompt Tuning:Â [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summarization Using MT5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/alizaidi/micromamba/envs/peft/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The maximum total input sequence length after tokenization.\n",
        "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
        "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"article\"], truncation=True), batched=True, remove_columns=[\"categories\", \"headline\"])\n",
        "input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max source length: 571\n"
          ]
        }
      ],
      "source": [
        "# take 85 percentile of max length for better utilization\n",
        "max_source_length = int(np.percentile(input_lenghts, 85))\n",
        "print(f\"Max source length: {max_source_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max target length: 50\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# The maximum total sequence length for target text after tokenization.\n",
        "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
        "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"headline\"], truncation=True), batched=True, remove_columns=[\"article\", \"categories\"])\n",
        "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
        "# take 90 percentile of max length for better utilization\n",
        "max_target_length = int(np.percentile(target_lenghts, 90))\n",
        "print(f\"Max target length: {max_target_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5c26cd20d7543378e96621f3c0eed9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/9497 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
          ]
        }
      ],
      "source": [
        "def preprocess_function(sample, padding=\"max_length\"):\n",
        "    # # add prefix to the input for t5\n",
        "    # inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
        "\n",
        "    # tokenize inputs\n",
        "    model_inputs = tokenizer(sample['article'], max_length=max_source_length, padding=padding, truncation=True)\n",
        "\n",
        "    # Tokenize targets with the `summary` keyword argument\n",
        "    labels = tokenizer(sample[\"headline\"], max_length=max_target_length, padding=padding, truncation=True)\n",
        "\n",
        "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
        "    # padding in the loss.\n",
        "    if padding == \"max_length\":\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"headline\", \"article\", \"categories\"])\n",
        "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab6504b1fa594b5198ceec4e53aef708",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/9497 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_dataset[\"test\"].save_to_disk(\"arabic-goud-data/eval\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PeftConfig, PeftModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 688,128 || all params: 300,864,896 || trainable%: 0.2287\n"
          ]
        }
      ],
      "source": [
        "lora_config = LoraConfig(\n",
        "  r=16,\n",
        "  lora_alpha=32,\n",
        "  target_modules=[\"q\", \"v\"],\n",
        "  lora_dropout=0.05,\n",
        "  bias=\"none\",\n",
        "  task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")\n",
        "# prepare int-8 model for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# add LoRA adaptor\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we want to ignore tokenizer pad token in the loss\n",
        "label_pad_token_id = -100\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=label_pad_token_id,\n",
        "    pad_to_multiple_of=8,\n",
        "    return_tensors='pt'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 10\n",
        "output_dir = \"lora-goud-mt5-small\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define training args\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"lora-mt5-goud\",\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=500,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset = tokenized_dataset[\"train\"],\n",
        "    eval_dataset = tokenized_dataset[\"validation\"].select(range(20)),\n",
        "    tokenizer = tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='174110' max='174110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [174110/174110 4:53:13, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>6.359000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>4.946300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>4.726200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>4.576100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>4.479900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>4.444000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>4.379300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>4.369500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>4.317400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>4.310300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>4.279000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>4.280200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>4.215100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>4.190200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>4.182300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>4.174800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>4.180200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>4.149700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>4.105200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>4.121100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>4.110700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>4.098900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>4.092600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>4.067900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>4.081800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>4.055000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>4.046200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>4.012600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>4.034700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>4.034300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>4.005400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>4.018000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>4.034400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>4.056800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>3.989400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>3.981100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>4.014700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>3.963600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>3.956200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>3.996800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>3.972200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>3.955100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>3.963700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>4.005800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>3.949900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>3.935000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>3.944400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>3.939000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>3.925900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>3.940500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>3.931800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>3.952100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>3.911500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>3.908300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>3.971000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>3.907800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>3.904700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>3.935800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29500</td>\n",
              "      <td>3.908500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>3.939600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30500</td>\n",
              "      <td>3.904400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>3.879800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31500</td>\n",
              "      <td>3.889600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>3.898500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32500</td>\n",
              "      <td>3.893600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>3.863100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33500</td>\n",
              "      <td>3.912100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34000</td>\n",
              "      <td>3.916100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34500</td>\n",
              "      <td>3.885500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35000</td>\n",
              "      <td>3.891300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35500</td>\n",
              "      <td>3.876000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36000</td>\n",
              "      <td>3.897200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36500</td>\n",
              "      <td>3.876800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37000</td>\n",
              "      <td>3.847700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37500</td>\n",
              "      <td>3.854600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38000</td>\n",
              "      <td>3.864100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38500</td>\n",
              "      <td>3.872300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39000</td>\n",
              "      <td>3.857700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39500</td>\n",
              "      <td>3.859300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40000</td>\n",
              "      <td>3.864500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40500</td>\n",
              "      <td>3.822400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41000</td>\n",
              "      <td>3.874400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41500</td>\n",
              "      <td>3.874100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42000</td>\n",
              "      <td>3.857600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42500</td>\n",
              "      <td>3.868900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43000</td>\n",
              "      <td>3.821800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43500</td>\n",
              "      <td>3.840500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44000</td>\n",
              "      <td>3.827700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44500</td>\n",
              "      <td>3.808200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45000</td>\n",
              "      <td>3.868900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45500</td>\n",
              "      <td>3.833800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46000</td>\n",
              "      <td>3.859500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46500</td>\n",
              "      <td>3.824000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47000</td>\n",
              "      <td>3.850600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47500</td>\n",
              "      <td>3.811700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48000</td>\n",
              "      <td>3.835200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48500</td>\n",
              "      <td>3.840200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49000</td>\n",
              "      <td>3.833200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49500</td>\n",
              "      <td>3.842900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50000</td>\n",
              "      <td>3.854100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50500</td>\n",
              "      <td>3.809700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51000</td>\n",
              "      <td>3.808300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51500</td>\n",
              "      <td>3.829700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52000</td>\n",
              "      <td>3.817500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52500</td>\n",
              "      <td>3.811000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53000</td>\n",
              "      <td>3.823900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53500</td>\n",
              "      <td>3.811000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54000</td>\n",
              "      <td>3.809800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54500</td>\n",
              "      <td>3.806200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55000</td>\n",
              "      <td>3.779400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55500</td>\n",
              "      <td>3.802500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56000</td>\n",
              "      <td>3.804900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56500</td>\n",
              "      <td>3.800700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57000</td>\n",
              "      <td>3.812000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57500</td>\n",
              "      <td>3.796400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58000</td>\n",
              "      <td>3.806900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58500</td>\n",
              "      <td>3.791400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59000</td>\n",
              "      <td>3.775000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59500</td>\n",
              "      <td>3.789100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60000</td>\n",
              "      <td>3.779800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60500</td>\n",
              "      <td>3.806400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61000</td>\n",
              "      <td>3.783600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61500</td>\n",
              "      <td>3.811300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62000</td>\n",
              "      <td>3.802300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62500</td>\n",
              "      <td>3.800900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63000</td>\n",
              "      <td>3.802800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63500</td>\n",
              "      <td>3.804900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64000</td>\n",
              "      <td>3.780800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64500</td>\n",
              "      <td>3.771900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65000</td>\n",
              "      <td>3.794500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65500</td>\n",
              "      <td>3.773400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66000</td>\n",
              "      <td>3.802500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66500</td>\n",
              "      <td>3.762100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67000</td>\n",
              "      <td>3.769300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67500</td>\n",
              "      <td>3.767300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68000</td>\n",
              "      <td>3.808200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68500</td>\n",
              "      <td>3.757000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69000</td>\n",
              "      <td>3.748400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69500</td>\n",
              "      <td>3.760500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70000</td>\n",
              "      <td>3.781700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70500</td>\n",
              "      <td>3.757500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71000</td>\n",
              "      <td>3.746300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71500</td>\n",
              "      <td>3.747300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72000</td>\n",
              "      <td>3.765600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72500</td>\n",
              "      <td>3.760900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73000</td>\n",
              "      <td>3.728800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73500</td>\n",
              "      <td>3.734100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74000</td>\n",
              "      <td>3.762600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74500</td>\n",
              "      <td>3.764300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75000</td>\n",
              "      <td>3.736300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75500</td>\n",
              "      <td>3.747700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76000</td>\n",
              "      <td>3.757800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76500</td>\n",
              "      <td>3.744400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77000</td>\n",
              "      <td>3.738500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77500</td>\n",
              "      <td>3.735100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78000</td>\n",
              "      <td>3.742700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78500</td>\n",
              "      <td>3.769400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79000</td>\n",
              "      <td>3.768200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79500</td>\n",
              "      <td>3.742400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80000</td>\n",
              "      <td>3.750900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80500</td>\n",
              "      <td>3.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81000</td>\n",
              "      <td>3.754000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81500</td>\n",
              "      <td>3.731900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82000</td>\n",
              "      <td>3.755400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82500</td>\n",
              "      <td>3.728100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83000</td>\n",
              "      <td>3.751300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83500</td>\n",
              "      <td>3.728600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84000</td>\n",
              "      <td>3.761400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84500</td>\n",
              "      <td>3.735600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85000</td>\n",
              "      <td>3.758800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85500</td>\n",
              "      <td>3.740100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86000</td>\n",
              "      <td>3.732300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86500</td>\n",
              "      <td>3.723900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87000</td>\n",
              "      <td>3.747800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87500</td>\n",
              "      <td>3.697400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88000</td>\n",
              "      <td>3.727300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88500</td>\n",
              "      <td>3.720000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89000</td>\n",
              "      <td>3.704600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89500</td>\n",
              "      <td>3.708600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90000</td>\n",
              "      <td>3.734100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90500</td>\n",
              "      <td>3.744600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91000</td>\n",
              "      <td>3.706500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91500</td>\n",
              "      <td>3.699500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92000</td>\n",
              "      <td>3.719100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92500</td>\n",
              "      <td>3.750200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93000</td>\n",
              "      <td>3.709500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93500</td>\n",
              "      <td>3.713400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94000</td>\n",
              "      <td>3.721600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94500</td>\n",
              "      <td>3.679000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95000</td>\n",
              "      <td>3.715200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95500</td>\n",
              "      <td>3.709100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96000</td>\n",
              "      <td>3.711500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96500</td>\n",
              "      <td>3.733000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97000</td>\n",
              "      <td>3.671600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97500</td>\n",
              "      <td>3.691100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98000</td>\n",
              "      <td>3.713300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98500</td>\n",
              "      <td>3.686100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99000</td>\n",
              "      <td>3.720900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99500</td>\n",
              "      <td>3.738000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100000</td>\n",
              "      <td>3.711800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100500</td>\n",
              "      <td>3.708900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101000</td>\n",
              "      <td>3.725700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101500</td>\n",
              "      <td>3.686400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102000</td>\n",
              "      <td>3.684000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102500</td>\n",
              "      <td>3.697700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103000</td>\n",
              "      <td>3.666300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103500</td>\n",
              "      <td>3.707000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104000</td>\n",
              "      <td>3.708300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104500</td>\n",
              "      <td>3.677200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105000</td>\n",
              "      <td>3.694500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105500</td>\n",
              "      <td>3.697400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106000</td>\n",
              "      <td>3.694600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106500</td>\n",
              "      <td>3.681200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107000</td>\n",
              "      <td>3.688100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107500</td>\n",
              "      <td>3.680400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108000</td>\n",
              "      <td>3.657200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108500</td>\n",
              "      <td>3.668800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109000</td>\n",
              "      <td>3.662700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109500</td>\n",
              "      <td>3.686500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110000</td>\n",
              "      <td>3.686500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110500</td>\n",
              "      <td>3.681200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111000</td>\n",
              "      <td>3.693100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111500</td>\n",
              "      <td>3.667900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112000</td>\n",
              "      <td>3.691000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112500</td>\n",
              "      <td>3.666900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113000</td>\n",
              "      <td>3.686900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113500</td>\n",
              "      <td>3.676900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114000</td>\n",
              "      <td>3.680900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114500</td>\n",
              "      <td>3.649900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115000</td>\n",
              "      <td>3.650600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115500</td>\n",
              "      <td>3.658600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116000</td>\n",
              "      <td>3.673600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116500</td>\n",
              "      <td>3.644400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117000</td>\n",
              "      <td>3.674100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117500</td>\n",
              "      <td>3.643800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118000</td>\n",
              "      <td>3.670400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118500</td>\n",
              "      <td>3.693900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119000</td>\n",
              "      <td>3.678000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119500</td>\n",
              "      <td>3.675600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120000</td>\n",
              "      <td>3.665200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120500</td>\n",
              "      <td>3.658700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121000</td>\n",
              "      <td>3.644300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121500</td>\n",
              "      <td>3.675000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122000</td>\n",
              "      <td>3.639900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122500</td>\n",
              "      <td>3.632800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123000</td>\n",
              "      <td>3.650300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123500</td>\n",
              "      <td>3.656400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124000</td>\n",
              "      <td>3.641300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124500</td>\n",
              "      <td>3.672600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125000</td>\n",
              "      <td>3.660000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125500</td>\n",
              "      <td>3.633200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126000</td>\n",
              "      <td>3.633600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126500</td>\n",
              "      <td>3.639800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127000</td>\n",
              "      <td>3.644300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127500</td>\n",
              "      <td>3.652800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128000</td>\n",
              "      <td>3.622000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128500</td>\n",
              "      <td>3.647700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129000</td>\n",
              "      <td>3.618600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129500</td>\n",
              "      <td>3.669200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130000</td>\n",
              "      <td>3.663900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130500</td>\n",
              "      <td>3.612300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131000</td>\n",
              "      <td>3.617100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131500</td>\n",
              "      <td>3.668600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132000</td>\n",
              "      <td>3.643500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132500</td>\n",
              "      <td>3.628100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133000</td>\n",
              "      <td>3.640400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133500</td>\n",
              "      <td>3.634500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134000</td>\n",
              "      <td>3.653000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134500</td>\n",
              "      <td>3.644700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135000</td>\n",
              "      <td>3.633600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135500</td>\n",
              "      <td>3.645900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136000</td>\n",
              "      <td>3.672400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136500</td>\n",
              "      <td>3.645700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137000</td>\n",
              "      <td>3.627900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137500</td>\n",
              "      <td>3.604900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138000</td>\n",
              "      <td>3.638000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138500</td>\n",
              "      <td>3.634400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139000</td>\n",
              "      <td>3.622600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139500</td>\n",
              "      <td>3.606300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140000</td>\n",
              "      <td>3.615000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140500</td>\n",
              "      <td>3.615600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141000</td>\n",
              "      <td>3.618500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141500</td>\n",
              "      <td>3.611900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142000</td>\n",
              "      <td>3.597900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142500</td>\n",
              "      <td>3.620300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143000</td>\n",
              "      <td>3.605200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143500</td>\n",
              "      <td>3.622500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144000</td>\n",
              "      <td>3.598600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144500</td>\n",
              "      <td>3.598100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145000</td>\n",
              "      <td>3.591600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145500</td>\n",
              "      <td>3.582100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146000</td>\n",
              "      <td>3.592300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146500</td>\n",
              "      <td>3.659300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147000</td>\n",
              "      <td>3.626500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147500</td>\n",
              "      <td>3.611800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148000</td>\n",
              "      <td>3.632100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148500</td>\n",
              "      <td>3.601500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149000</td>\n",
              "      <td>3.613500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149500</td>\n",
              "      <td>3.623900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150000</td>\n",
              "      <td>3.615900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150500</td>\n",
              "      <td>3.588100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151000</td>\n",
              "      <td>3.605800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151500</td>\n",
              "      <td>3.600600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152000</td>\n",
              "      <td>3.582900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152500</td>\n",
              "      <td>3.622700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153000</td>\n",
              "      <td>3.600100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153500</td>\n",
              "      <td>3.617200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154000</td>\n",
              "      <td>3.636600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154500</td>\n",
              "      <td>3.614700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155000</td>\n",
              "      <td>3.619600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155500</td>\n",
              "      <td>3.618300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156000</td>\n",
              "      <td>3.604400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156500</td>\n",
              "      <td>3.601800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157000</td>\n",
              "      <td>3.581600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157500</td>\n",
              "      <td>3.598300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158000</td>\n",
              "      <td>3.597700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158500</td>\n",
              "      <td>3.578900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159000</td>\n",
              "      <td>3.573900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159500</td>\n",
              "      <td>3.595400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160000</td>\n",
              "      <td>3.603600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160500</td>\n",
              "      <td>3.610700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161000</td>\n",
              "      <td>3.559900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161500</td>\n",
              "      <td>3.610600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162000</td>\n",
              "      <td>3.567900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162500</td>\n",
              "      <td>3.578700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163000</td>\n",
              "      <td>3.580100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163500</td>\n",
              "      <td>3.569900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164000</td>\n",
              "      <td>3.575000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164500</td>\n",
              "      <td>3.558700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165000</td>\n",
              "      <td>3.596300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165500</td>\n",
              "      <td>3.585100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166000</td>\n",
              "      <td>3.587100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166500</td>\n",
              "      <td>3.624000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167000</td>\n",
              "      <td>3.596600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167500</td>\n",
              "      <td>3.569500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168000</td>\n",
              "      <td>3.577000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168500</td>\n",
              "      <td>3.606800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169000</td>\n",
              "      <td>3.602000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169500</td>\n",
              "      <td>3.585600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170000</td>\n",
              "      <td>3.574000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170500</td>\n",
              "      <td>3.603400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171000</td>\n",
              "      <td>3.600400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171500</td>\n",
              "      <td>3.589800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172000</td>\n",
              "      <td>3.555300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172500</td>\n",
              "      <td>3.576900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173000</td>\n",
              "      <td>3.593600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173500</td>\n",
              "      <td>3.573500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174000</td>\n",
              "      <td>3.611700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=174110, training_loss=3.781245626474014, metrics={'train_runtime': 17594.0127, 'train_samples_per_second': 79.168, 'train_steps_per_second': 9.896, 'total_flos': 8.31857984054231e+17, 'train_loss': 3.781245626474014, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "peft_model_id=\"peft-lora-mt5-goud-results\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('peft-lora-mt5-goud-results/tokenizer_config.json',\n",
              " 'peft-lora-mt5-goud-results/special_tokens_map.json',\n",
              " 'peft-lora-mt5-goud-results/spiece.model',\n",
              " 'peft-lora-mt5-goud-results/added_tokens.json',\n",
              " 'peft-lora-mt5-goud-results/tokenizer.json')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.model.save_pretrained(peft_model_id)\n",
        "tokenizer.save_pretrained(peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = PeftConfig.from_pretrained(peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/alizaidi/micromamba/envs/peft/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# load base LLM model and tokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  load_in_8bit=True,  device_map={\"\":0})\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ÙØ³ÙØ³Ù Ø§ÙØºØ¶Ø¨ Ø¨Ø¬Ø§ÙØ¹Ø© Ø¸ÙØ± Ø§ÙÙÙØ±Ø§Ø².. Ø·Ø§ÙØ¨Ø© ÙÙÙØ§Øª ÙØ§Ø³ Ø®Ø±Ø¬Ø§Øª ÙÙØ§Ø­ØªØ¬Ø§Ø¬Ø§Øª\n"
          ]
        }
      ],
      "source": [
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
        "\n",
        "text = dataset[\"test\"][0][\"article\"]\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=128)\n",
        "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Ø±ÙØ¨ÙØ±Ø·Ø§Ø¬.. Ø§ÙØ¨ÙØ§ØªØ© Ø¨Ø§ÙÙÙÙ Ø¹ÙÙ Ø¨Ø±Ø§ Ø´ÙÙ Ø§Ø­ØªØ¬Ø§Ø¬Ù Ø¬Ø¯ÙØ¯ ÙÙÙØ·Ø§ÙØ¨Ø© Ø¨ÙØªØ­ Ø§ÙØ£Ø­ÙØ§Ø¡ Ø§ÙØ¬Ø§ÙØ¹ÙØ© Ø¨ÙØ§Ø³'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset[\"test\"][0][\"headline\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from datasets import load_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
        "    \"\"\"\n",
        "    split the dataset into smaller batches that we can process simultaneously\n",
        "    Yield successive batch-sized chunks from list_of_elements.\n",
        "    \"\"\"\n",
        "    for i in range(0, len(list_of_elements), batch_size):\n",
        "        yield list_of_elements[i : i + batch_size]\n",
        "\n",
        "\n",
        "def calculate_metric_on_test_ds(\n",
        "    dataset,\n",
        "    metric,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    batch_size=16,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    column_text=\"article\",\n",
        "    column_summary=\"highlights\",\n",
        "):\n",
        "    article_batches = list(\n",
        "        generate_batch_sized_chunks(dataset[column_text], batch_size)\n",
        "    )\n",
        "    target_batches = list(\n",
        "        generate_batch_sized_chunks(dataset[column_summary], batch_size)\n",
        "    )\n",
        "\n",
        "    for article_batch, target_batch in tqdm(\n",
        "        zip(article_batches, target_batches), total=len(article_batches)\n",
        "    ):\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            article_batch,\n",
        "            max_length=1024,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        summaries = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"].to(device),\n",
        "            attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "            length_penalty=0.8,\n",
        "            num_beams=8,\n",
        "            max_length=128,\n",
        "        )\n",
        "        \"\"\" parameter for length penalty ensures that the model does not generate sequences that are too long. \"\"\"\n",
        "\n",
        "        # Finally, we decode the generated texts,\n",
        "        # replace the  token, and add the decoded texts with the references to the metric.\n",
        "        decoded_summaries = [\n",
        "            tokenizer.decode(\n",
        "                s, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "            )\n",
        "            for s in summaries\n",
        "        ]\n",
        "\n",
        "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
        "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
        "\n",
        "    #  Finally compute and return the ROUGE scores.\n",
        "    score = metric.compute()\n",
        "    return score\n",
        "\n",
        "\n",
        "def evaluation(tokenizer, model, dataset):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # loading data\n",
        "    rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "    rouge_metric = load_metric(\"rouge\")\n",
        "    score = calculate_metric_on_test_ds(\n",
        "        dataset[\"test\"][0:10],\n",
        "        rouge_metric,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        batch_size=2,\n",
        "        column_text=\"article\",\n",
        "        column_summary=\"headline\",\n",
        "    )\n",
        "\n",
        "    rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
        "\n",
        "    return rouge_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3830260/3742215907.py:70: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ð¤ Evaluate: https://huggingface.co/docs/evaluate\n",
            "  rouge_metric = load_metric(\"rouge\")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "  0%|                                                                       | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            " 20%|âââââââââââââ                                                  | 1/5 [00:02<00:10,  2.60s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            " 40%|ââââââââââââââââââââââââââ                                     | 2/5 [00:04<00:07,  2.45s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            " 60%|ââââââââââââââââââââââââââââââââââââââ                         | 3/5 [00:06<00:04,  2.27s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            " 80%|âââââââââââââââââââââââââââââââââââââââââââââââââââ            | 4/5 [00:10<00:02,  2.82s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 5/5 [00:13<00:00,  2.84s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 5/5 [00:13<00:00,  2.71s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'rouge1': 0.1, 'rouge2': 0.0, 'rougeL': 0.1, 'rougeLsum': 0.1}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "evaluation(tokenizer, model, dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section2: Summarization using GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import AzureOpenAI\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "import openai\n",
        "import time\n",
        "\n",
        "DATASET = \"Goud\"\n",
        "MAX_TRAIN = 20\n",
        "\n",
        "goud_data = load_dataset(\"Goud/Goud-sum\")\n",
        "\n",
        "train_source = goud_data[\"train\"][\"article\"]\n",
        "train_target = goud_data[\"train\"][\"headline\"]\n",
        "train_len = len(train_source)\n",
        "\n",
        "test_source = goud_data[\"test\"][\"article\"]\n",
        "\n",
        "output_filename = f\"./{DATASET}_test_generated_{MAX_TRAIN}.csv\"\n",
        "\n",
        "def summarize_news_article():\n",
        "    rewritten_prompt_count = 0\n",
        "    line_count = 0\n",
        "    wait_time = 1\n",
        "    df_lines = []\n",
        "    client = AzureOpenAI(\n",
        "        azure_endpoint=\"\",\n",
        "        api_version=\"\",\n",
        "        api_key=\"\",\n",
        "    )\n",
        "    existing_len = 0\n",
        "    if os.path.exists(output_filename):\n",
        "        existing_df = pd.read_csv(output_filename)\n",
        "        existing_len = existing_df.shape[0]\n",
        "        rewritten_prompt_count = existing_len\n",
        "        line_count = existing_len\n",
        "        df_lines = existing_df.to_dict('records')\n",
        "    for data in tqdm(test_source[existing_len:], desc=f\"Lines processed from {existing_len}-th line\"):\n",
        "        news_article = data.strip()\n",
        "        line_count += 1\n",
        "        made_error = True\n",
        "        num_error = 0\n",
        "        while made_error:\n",
        "            messages = [{\"role\": \"system\", \"content\": \"You are asked to summarize a news article written in Modern Standard Arabic and Moroccan Darija, and write that summary as a clickbait headline, in Moroccan Darija only.\\n\"}]\n",
        "            if MAX_TRAIN - num_error > 0:\n",
        "                for _ in range(MAX_TRAIN-num_error):\n",
        "                    idx = random.choice(range(train_len))\n",
        "                    train_src = train_source[idx]\n",
        "                    train_tgt = train_target[idx]\n",
        "                    messages.append({\"role\": \"user\", \"content\": f\"Summarize the following news article into a headline in Moroccan Darija only:\\n\\\"{train_src}\\\"\"})\n",
        "                    messages.append({\"role\": \"assistant\", \"content\": f\"Absolutely! Here is the headline summarizing your news article:\\n\\\"{train_tgt}\\\"\"})\n",
        "            messages.append({\"role\": \"user\", \"content\": f\"Summarize the following news article into a headline in Moroccan Darija only:\\n\\\"{news_article}\\\"\"})\n",
        "            try:\n",
        "                response = client.chat.completions.create(\n",
        "                    messages=messages,\n",
        "                    model=\"gpt-4-32k-0613\", #Must fill in, optional: gpt-35-turboãgpt-4ãgpt-4-32k\n",
        "                )\n",
        "                rewritten_prompt = response.choices[0].message.content\n",
        "                df_lines.append({\"article\": news_article, \"generated_headline\": rewritten_prompt})\n",
        "                rewritten_prompt_count += 1\n",
        "                made_error = False\n",
        "            except Exception as e:\n",
        "                if type(e) is openai.RateLimitError:\n",
        "                    print(\"Rate limit error\")\n",
        "                    print(f\"Wait for {wait_time} seconds because all calls failed: \", flush=True)\n",
        "                    time.sleep(wait_time)\n",
        "                    wait_time *= 2\n",
        "                else:\n",
        "                    print(e)\n",
        "                    num_error += 1\n",
        "                    print(\"May be too long, reducing context to:\", MAX_TRAIN-num_error)\n",
        "                \n",
        "    \n",
        "    df = pd.DataFrame.from_dict(df_lines)\n",
        "    df.to_csv(output_filename)\n",
        "\n",
        "summarize_news_article()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from rouge_metric import PyRouge\n",
        "\n",
        "def evaluate_rouge(hypotheses, references):\n",
        "    these_refs = [[ref.strip().lower()] for ref in references]\n",
        "    rouge = PyRouge(rouge_n=(1, 2), rouge_l=True)\n",
        "    scores = rouge.evaluate(hypotheses, these_refs)\n",
        "    print(scores)\n",
        "\n",
        "def substring_after_colon(input_string):\n",
        "    # Find the index of the first colon\n",
        "    colon_index = input_string.find(':')\n",
        "    \n",
        "    # If a colon is found, return the substring starting just after it\n",
        "    if colon_index != -1:\n",
        "        return input_string[colon_index + 1:]\n",
        "    else:\n",
        "        # If no colon is found, return an empty string\n",
        "        return input_string\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    DATASET = \"Goud\"\n",
        "    hypotheses = pd.read_csv(f\"./{DATASET}_test_generated_0.csv\")[\"generated_headline\"].tolist()\n",
        "    hypotheses = [substring_after_colon(hypo).replace(\"\\\"\", \"\").strip() for hypo in hypotheses]\n",
        "    goud_data = load_dataset(\"Goud/Goud-sum\")\n",
        "    references = goud_data[\"test\"][\"headline\"]\n",
        "    evaluate_rouge(hypotheses, references)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhIkaajyaYGl"
      },
      "source": [
        "**Group Task:**\n",
        "\n",
        "Task that involves asking your neighbour or a group a question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L03B3HKwhAhK"
      },
      "outputs": [],
      "source": [
        "# @title Generate Quiz Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/zbJoTSz3nfYq1VrY6\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "## Conclusion\n",
        "**Summary:**\n",
        "\n",
        "[Summary of the main points/takeaways from the prac.]\n",
        "\n",
        "**Next Steps:**\n",
        "\n",
        "[Next steps for people who have completed the prac, like optional reading (e.g. blogs, papers, courses, youtube videos). This could also link to other pracs.]\n",
        "\n",
        "**Appendix:**\n",
        "\n",
        "[Anything (probably math heavy stuff) we don't have space for in the main practical sections.]\n",
        "\n",
        "**References:**\n",
        "\n",
        "[References for any content used in the notebook.]\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2022)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "## Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/WUpRupqfhFtbLXtN6\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "WILOYJH4gCnD"
      ],
      "name": "Indaba_2022_Prac_Template.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "145833166d986a8417df3c7acb65d917d84b716b5a452e57fcacdc66f1a168c9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
