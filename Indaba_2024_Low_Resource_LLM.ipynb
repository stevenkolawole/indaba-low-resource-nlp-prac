{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7MEopc0PR-C"
      },
      "source": [
        "# **Low-resource NLP**\n",
        "\n",
        "<img src=\"https://github.com/stevenkolawole/indaba-low-resource-nlp-prac/blob/main/content/lr_llm_header.png?raw=1\" width=\"60%\" allign =\"center\"/>\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2024/blob/main/practicals/Indaba_2024_Prac_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [Change colab link to point to prac.]\n",
        "\n",
        "Â© Deep Learning Indaba 2024. Apache License 2.0.\n",
        "\n",
        "**Authors:**\n",
        "- Ali Zaidi\n",
        "- Aya Salama\n",
        "- Khalil Mrini\n",
        "- Steven Kolawole\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "Low-resource NLP (Natural Language Processing) refers to the study and development of NLP models and systems for languages, tasks, or domains that have limited data and resources available. These can include languages with fewer digital text corpora, limited computational tools, or less-developed linguistic research.\n",
        "\n",
        "**Key Challenges in Low-Resource NLP**\n",
        "\n",
        "1. **Data Scarcity:**\n",
        "   - **Limited Training Data:** Many languages lack large annotated corpora necessary for training NLP models.\n",
        "   - **Lack of Pre-trained Models:** Popular NLP models like BERT, GPT, and others are often not available for low-resource languages.\n",
        "\n",
        "2. **Linguistic Diversity:**\n",
        "   - **Morphological Complexity:** Some languages have complex grammatical structures and morphological richness.\n",
        "   - **Dialectal Variations:** A lack of standardized versions can complicate NLP tasks.\n",
        "\n",
        "3. **Resource Limitations:**\n",
        "   - **Computational Constraints:** Low-resource scenarios often involve limited access to computational power and storage.\n",
        "   - **Expertise and Tools:** Fewer linguistic experts and fewer NLP tools are tailored for these languages.\n",
        "\n",
        "**Topics:**\n",
        "\n",
        "Content: [Natural Language Processing, Low-resource, Large Language Models, Parameter Efficient Finetuning, Adaptation]  \n",
        "Level: [Intermediate]\n",
        "\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "- Exploring data scarcity challenges\n",
        "- Exploring Compute resource limitations and addressing them with Parameter efficient finetuning\n",
        "- Comparing Performance between Small (BERT) and Large (GPT) Language models on low-resource languages/tasks\n",
        "  \n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "[Knowledge required for this prac. You can link a relevant parallel track session, blogs, papers, courses, topics etc.]\n",
        "_ link resources on LLMs, Bert, Masakhane papers on low resouce conversations\n",
        "\n",
        "**Outline:**\n",
        "\n",
        "[Points that link to each section. Auto-generate following the instructions [here](https://stackoverflow.com/questions/67458990/how-to-automatically-generate-a-table-of-contents-in-colab-notebook).]\n",
        "\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "[Tasks just before starting.]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J6yAAePPR-D"
      },
      "source": [
        "Storyline: working on a task with scarce data which is summarization in Moroccan Darija.\n",
        "this task pose resources constrainst as the Moroccan Darija can be considered a low resource dialect of Arabic, we will set this task in a coumputational resource poor environment so our training should be able to run on a commodity GPU\n",
        "we will be using parameter efficient fine tuning technique (LORA) to optimize the training procedure in order to make it feasible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wslj6xaTPR-D"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE6olWFpPR-E"
      },
      "source": [
        "##  Run cell to setup the needed packages and resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HObGQMKPR-E"
      },
      "source": [
        "**Resource folders**:\n",
        "\n",
        "The resources you'll need to run this practical will be downloaded when you run the next cell.\n",
        "After downloading and extraction are complete, you'll have the following folders present in the \"resources\" folder in the parent directory:\n",
        "\n",
        "- *models* folder: this folder has the pre-trained models that will be utilized in the practical\n",
        "- *dataset* folder: this folder has the Goud-sum dataset that we will be utilizing in the pratical\n",
        "- *genrated_responses* folder: this folder has pregenrated summaries that will be utilized in Section2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o74Lcs8CPR-E",
        "outputId": "4d4ab5a6-70e6-4dc8-f167-77a0eb481adf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'indaba-low-resource-nlp-prac'...\n",
            "remote: Enumerating objects: 70, done.\u001b[K\n",
            "remote: Counting objects: 100% (70/70), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 70 (delta 25), reused 32 (delta 12), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (70/70), 3.26 MiB | 7.96 MiB/s, done.\n",
            "Resolving deltas: 100% (25/25), done.\n",
            "/content/indaba-low-resource-nlp-prac\n",
            "Collecting datasets==2.14.4 (from -r requirements.txt (line 1))\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting peft==0.6.0 (from -r requirements.txt (line 2))\n",
            "  Downloading peft-0.6.0-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting transformers==4.32.0 (from -r requirements.txt (line 3))\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl.metadata (118 kB)\n",
            "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 118.5/118.5 kB 5.3 MB/s eta 0:00:00\n",
            "Collecting tqdm==4.66.1 (from -r requirements.txt (line 4))\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
            "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 57.6/57.6 kB 1.7 MB/s eta 0:00:00\n",
            "Collecting huggingface_hub==0.17.3 (from -r requirements.txt (line 5))\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting python-dotenv==1.0.0 (from -r requirements.txt (line 6))\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting wandb==0.15.8 (from -r requirements.txt (line 7))\n",
            "  Downloading wandb-0.15.8-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting torch==2.0.1 (from -r requirements.txt (line 8))\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting numpy==1.25.0 (from -r requirements.txt (line 9))\n",
            "  Downloading numpy-1.25.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting pandas==2.0.3 (from -r requirements.txt (line 10))\n",
            "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting rouge-metric==1.0.1 (from -r requirements.txt (line 11))\n",
            "  Downloading rouge_metric-1.0.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting openai==0.28.0 (from -r requirements.txt (line 12))\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.4->-r requirements.txt (line 1)) (14.0.2)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.14.4->-r requirements.txt (line 1))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.4->-r requirements.txt (line 1)) (2.32.3)\n",
            "Collecting xxhash (from datasets==2.14.4->-r requirements.txt (line 1))\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets==2.14.4->-r requirements.txt (line 1))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.11.1->datasets==2.14.4->-r requirements.txt (line 1)) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.4->-r requirements.txt (line 1)) (3.10.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.4->-r requirements.txt (line 1)) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.4->-r requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0->-r requirements.txt (line 2)) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0->-r requirements.txt (line 2)) (0.32.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0->-r requirements.txt (line 2)) (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0->-r requirements.txt (line 3)) (3.15.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0->-r requirements.txt (line 3)) (2024.5.15)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.32.0->-r requirements.txt (line 3))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.17.3->-r requirements.txt (line 5)) (4.12.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.8->-r requirements.txt (line 7)) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb==0.15.8->-r requirements.txt (line 7))\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb==0.15.8->-r requirements.txt (line 7))\n",
            "  Downloading sentry_sdk-2.13.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb==0.15.8->-r requirements.txt (line 7))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting pathtools (from wandb==0.15.8->-r requirements.txt (line 7))\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting setproctitle (from wandb==0.15.8->-r requirements.txt (line 7))\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.8->-r requirements.txt (line 7)) (71.0.4)\n",
            "Collecting appdirs>=1.4.3 (from wandb==0.15.8->-r requirements.txt (line 7))\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.8->-r requirements.txt (line 7)) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 8)) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 8)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 8)) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 8))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 8))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->-r requirements.txt (line 8))\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->-r requirements.txt (line 8))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->-r requirements.txt (line 8))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->-r requirements.txt (line 8))\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->-r requirements.txt (line 8))\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->-r requirements.txt (line 8))\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->-r requirements.txt (line 8))\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->-r requirements.txt (line 8))\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->-r requirements.txt (line 8))\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1->-r requirements.txt (line 8))\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->-r requirements.txt (line 10)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->-r requirements.txt (line 10)) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->-r requirements.txt (line 10)) (2024.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 8)) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 8)) (3.30.2)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 8))\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb==0.15.8->-r requirements.txt (line 7)) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.4->-r requirements.txt (line 1)) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.4->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.4->-r requirements.txt (line 1)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.4->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.4->-r requirements.txt (line 1)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.4->-r requirements.txt (line 1)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.4->-r requirements.txt (line 1)) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.8->-r requirements.txt (line 7))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.4->-r requirements.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.4->-r requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.4->-r requirements.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.4->-r requirements.txt (line 1)) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->-r requirements.txt (line 8)) (2.1.5)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.14.4->-r requirements.txt (line 1))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->-r requirements.txt (line 8)) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.8->-r requirements.txt (line 7))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 519.3/519.3 kB 26.6 MB/s eta 0:00:00\n",
            "Downloading peft-0.6.0-py3-none-any.whl (134 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 134.9/134.9 kB 12.0 MB/s eta 0:00:00\n",
            "Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7.5/7.5 MB 69.0 MB/s eta 0:00:00\n",
            "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 78.3/78.3 kB 6.6 MB/s eta 0:00:00\n",
            "Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 295.0/295.0 kB 15.0 MB/s eta 0:00:00\n",
            "Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Downloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2.1/2.1 MB 47.9 MB/s eta 0:00:00\n",
            "Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 619.9/619.9 MB 1.1 MB/s eta 0:00:00\n",
            "Downloading numpy-1.25.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 17.6/17.6 MB 97.5 MB/s eta 0:00:00\n",
            "Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12.3/12.3 MB 103.9 MB/s eta 0:00:00\n",
            "Downloading rouge_metric-1.0.1-py3-none-any.whl (151 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 151.7/151.7 kB 12.7 MB/s eta 0:00:00\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 76.5/76.5 kB 6.4 MB/s eta 0:00:00\n",
            "Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 317.1/317.1 MB 3.0 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 11.8/11.8 MB 105.7 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 21.0/21.0 MB 92.9 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 849.3/849.3 kB 48.9 MB/s eta 0:00:00\n",
            "Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 557.1/557.1 MB 2.1 MB/s eta 0:00:00\n",
            "Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 168.4/168.4 MB 7.4 MB/s eta 0:00:00\n",
            "Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 54.6/54.6 MB 15.7 MB/s eta 0:00:00\n",
            "Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 102.6/102.6 MB 8.4 MB/s eta 0:00:00\n",
            "Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 173.2/173.2 MB 6.7 MB/s eta 0:00:00\n",
            "Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 177.1/177.1 MB 7.2 MB/s eta 0:00:00\n",
            "Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 98.6/98.6 kB 8.6 MB/s eta 0:00:00\n",
            "Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 63.3/63.3 MB 13.2 MB/s eta 0:00:00\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 115.3/115.3 kB 10.0 MB/s eta 0:00:00\n",
            "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 207.3/207.3 kB 17.2 MB/s eta 0:00:00\n",
            "Downloading sentry_sdk-2.13.0-py2.py3-none-any.whl (309 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 309.1/309.1 kB 25.9 MB/s eta 0:00:00\n",
            "Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7.8/7.8 MB 103.8 MB/s eta 0:00:00\n",
            "Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 134.8/134.8 kB 9.5 MB/s eta 0:00:00\n",
            "Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 194.1/194.1 kB 12.3 MB/s eta 0:00:00\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 62.7/62.7 kB 5.4 MB/s eta 0:00:00\n",
            "Downloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 96.4/96.4 kB 9.1 MB/s eta 0:00:00\n",
            "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py): started\n",
            "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8792 sha256=5bd38ed634c4dbb4ebc6ae649c40269ad572f64d445943b65adde2a147642cd2\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: tokenizers, pathtools, lit, appdirs, xxhash, tqdm, smmap, setproctitle, sentry-sdk, rouge-metric, python-dotenv, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, docker-pycreds, dill, pandas, nvidia-cusolver-cu11, nvidia-cudnn-cu11, multiprocess, huggingface_hub, gitdb, transformers, openai, GitPython, wandb, datasets, triton, torch, peft\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.5\n",
            "    Uninstalling tqdm-4.66.5:\n",
            "      Successfully uninstalled tqdm-4.66.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.1.4\n",
            "    Uninstalling pandas-2.1.4:\n",
            "      Successfully uninstalled pandas-2.1.4\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.23.5\n",
            "    Uninstalling huggingface-hub-0.23.5:\n",
            "      Successfully uninstalled huggingface-hub-0.23.5\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.42.4\n",
            "    Uninstalling transformers-4.42.4:\n",
            "      Successfully uninstalled transformers-4.42.4\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.1\n",
            "    Uninstalling triton-2.3.1:\n",
            "      Successfully uninstalled triton-2.3.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1+cu121\n",
            "    Uninstalling torch-2.3.1+cu121:\n",
            "      Successfully uninstalled torch-2.3.1+cu121\n",
            "Successfully installed GitPython-3.1.43 appdirs-1.4.4 datasets-2.14.4 dill-0.3.7 docker-pycreds-0.4.0 gitdb-4.0.11 huggingface_hub-0.17.3 lit-18.1.8 multiprocess-0.70.15 numpy-1.25.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 openai-0.28.0 pandas-2.0.3 pathtools-0.1.2 peft-0.6.0 python-dotenv-1.0.0 rouge-metric-1.0.1 sentry-sdk-2.13.0 setproctitle-1.3.3 smmap-5.0.1 tokenizers-0.13.3 torch-2.0.1 tqdm-4.66.1 transformers-4.32.0 triton-2.0.0 wandb-0.15.8 xxhash-3.5.0\n",
            "Starting download from https://dli2024prac.blob.core.windows.net/resources/resources.zip...\n",
            "Download complete. Extracting 30.14 MB...\n",
            "Extracted 8 files to '.':\n",
            " - resources/generated_responses/\n",
            " - resources/generated_responses/gpt4/\n",
            " - resources/generated_responses/gpt4/Goud_test_generated_0.csv\n",
            " - resources/generated_responses/gpt4/Goud_test_generated_1.csv\n",
            " - resources/generated_responses/gpt4/Goud_test_generated_20.csv\n",
            " - resources/generated_responses/gpt4/Goud_test_generated_5.csv\n",
            " - resources/generated_responses/gpt-4o-mini/\n",
            " - resources/generated_responses/gpt-4o-mini/Goud_gpt-4o-mini_test_generated_0.csv\n",
            "Download and extraction complete.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/stevenkolawole/indaba-low-resource-nlp-prac.git\n",
        "%cd indaba-low-resource-nlp-prac\n",
        "\n",
        "import utils\n",
        "\n",
        "# Install the required packages\n",
        "utils.install_requirements()\n",
        "\n",
        "# Download and extract the zip file containing the resources\n",
        "utils.download_and_extract_zip(\"https://dli2024prac.blob.core.windows.net/testres/resources.zip\")\n",
        "model, tokenizer, config = utils.load_models()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7kzqCkyPR-F"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L530_VU2PR-F"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset, concatenate_datasets, load_metric, load_from_disk\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PeftConfig, PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, EncoderDecoderModel, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85IQK28VPR-F"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V41BU0U_PR-F"
      },
      "source": [
        "In this practical we are interested in generating headlines for news articles featured on the news website [Goud.ma](www.Gound.ma).\n",
        "Refer to the [github.com/issam9/goud-summarization-dataset](https://github.com/issam9/goud-summarization-dataset) repository for the training code for the workshop paper [Goud.ma: a News Dataset for Summarization in Moroccan Darija](https://openreview.net/forum?id=BMVq5MELb9).\n",
        "\n",
        "We will frame this as a summarization task where the input is the body of a news article and the output is an appropriate headline. The [Goud dataset](https://huggingface.co/datasets/Goud/Goud-sum) contains 158k articles and their headlines. All headlines are in Moroccan Darija, while articles may be in Moroccan Darija, in Modern Standard Arabic, or a mix of both (code-switched Moroccan Darija).\n",
        "\n",
        "**Data Fields**\n",
        "- *article*: a string containing the body of the news article\n",
        "- *headline*: a string containing the article's headline\n",
        "- *categories*: a list of string of article categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <iframe\n",
              "    src=\"https://huggingface.co/datasets/Goud/Goud-sum/embed/viewer/default/train\"\n",
              "    frameborder=\"0\"\n",
              "    width=\"100%\"\n",
              "    height=\"560px\"\n",
              "    ></iframe>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(HTML(\n",
        "    \"\"\"\n",
        "    <iframe\n",
        "    src=\"https://huggingface.co/datasets/Goud/Goud-sum/embed/viewer/default/train\"\n",
        "    frameborder=\"0\"\n",
        "    width=\"100%\"\n",
        "    height=\"560px\"\n",
        "    ></iframe>\n",
        "    \"\"\"\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmX8ef0uPR-F"
      },
      "source": [
        "## What we will do:\n",
        "<img src=\"https://github.com/stevenkolawole/indaba-low-resource-nlp-prac/blob/main/content/DLI_LR_llm_prac_1.png?raw=1\" width=\"40%\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtYLZ9loPR-F"
      },
      "source": [
        "## Evaluation Metric: ROUGE\n",
        "\n",
        "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate the quality of summaries by comparing them to reference (or ground truth) summaries. ROUGE is widely used in Natural Language Processing (NLP) tasks, particularly for evaluating the performance of text summarization models.\n",
        "\n",
        "![ROUGE-Base](https://i0.wp.com/blog.uptrain.ai/wp-content/uploads/2024/01/rouge-n.webp?resize=700%2C228&ssl=1)\n",
        "\n",
        "### Key ROUGE Variants\n",
        "\n",
        "1. **ROUGE-N**: Measures the overlap of n-grams between the candidate summary and the reference summary.\n",
        "\n",
        "![ROUGE-1](https://clementbm.github.io/assets/2021-12-23/rouge-unigrams.png)\n",
        "\n",
        "*caption:*\n",
        "$ROUGE_1 = \\frac{7}{10} = 0.7$\n",
        "\n",
        "   - **ROUGE-1**: Overlap of unigrams (1-gram).\n",
        "   - **ROUGE-2**: Overlap of bigrams (2-grams).\n",
        "   - **ROUGE-L**: Measures the longest common subsequence (LCS) between the candidate and reference summaries.\n",
        "\n",
        "2. **ROUGE-L**: Measures the longest common subsequence (LCS) between the candidate summary and the reference summary. Unlike ROUGE-N, ROUGE-L considers sentence-level structure similarity by identifying the longest co-occurring sequence of words in both summaries.\n",
        "\n",
        "\n",
        "\n",
        "3. **ROUGE-W**: A weighted version of ROUGE-L that gives more importance to the contiguous LCS.\n",
        "\n",
        "4. **ROUGE-S**: Measures the overlap of skip-bigrams, which are pairs of words in their order of appearance that can have any number of gaps between them.\n",
        "\n",
        "### How ROUGE is Computed\n",
        "\n",
        "ROUGE metrics can be calculated in terms of three measures:\n",
        "\n",
        "- **Recall**: The ratio of overlapping units (n-grams, LCS, or skip-bigrams) between the candidate summary and the reference summary to the total units in the reference summary. It answers, \"How much of the reference summary is captured by the candidate summary?\"\n",
        "\n",
        "- **Precision**: The ratio of overlapping units between the candidate summary and the reference summary to the total units in the candidate summary. It answers, \"How much of the candidate summary is relevant to the reference summary?\"\n",
        "\n",
        "- **F1-Score**: The harmonic mean of Precision and Recall. This gives a balanced measure that considers both precision and recall.\n",
        "\n",
        "### Importance of ROUGE\n",
        "\n",
        "ROUGE is essential for summarization tasks because it provides a standardized way to evaluate and compare different summarization models. Higher ROUGE scores generally indicate that the candidate summary is more similar to the reference summary, meaning the model is likely performing well.\n",
        "\n",
        "#### NOTE: Caveat\n",
        "<div style=\"display: flex; justify-content: space-between;\">\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*8ZNpaag-Nr2GLs3A-sz0aQ.png\" alt=\"limitation 1\" width=\"250\"/>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*CLIKeyKYiR6sNA4yjIkCWg.png\" alt=\"limitation 2\" width=\"250\"/>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*667HMbjSLJhwR_xqBau3JQ.png\" alt=\"limitation 3\" width=\"250\"/>\n",
        "</div>\n",
        "\n",
        "While ROUGE and other evaluation metrics (e.g., BLEU, METEOR, etc) serve as valuable tools for quick and straightforward evaluation of language models, they have certain limitations that render them less than ideal. To begin with, they fall short when it comes to assessing the fluency, coherence, and overall meaning of passages. They are also relatively insensitive to word order. ROUGE primarily measures lexical overlap and may not fully capture the semantic meaning or quality of a summary. For these reasons, researchers are still trying to find improved metrics.\n",
        "\n",
        "Therefore, these metrics are not shoe-in replacements for human evaluation, but are best used in conjunction with human evaluations for a more comprehensive assessment of summary quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiqKu07MPR-F"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8qDLouRPR-F"
      },
      "outputs": [],
      "source": [
        "# Load the dataset from disk\n",
        "dataset = load_from_disk(\"./resources/data/Goud-sum/Goud-sum\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cjG8W9iPR-G"
      },
      "source": [
        "## Check dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pj99vydiPR-G",
        "outputId": "9694b48b-1734-4a06-b22e-8012e79f741a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['article', 'headline', 'categories'],\n",
            "        num_rows: 139288\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['article', 'headline', 'categories'],\n",
            "        num_rows: 9497\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['article', 'headline', 'categories'],\n",
            "        num_rows: 9497\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Xs4_pevPR-G",
        "outputId": "f5028f17-9361-4a2f-dcae-75217ecf0401"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'article': 'Ù…Ù†ÙŠØ± Ø§Ù„Ø¹Ù„Ù…ÙŠ Ù…Ù† Ù…Ø±Ø§ÙƒØ´: ØªØ­ÙˆÙ„ ÙØ¶Ø§Ø¡ Ù…Ù‚Ø± Ø§Ù„ØºØ±ÙØ© Ø§Ù„ÙÙ„Ø§Ø­ÙŠØ© Ø¨Ù…Ø¯ÙŠÙ†Ø© Ù…Ø±Ø§ÙƒØ´ØŒ Ø§Ù„Ø°ÙŠ ÙŠØ­ØªØ¶Ù† ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø§Ù”Ø«Ù†Ø§Ø¡ØŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø±ÙŠÙ”ÙŠØ³ ÙˆØ§Ù”Ø¹Ø¶Ø§Ø¡ Ø§Ù„Ù…ÙƒØªØ¨ Ø§Ù„Ù…Ø³ÙŠØ± Ù„Ù„ØºØ±ÙØ© Ø§Ù„ÙÙ„Ø§Ø­ÙŠØ© Ø¨Ø¬Ù‡Ø© Ù…Ø±Ø§ÙƒØ´ Ø§Ù“Ø³ÙÙŠØŒ Ø§Ù•Ù„Ù‰ Ø­Ù„Ø¨Ø© Ù„Ù„Ø§Ø´ØªØ¨Ø§ÙƒØ§Øª ÙˆØ§Ù„Ù…Ù„Ø§Ø³Ù†Ø§ØªØŒ Ø¨Ø¹Ø¯ Ø§Ø´ØªØ¯Ø§Ø¯ Ø§Ù„Ø®Ù„Ø§Ù Ø¨ÙŠÙ† Ø§Ù„Ø¨Ø±Ù„Ù…Ø§Ù†ÙŠÙŠÙ† Ø­Ù…ÙŠØ¯ Ø§Ù„Ø¹ÙƒØ±ÙˆØ¯ ÙˆØ¹Ù…Ø± Ø®ÙÙŠÙØŒ Ø§Ù„Ù„Ø°ÙŠÙ† ÙŠÙ†ØªÙ…ÙŠØ§Ù† Ø§Ù•Ù„Ù‰ Ø­Ø²Ø¨ Ø§Ù„ØªØ¬Ù…Ø¹ Ø§Ù„ÙˆØ·Ù†ÙŠ Ù„Ù„Ø§Ù”Ø­Ø±Ø§Ø±ØŒ Ù…Ø§ ÙƒØ§Ø¯ ÙŠØ¹ØµÙ Ø¨Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ Ø¨Ø¹Ø¯ Ø§Ù†Ø·Ù„Ø§Ù‚ Ø´Ø±Ø§Ø±Ø© Ø§Ù„Ø§Ø´ØªØ¨Ø§Ùƒ Ø¨Ø§Ù„Ø§Ù”ÙŠØ§Ø¯ÙŠ Ø§Ù„ØªÙŠ Ø§Ù”Ø¬Ù‡Ø¶Øª ÙÙŠ Ù…Ù‡Ø¯Ù‡Ø§ Ø¨ØªØ¯Ø®Ù„ Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø§Ø¶Ø±ÙŠÙ†. ÙˆØ­Ø³Ø¨ Ø´Ù‡ÙˆØ¯ Ø¹ÙŠØ§Ù†ØŒ ÙØ§Ù•Ù† Ø¹Ù…Ø± Ø®ÙÙŠÙØŒ Ø§Ù„Ø°ÙŠ ÙŠØ´ØºÙ„ Ø±ÙŠÙ”ÙŠØ³ Ø¬Ù…Ø§Ø¹Ø© Ø§Ù”ÙƒÙØ§ÙŠØŒ ÙˆÙ…Ø¯Ø¹Ù… Ø§Ù„Ø­Ø¨ÙŠØ¨ Ø¨Ù† Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„Ù…Ù†Ø³Ù‚ Ø§Ù„Ø§Ù‚Ù„ÙŠÙ…ÙŠ Ù„Ø­Ø²Ø¨ Ø§Ù„Ø§Ù”ØµØ§Ù„Ø© ÙˆØ§Ù„Ù…Ø¹Ø§ØµØ± Ø§Ù„Ø°ÙŠ ÙŠØªØ¬Ù‡ Ù„ØªÙˆÙ„ÙŠ Ø±ÙŠÙ”Ø§Ø³Ø© Ø§Ù„ØºØ±ÙØ© Ù„ÙˆÙ„Ø§ÙŠØ© ØªØ§Ù†ÙŠØ©ØŒ Ø±ÙØ¶ Ø¯Ø®ÙˆÙ„ Ø­Ù…ÙŠØ¯ Ø§Ù„Ø¹ÙƒØ±ÙˆØ¯ Ù„Ù„Ù…Ù†Ø§ÙØ³Ø© Ø¹Ù„Ù‰ Ø±ÙŠÙ”Ø§Ø³Ø© Ø§Ù„ØºØ±ÙØ©ØŒ ÙˆØ§ØµÙØ§ Ø§Ù•ÙŠØ§Ù‡ Ø¨Ù€ â€œØ§Ù„Ø§Ù”Ù…ÙŠ Ø§Ù„Ø°ÙŠ Ù„Ø§ÙŠÙÙ‚Ù‡ Ø´ÙŠÙŠÙ”Ø§â€ØŒ Ù„ÙŠØ¯Ø®Ù„ Ø§Ù„Ø·Ø±ÙØ§Ù† ÙÙŠ Ù…Ù„Ø§Ø³Ù†Ø§Øª ÙƒÙ„Ø§Ù…ÙŠØ© Ù‚Ø¨Ù„ Ø§Ù”Ù† ÙŠØªØ­ÙˆÙ„ Ø§Ù„ØµØ±Ø§Ø¹ Ø§Ù•Ù„Ù‰ ØªØ´Ø§Ø¨Ùƒ Ø¨Ø§Ù„Ø§Ù”ÙŠØ¯ÙŠ. ',\n",
              " 'headline': 'Ø¨Ø±Ù„Ù…Ø§Ù†ÙŠÙŠÙ† Ù…Ù† Ø­Ø²Ø¨ Ø§Ù„Ø­Ù…Ø§Ù…Ø© Ù‚Ù„Ø¨ÙˆÙ‡Ø§ Ø¨ÙˆÙ†ÙŠØ§ Ù‚Ø¨Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø±Ø¦ÙŠØ³ ÙˆØ£Ø¹Ø¶Ø§Ø¡ ØºØ±ÙØ© Ø§Ù„ÙÙ„Ø§Ø­Ø© Ø¨Ø¬Ù‡Ø© Ù…Ø±Ø§ÙƒØ´ Ø¢Ø³ÙÙŠ (ØµÙˆØ±)',\n",
              " 'categories': \"['Ø¢Ø´ ÙˆØ§Ù‚Ø¹', 'Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©']\"}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYkzmZO9PR-G"
      },
      "source": [
        "# Section 1: Efficiently Fine-Tune Seq2Seq Models with Low Rank Adaptation (LoRA)\n",
        "\n",
        "The goal of this section is to fine-tune a base model for our summarization task using a parameter efficient mechanism called low-rank adapatation (LoRA). An implementation of this technique is part of the Parameter Efficient Fine-Tuning (PEFT) library from Hugging Face. We will leverage the ğŸ¤— [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), and [PEFT](https://github.com/huggingface/peft) in this section.\n",
        "\n",
        "You will learn how to:\n",
        "\n",
        "1. Setup Development Environment\n",
        "2. Load and prepare the dataset\n",
        "3. Fine-Tune Multilingual BERT with LoRA and bnb int-8\n",
        "4. Evaluate & run inference\n",
        "5. Cost performance comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick intro to PEFT or Parameter Efficient Fine-tuning\n",
        "<div style=\"display: flex; justify-content: center; align-items: flex-start;\">    <figure style=\"text-align: center;\">\n",
        "        <a href=\"https://arxiv.org/abs/2303.15647#\" target=\"_blank\">\n",
        "            <img src=\"https://github.com/stevenkolawole/indaba-low-resource-nlp-prac/blob/main/content/PEFT_method.png?raw=1\" width=\"90%\" />\n",
        "        </a>\n",
        "        <figcaption><a href=\"https://arxiv.org/abs/2303.15647#\" target=\"_blank\">PEFT Methods, from the paper \"Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning\"\n",
        "</a></figcaption>\n",
        "    </figure>\n",
        "</div>\n",
        "\n",
        "[PEFT](https://github.com/huggingface/peft), or Parameter Efficient Fine-tuning, is a new open-source library from Hugging Face to enable efficient adaptation of pre-trained models, including but not limited to language models and diffusion mdoels, to various downstream applications without needing fine-tuning all the model's parameters. PEFT includes techniques and variants of many methods such as:\n",
        "\n",
        "- LoRA:Â [LoRA: Low-Rank Adaptation of Language models](https://arxiv.org/abs/2106.09685)\n",
        "- Prefix Tuning:Â [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/abs/2110.07602)\n",
        "- P-Tuning:Â [GPT Understands, Too](https://arxiv.org/abs/2103.10385)\n",
        "- Prompt Tuning:Â [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtgK-3iMPR-G"
      },
      "source": [
        "## Low-Rank Adaptation (LoRA)\n",
        "\n",
        "While large language models (LLMs) have shown remarkable performance across a wide range of NLP tasks, they require significant computation resources to train, fine-tune and deploy. In addition, many real-world use-cases require adapting avialable LLMs to their target task in order to achieve desired performance.\n",
        "\n",
        "While fine-tuning an entire LLM is cost prohibitive, even on small datasets. For example, fully fine-tuning the Llama7B model requires 112GB of VRAM, i.e. at least two 80GB A100 GPUs. Fortunately, parameter efficient fine-tuning methods such as LoRA allow users with meager resources to adapat an LLM to their target task efficiently and effectively.\n",
        "\n",
        "In this tutorial we explore QLoRA, which is a parameter-efficient fine-tuning technique that reduces the number of parameters fine-tuned during the adaptation process, and additionally introduces quantization to further lower the memory footprint of the adapted model.\n",
        "\n",
        "### How Does LoRA Work?\n",
        "\n",
        "The paper [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) takes inspiration from the conjecture that over-parameterized models span a low-rank intrinsic dimension. A low intrinsic dimension means the data can be effectively represented or approximated by a lower-dimensional space while retaining most of its essential information or structure. In other words, this means we can decompose the new weight matrix for the adapted task into lower-dimensional (smaller) matrices without losing significant information.\n",
        "\n",
        "Concretely, let us suppose $\\delta W$ is the weight update for an $A\\times B$ weight matrix. Then, a low-rank decompsoition of $\\delta W$ can be expressed as: $\\delta W = W_A W_B$, where $W_A$ is an $A\\times k$ matrix and $W_B$ is a $k\\times B$ matrix. Here, $k$ is the rank of the decomposition, and is typically much smaller than $A$ and $B$.\n",
        "\n",
        "![Image courtesty from Sebastian Raschka's Ligthning.AI tutorial on LoRA](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/lora-4-300x226@2x.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m25VCT7RPR-G"
      },
      "source": [
        "## Summarization Using MT5\n",
        "\n",
        "Prior to fine-tuning our model, we need to select the model we will use as our base model. In this case, we will use the [MT5](https://huggingface.co/google/mt5-small) model, which is a multilingual variant of the T5 model. The MT5 model is trained on a large multilingual corpus and is capable of performing a wide range of NLP tasks, including summarization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB7YfDyXPR-G",
        "outputId": "dc8be7d0-4a6f-4747-a29a-522f67cca0c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/alizaidi/micromamba/envs/peft/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6mLJvO3PR-G"
      },
      "source": [
        "Next we prepare our datasets for training. This requires tokenizing the input and output sequences, padding them to the desired length, and then converting them into PyTorch Dataset objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWB3JXWxPR-G"
      },
      "outputs": [],
      "source": [
        "# The maximum total input sequence length after tokenization.\n",
        "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
        "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(\n",
        "    lambda x: tokenizer(x[\"article\"], truncation=True),\n",
        "    batched=True,\n",
        "    remove_columns=[\"categories\", \"headline\"],\n",
        ")\n",
        "input_lengths = [len(x) for x in tokenized_inputs[\"input_ids\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siNv9AoWPR-G",
        "outputId": "0e6f8078-3efd-4e1d-c653-55156ac404c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max source length: 571\n"
          ]
        }
      ],
      "source": [
        "# take 85 percentile of max length for better utilization\n",
        "max_source_length = int(np.percentile(input_lengths, 85))\n",
        "print(f\"Max source length: {max_source_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PI2Nl25bPR-G",
        "outputId": "0fc13ae7-4df3-45b6-8d25-301958cb5f3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max target length: 50\n"
          ]
        }
      ],
      "source": [
        "# The maximum total sequence length for target text after tokenization.\n",
        "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
        "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(\n",
        "    lambda x: tokenizer(x[\"headline\"], truncation=True),\n",
        "    batched=True,\n",
        "    remove_columns=[\"article\", \"categories\"],\n",
        ")\n",
        "target_lengths = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
        "# take 90 percentile of max length for better utilization\n",
        "max_target_length = int(np.percentile(target_lengths, 90))\n",
        "print(f\"Max target length: {max_target_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0eoq_wNPR-G",
        "outputId": "18044eeb-3fb7-4544-a20c-b0708772641d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
          ]
        }
      ],
      "source": [
        "def preprocess_function(sample, padding=\"max_length\"):\n",
        "    # # add prefix to the input for t5\n",
        "    # inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
        "\n",
        "    # tokenize inputs\n",
        "    model_inputs = tokenizer(sample['article'], max_length=max_source_length, padding=padding, truncation=True)\n",
        "\n",
        "    # Tokenize targets with the `summary` keyword argument\n",
        "    labels = tokenizer(sample[\"headline\"], max_length=max_target_length, padding=padding, truncation=True)\n",
        "\n",
        "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
        "    # padding in the loss.\n",
        "    if padding == \"max_length\":\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"headline\", \"article\", \"categories\"])\n",
        "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9239cc1fc6694dd1b0edd5613e98d9e7"
          ]
        },
        "id": "7xlkyZtVPR-G",
        "outputId": "a2180385-fa79-4d04-c8df-34c90a92ab66"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9239cc1fc6694dd1b0edd5613e98d9e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/9497 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# tokenized_dataset[\"test\"].save_to_disk(\"arabic-goud-data/eval\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wBs-Jz0PR-G"
      },
      "source": [
        "Finally we need to define our configuration for LoRA. The primary parameters for LoRA are:\n",
        "\n",
        "* `r`: this is the rank of the decomposed matrices $A$ and $B$ to be learned during fine-tuning. A smaller number will save more GPU memory but might decrease performance.\n",
        "* `lora_alpha`: this is the weight of the low-rank loss in the total loss function, or the coefficient for the learned $\\Delta W$ factor. A larger number will typically result in a larger behavior change after fine-tuning.\n",
        "* `lora_dropout`: the dropout ratio for layers in the LoRA adapters $A$ and $B$.\n",
        "* `target_modules`: which modules to learn the low-rank decomposition for. This could be all linear layers, for example, or specific modules in the base network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGnxfkBwPR-G",
        "outputId": "ab0ba706-0c05-42c2-f852-a8a603395ce2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 688,128 || all params: 300,864,896 || trainable%: 0.2287\n"
          ]
        }
      ],
      "source": [
        "lora_config = LoraConfig(\n",
        "  r=16,\n",
        "  lora_alpha=32,\n",
        "  target_modules=[\"q\", \"v\"],\n",
        "  lora_dropout=0.05,\n",
        "  bias=\"none\",\n",
        "  task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")\n",
        "# prepare int-8 model for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# add LoRA adaptor\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgLuT5ihPR-G"
      },
      "outputs": [],
      "source": [
        "# we want to ignore tokenizer pad token in the loss\n",
        "label_pad_token_id = -100\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=label_pad_token_id,\n",
        "    pad_to_multiple_of=8,\n",
        "    return_tensors='pt'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukBsDPC_PR-H"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 10\n",
        "output_dir = \"lora-goud-mt5-small\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQgRCmfIPR-H"
      },
      "outputs": [],
      "source": [
        "# Define training args\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"lora-mt5-goud\",\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=500,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=[\"tensorboard\",\n",
        "              #  \"wandb\",\n",
        "               ],\n",
        ")\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset = tokenized_dataset[\"train\"],\n",
        "    eval_dataset = tokenized_dataset[\"validation\"].select(range(20)),\n",
        "    tokenizer = tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <iframe src=\"https://wandb.ai/alizaidi/huggingface/runs/2rwkxynz?nw=nwuseralizaidi\" style=\"border:none;height:1024px;width:100%\"> \n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(HTML(\n",
        "    \"\"\"\n",
        "    <iframe src=\"https://wandb.ai/alizaidi/huggingface/runs/2rwkxynz?nw=nwuseralizaidi\" style=\"border:none;height:1024px;width:100%\"> \n",
        "    \"\"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vWMoe1GMPR-H",
        "outputId": "9b5ae0c5-055d-482c-b4fe-732a63b2ac3c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.7 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/alizaidi/dev/nlp/llms/indaba/indaba-low-resource-nlp-prac/wandb/run-20240816_012514-brfaca9p\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora-mt5-goud\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/alizaidi/huggingface\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/alizaidi/huggingface/runs/brfaca9p\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='174110' max='174110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [174110/174110 4:54:01, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>6.364100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>5.019700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>4.733600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>4.573900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>4.487000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>4.441000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>4.375900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>4.370600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>4.312700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>4.309900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>4.276800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>4.278400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>4.213300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>4.183900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>4.184700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>4.174400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>4.174500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>4.145700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>4.104400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>4.122800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>4.112100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>4.096600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>4.086700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>4.070700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>4.075500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>4.048700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>4.044400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>4.013700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>4.032200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>4.025100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>4.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>4.017300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>4.028800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>4.054900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>3.983700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>3.977800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>4.015500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>3.962000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>3.950300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>3.988000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>3.963800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>3.948300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>3.954600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>4.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>3.944200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>3.936600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>3.942800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>3.938300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>3.926900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>3.943500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>3.933100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>3.951000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>3.914700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>3.912500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>3.971900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>3.906500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>3.913700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>3.937000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29500</td>\n",
              "      <td>3.909400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>3.934100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30500</td>\n",
              "      <td>3.904400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>3.883700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31500</td>\n",
              "      <td>3.890200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>3.901500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32500</td>\n",
              "      <td>3.899800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>3.869100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33500</td>\n",
              "      <td>3.912900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34000</td>\n",
              "      <td>3.917400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34500</td>\n",
              "      <td>3.887100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35000</td>\n",
              "      <td>3.892100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35500</td>\n",
              "      <td>3.874800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36000</td>\n",
              "      <td>3.897100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36500</td>\n",
              "      <td>3.874900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37000</td>\n",
              "      <td>3.846300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37500</td>\n",
              "      <td>3.858400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38000</td>\n",
              "      <td>3.864700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38500</td>\n",
              "      <td>3.873700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39000</td>\n",
              "      <td>3.855400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39500</td>\n",
              "      <td>3.860700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40000</td>\n",
              "      <td>3.864000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40500</td>\n",
              "      <td>3.822500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41000</td>\n",
              "      <td>3.871400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41500</td>\n",
              "      <td>3.873800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42000</td>\n",
              "      <td>3.864700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42500</td>\n",
              "      <td>3.871200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43000</td>\n",
              "      <td>3.824300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43500</td>\n",
              "      <td>3.836300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44000</td>\n",
              "      <td>3.822100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44500</td>\n",
              "      <td>3.809800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45000</td>\n",
              "      <td>3.870400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45500</td>\n",
              "      <td>3.828100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46000</td>\n",
              "      <td>3.858900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46500</td>\n",
              "      <td>3.821300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47000</td>\n",
              "      <td>3.850200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47500</td>\n",
              "      <td>3.808800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48000</td>\n",
              "      <td>3.836400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48500</td>\n",
              "      <td>3.848400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49000</td>\n",
              "      <td>3.831800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49500</td>\n",
              "      <td>3.844700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50000</td>\n",
              "      <td>3.852400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50500</td>\n",
              "      <td>3.810800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51000</td>\n",
              "      <td>3.810000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51500</td>\n",
              "      <td>3.826900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52000</td>\n",
              "      <td>3.818600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52500</td>\n",
              "      <td>3.808900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53000</td>\n",
              "      <td>3.829400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53500</td>\n",
              "      <td>3.811100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54000</td>\n",
              "      <td>3.810500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54500</td>\n",
              "      <td>3.805000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55000</td>\n",
              "      <td>3.779800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55500</td>\n",
              "      <td>3.802900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56000</td>\n",
              "      <td>3.798900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56500</td>\n",
              "      <td>3.799100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57000</td>\n",
              "      <td>3.812200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57500</td>\n",
              "      <td>3.799300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58000</td>\n",
              "      <td>3.808400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58500</td>\n",
              "      <td>3.793900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59000</td>\n",
              "      <td>3.777100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59500</td>\n",
              "      <td>3.790800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60000</td>\n",
              "      <td>3.783400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60500</td>\n",
              "      <td>3.804400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61000</td>\n",
              "      <td>3.785300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61500</td>\n",
              "      <td>3.806300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62000</td>\n",
              "      <td>3.804200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62500</td>\n",
              "      <td>3.801200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63000</td>\n",
              "      <td>3.806000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63500</td>\n",
              "      <td>3.808200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64000</td>\n",
              "      <td>3.785500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64500</td>\n",
              "      <td>3.775000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65000</td>\n",
              "      <td>3.795900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65500</td>\n",
              "      <td>3.775200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66000</td>\n",
              "      <td>3.803400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66500</td>\n",
              "      <td>3.763200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67000</td>\n",
              "      <td>3.774300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67500</td>\n",
              "      <td>3.775800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68000</td>\n",
              "      <td>3.811800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68500</td>\n",
              "      <td>3.761400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69000</td>\n",
              "      <td>3.747300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69500</td>\n",
              "      <td>3.763600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70000</td>\n",
              "      <td>3.779900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70500</td>\n",
              "      <td>3.755300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71000</td>\n",
              "      <td>3.745700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71500</td>\n",
              "      <td>3.752600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72000</td>\n",
              "      <td>3.764000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72500</td>\n",
              "      <td>3.755300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73000</td>\n",
              "      <td>3.731200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73500</td>\n",
              "      <td>3.735600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74000</td>\n",
              "      <td>3.761100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74500</td>\n",
              "      <td>3.765700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75000</td>\n",
              "      <td>3.735500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75500</td>\n",
              "      <td>3.749600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76000</td>\n",
              "      <td>3.758200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76500</td>\n",
              "      <td>3.745700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77000</td>\n",
              "      <td>3.735900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77500</td>\n",
              "      <td>3.738800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78000</td>\n",
              "      <td>3.742200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78500</td>\n",
              "      <td>3.770900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79000</td>\n",
              "      <td>3.771400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79500</td>\n",
              "      <td>3.741600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80000</td>\n",
              "      <td>3.754100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80500</td>\n",
              "      <td>3.754300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81000</td>\n",
              "      <td>3.753500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81500</td>\n",
              "      <td>3.734200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82000</td>\n",
              "      <td>3.761200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82500</td>\n",
              "      <td>3.732100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83000</td>\n",
              "      <td>3.752900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83500</td>\n",
              "      <td>3.728700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84000</td>\n",
              "      <td>3.766300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84500</td>\n",
              "      <td>3.738100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85000</td>\n",
              "      <td>3.760300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85500</td>\n",
              "      <td>3.742100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86000</td>\n",
              "      <td>3.735100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86500</td>\n",
              "      <td>3.719900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87000</td>\n",
              "      <td>3.751800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87500</td>\n",
              "      <td>3.702000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88000</td>\n",
              "      <td>3.728800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88500</td>\n",
              "      <td>3.727000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89000</td>\n",
              "      <td>3.711700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89500</td>\n",
              "      <td>3.710300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90000</td>\n",
              "      <td>3.736100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90500</td>\n",
              "      <td>3.745400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91000</td>\n",
              "      <td>3.706500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91500</td>\n",
              "      <td>3.698400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92000</td>\n",
              "      <td>3.720200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92500</td>\n",
              "      <td>3.753100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93000</td>\n",
              "      <td>3.712200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93500</td>\n",
              "      <td>3.714400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94000</td>\n",
              "      <td>3.725300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94500</td>\n",
              "      <td>3.681700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95000</td>\n",
              "      <td>3.720100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95500</td>\n",
              "      <td>3.714700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96000</td>\n",
              "      <td>3.709200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96500</td>\n",
              "      <td>3.732100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97000</td>\n",
              "      <td>3.673100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97500</td>\n",
              "      <td>3.695300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98000</td>\n",
              "      <td>3.707700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98500</td>\n",
              "      <td>3.690500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99000</td>\n",
              "      <td>3.725000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99500</td>\n",
              "      <td>3.737600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100000</td>\n",
              "      <td>3.713200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100500</td>\n",
              "      <td>3.707300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101000</td>\n",
              "      <td>3.723000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101500</td>\n",
              "      <td>3.688400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102000</td>\n",
              "      <td>3.682600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102500</td>\n",
              "      <td>3.701200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103000</td>\n",
              "      <td>3.667900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103500</td>\n",
              "      <td>3.709200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104000</td>\n",
              "      <td>3.710600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104500</td>\n",
              "      <td>3.678600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105000</td>\n",
              "      <td>3.696300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105500</td>\n",
              "      <td>3.695500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106000</td>\n",
              "      <td>3.697400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106500</td>\n",
              "      <td>3.683600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107000</td>\n",
              "      <td>3.691600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107500</td>\n",
              "      <td>3.684500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108000</td>\n",
              "      <td>3.657400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108500</td>\n",
              "      <td>3.670300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109000</td>\n",
              "      <td>3.665800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109500</td>\n",
              "      <td>3.687900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110000</td>\n",
              "      <td>3.693500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110500</td>\n",
              "      <td>3.682900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111000</td>\n",
              "      <td>3.691300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111500</td>\n",
              "      <td>3.666200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112000</td>\n",
              "      <td>3.695800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112500</td>\n",
              "      <td>3.669500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113000</td>\n",
              "      <td>3.689800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113500</td>\n",
              "      <td>3.674900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114000</td>\n",
              "      <td>3.685900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114500</td>\n",
              "      <td>3.649200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115000</td>\n",
              "      <td>3.652300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115500</td>\n",
              "      <td>3.663400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116000</td>\n",
              "      <td>3.672100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116500</td>\n",
              "      <td>3.641500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117000</td>\n",
              "      <td>3.679800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117500</td>\n",
              "      <td>3.649500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118000</td>\n",
              "      <td>3.673100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118500</td>\n",
              "      <td>3.693000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119000</td>\n",
              "      <td>3.678800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119500</td>\n",
              "      <td>3.679000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120000</td>\n",
              "      <td>3.672200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120500</td>\n",
              "      <td>3.664500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121000</td>\n",
              "      <td>3.643500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121500</td>\n",
              "      <td>3.676300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122000</td>\n",
              "      <td>3.649900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122500</td>\n",
              "      <td>3.636700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123000</td>\n",
              "      <td>3.652600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123500</td>\n",
              "      <td>3.651200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124000</td>\n",
              "      <td>3.646900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124500</td>\n",
              "      <td>3.672800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125000</td>\n",
              "      <td>3.664500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125500</td>\n",
              "      <td>3.633600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126000</td>\n",
              "      <td>3.634400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126500</td>\n",
              "      <td>3.642600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127000</td>\n",
              "      <td>3.645700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127500</td>\n",
              "      <td>3.658100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128000</td>\n",
              "      <td>3.620000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128500</td>\n",
              "      <td>3.650900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129000</td>\n",
              "      <td>3.620500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129500</td>\n",
              "      <td>3.672300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130000</td>\n",
              "      <td>3.665000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130500</td>\n",
              "      <td>3.613000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131000</td>\n",
              "      <td>3.615800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131500</td>\n",
              "      <td>3.667500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132000</td>\n",
              "      <td>3.650500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132500</td>\n",
              "      <td>3.629600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133000</td>\n",
              "      <td>3.642500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133500</td>\n",
              "      <td>3.636100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134000</td>\n",
              "      <td>3.658000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134500</td>\n",
              "      <td>3.647200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135000</td>\n",
              "      <td>3.633500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135500</td>\n",
              "      <td>3.648700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136000</td>\n",
              "      <td>3.672900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136500</td>\n",
              "      <td>3.644700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137000</td>\n",
              "      <td>3.627500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137500</td>\n",
              "      <td>3.612800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138000</td>\n",
              "      <td>3.642700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138500</td>\n",
              "      <td>3.635900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139000</td>\n",
              "      <td>3.625600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139500</td>\n",
              "      <td>3.611400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140000</td>\n",
              "      <td>3.620800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140500</td>\n",
              "      <td>3.616100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141000</td>\n",
              "      <td>3.616600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141500</td>\n",
              "      <td>3.617400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142000</td>\n",
              "      <td>3.597200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142500</td>\n",
              "      <td>3.621700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143000</td>\n",
              "      <td>3.609700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143500</td>\n",
              "      <td>3.622200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144000</td>\n",
              "      <td>3.604300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144500</td>\n",
              "      <td>3.599500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145000</td>\n",
              "      <td>3.595100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145500</td>\n",
              "      <td>3.586200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146000</td>\n",
              "      <td>3.594400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146500</td>\n",
              "      <td>3.657200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147000</td>\n",
              "      <td>3.630300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147500</td>\n",
              "      <td>3.614800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148000</td>\n",
              "      <td>3.636200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148500</td>\n",
              "      <td>3.605800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149000</td>\n",
              "      <td>3.612300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149500</td>\n",
              "      <td>3.623400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150000</td>\n",
              "      <td>3.620400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150500</td>\n",
              "      <td>3.590200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151000</td>\n",
              "      <td>3.609600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151500</td>\n",
              "      <td>3.600400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152000</td>\n",
              "      <td>3.583000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152500</td>\n",
              "      <td>3.626200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153000</td>\n",
              "      <td>3.603100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153500</td>\n",
              "      <td>3.619900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154000</td>\n",
              "      <td>3.641200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154500</td>\n",
              "      <td>3.616100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155000</td>\n",
              "      <td>3.620600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155500</td>\n",
              "      <td>3.623900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156000</td>\n",
              "      <td>3.607400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156500</td>\n",
              "      <td>3.605200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157000</td>\n",
              "      <td>3.587200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157500</td>\n",
              "      <td>3.599800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158000</td>\n",
              "      <td>3.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158500</td>\n",
              "      <td>3.575400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159000</td>\n",
              "      <td>3.574400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159500</td>\n",
              "      <td>3.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160000</td>\n",
              "      <td>3.603000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160500</td>\n",
              "      <td>3.611400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161000</td>\n",
              "      <td>3.566600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161500</td>\n",
              "      <td>3.615100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162000</td>\n",
              "      <td>3.571300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162500</td>\n",
              "      <td>3.581900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163000</td>\n",
              "      <td>3.581500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163500</td>\n",
              "      <td>3.574000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164000</td>\n",
              "      <td>3.578700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164500</td>\n",
              "      <td>3.566000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165000</td>\n",
              "      <td>3.599700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165500</td>\n",
              "      <td>3.584300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166000</td>\n",
              "      <td>3.587500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166500</td>\n",
              "      <td>3.632200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167000</td>\n",
              "      <td>3.598800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167500</td>\n",
              "      <td>3.570100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168000</td>\n",
              "      <td>3.575700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168500</td>\n",
              "      <td>3.608400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169000</td>\n",
              "      <td>3.607100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169500</td>\n",
              "      <td>3.590300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170000</td>\n",
              "      <td>3.575900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170500</td>\n",
              "      <td>3.604300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171000</td>\n",
              "      <td>3.600400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171500</td>\n",
              "      <td>3.592300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172000</td>\n",
              "      <td>3.559900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172500</td>\n",
              "      <td>3.577000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173000</td>\n",
              "      <td>3.594700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173500</td>\n",
              "      <td>3.576800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174000</td>\n",
              "      <td>3.613900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=174110, training_loss=3.7825945418129274, metrics={'train_runtime': 17642.8655, 'train_samples_per_second': 78.949, 'train_steps_per_second': 9.869, 'total_flos': 8.31857984054231e+17, 'train_loss': 3.7825945418129274, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyzXwwDjPR-H"
      },
      "outputs": [],
      "source": [
        "peft_model_id=\"peft-lora-mt5-goud-results\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJ37AEcaPR-H",
        "outputId": "5e3d86e1-df3c-4af4-cbd0-14dc3eaf511b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('peft-lora-mt5-goud-results/tokenizer_config.json',\n",
              " 'peft-lora-mt5-goud-results/special_tokens_map.json',\n",
              " 'peft-lora-mt5-goud-results/spiece.model',\n",
              " 'peft-lora-mt5-goud-results/added_tokens.json',\n",
              " 'peft-lora-mt5-goud-results/tokenizer.json')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# trainer.model.save_pretrained(peft_model_id)\n",
        "# tokenizer.save_pretrained(peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ff8860c96231496a83995df93aa5b870",
            "95d8f0fc17824720a34d4e75b778b1ed",
            "56e2bd214de94322ac57fa72945058aa",
            "b77fa9fe51ed4b2f9b73db57b14eeecb",
            "c4aec675d46d456d9826e38d3879f121"
          ]
        },
        "id": "b1q0Vg5rPR-H",
        "outputId": "902b96b8-3a6f-405c-995e-e8a80b3c7cb3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff8860c96231496a83995df93aa5b870",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95d8f0fc17824720a34d4e75b778b1ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/2.77M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56e2bd214de94322ac57fa72945058aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b77fa9fe51ed4b2f9b73db57b14eeecb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4aec675d46d456d9826e38d3879f121",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/alizaidi/lora-mt5-goud/commit/fee52adbeda959a97d9b839e0b8f0a5f315b0713', commit_message='alizaidi/lora-mt5-goud-ar', commit_description='', oid='fee52adbeda959a97d9b839e0b8f0a5f315b0713', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# trainer.push_to_hub(\"alizaidi/lora-mt5-goud-ar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation 1: LoRA Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/alizaidi/micromamba/envs/peft/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "config = PeftConfig.from_pretrained(\"alizaidi/lora-mt5-goud\")\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-small\")\n",
        "device_map = {\"\": 0} if torch.cuda.is_available() else None\n",
        "model = PeftModel.from_pretrained(base_model, \"alizaidi/lora-mt5-goud\", device_map=device_map)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"alizaidi/lora-mt5-goud\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ø¨Ø§Ù„ÙÙŠØ¯ÙŠÙˆ. Ø·Ø§Ù„Ø¨Ø© ÙƒÙ„ÙŠØ§Øª ÙØ§Ø³ Ø®Ø±Ø¬Ø§Øª Ù„Ù„Ø§Ø­ØªØ¬Ø§Ø¬ Ø¹Ù„Ù‰ ÙØªØ­ Ø§Ù„Ø£Ø­ÙŠØ§Ø¡ ÙˆØ§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª\n"
          ]
        }
      ],
      "source": [
        "text = dataset[\"test\"][0][\"article\"]\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=128)\n",
        "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pmCxzwxwPR-K",
        "outputId": "b1101128-bfaf-434a-9452-c2d659fe4349"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Ø±ÙˆØ¨ÙˆØ±Ø·Ø§Ø¬.. Ø§Ù„Ø¨ÙŠØ§ØªØ© Ø¨Ø§Ù„Ù„ÙŠÙ„ Ø¹Ù„Ù‰ Ø¨Ø±Ø§ Ø´ÙƒÙ„ Ø§Ø­ØªØ¬Ø§Ø¬ÙŠ Ø¬Ø¯ÙŠØ¯ Ù„Ù„Ù…Ø·Ø§Ù„Ø¨Ø© Ø¨ÙØªØ­ Ø§Ù„Ø£Ø­ÙŠØ§Ø¡ Ø§Ù„Ø¬Ø§Ù…Ø¹ÙŠØ© Ø¨ÙØ§Ø³'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"test\"][0][\"headline\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "wLv8oPFtPR-K"
      },
      "outputs": [],
      "source": [
        "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
        "    \"\"\"\n",
        "    split the dataset into smaller batches that we can process simultaneously\n",
        "    Yield successive batch-sized chunks from list_of_elements.\n",
        "    \"\"\"\n",
        "    for i in range(0, len(list_of_elements), batch_size):\n",
        "        yield list_of_elements[i : i + batch_size]\n",
        "\n",
        "\n",
        "def calculate_metric_on_test_ds(\n",
        "    dataset,\n",
        "    metric,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    batch_size=16,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    column_text=\"article\",\n",
        "    column_summary=\"highlights\",\n",
        "):\n",
        "    article_batches = list(\n",
        "        generate_batch_sized_chunks(dataset[column_text], batch_size)\n",
        "    )\n",
        "    target_batches = list(\n",
        "        generate_batch_sized_chunks(dataset[column_summary], batch_size)\n",
        "    )\n",
        "\n",
        "    for article_batch, target_batch in tqdm(\n",
        "        zip(article_batches, target_batches), total=len(article_batches)\n",
        "    ):\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            article_batch,\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        summaries = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"].to(device),\n",
        "            attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "            # length_penalty=0.8,\n",
        "            # num_beams=8,\n",
        "            # max_length=128,\n",
        "        )\n",
        "        \"\"\" parameter for length penalty ensures that the model does not generate sequences that are too long. \"\"\"\n",
        "\n",
        "        # Finally, we decode the generated texts,\n",
        "        # replace the  token, and add the decoded texts with the references to the metric.\n",
        "        decoded_summaries = [\n",
        "            tokenizer.decode(\n",
        "                s, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "            )\n",
        "            for s in summaries\n",
        "        ]\n",
        "\n",
        "        decoded_summaries = [d.replace(\"\", \" \").strip().lower() for d in decoded_summaries]\n",
        "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
        "\n",
        "    #  Finally compute and return the ROUGE scores.\n",
        "    score = metric.compute()\n",
        "    return score\n",
        "\n",
        "\n",
        "def evaluation(tokenizer, model, dataset):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # loading data\n",
        "    rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "    rouge_metric = load_metric(\"rouge\")\n",
        "    score = calculate_metric_on_test_ds(\n",
        "        dataset[\"test\"][0:10],\n",
        "        rouge_metric,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        batch_size=2,\n",
        "        column_text=\"article\",\n",
        "        column_summary=\"headline\",\n",
        "        device=\"cpu\"\n",
        "    )\n",
        "\n",
        "    rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
        "\n",
        "    return rouge_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "collapsed": true,
        "id": "FgQhvsIvPR-K",
        "outputId": "2e1c2001-a726-477c-aa2e-28819be7742c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]/home/alizaidi/micromamba/envs/peft/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.42it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'rouge1': 0.1, 'rouge2': 0.0, 'rougeL': 0.1, 'rougeLsum': 0.1}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluation(tokenizer, model, dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy6YM9GdPR-K"
      },
      "source": [
        "## Evaluation 2: Evaluation on Already-Finetuned Models (AraBERT, DziriBERT, DarijaBERT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dlbRoHjyXcB8"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"Goud/Goud-sum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "jMbl7fK_itTo"
      },
      "outputs": [],
      "source": [
        "# List of models to evaluate\n",
        "models = [\n",
        "    \"Goud/AraBERT-summarization-goud\",\n",
        "    \"Goud/DziriBERT-summarization-goud\",\n",
        "    \"Goud/DarijaBERT-summarization-goud\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlIoun3gi9mA",
        "outputId": "bdc7b0fe-062c-4b1b-d303-37adf0f88b14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Goud/AraBERT-summarization-goud\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4749/4749 [46:38<00:00,  1.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE scores for Goud/AraBERT-summarization-goud:\n",
            "{'rouge1': 0.022797056089286873, 'rouge2': 0.0014566003299287494, 'rougeL': 0.022827197556336394, 'rougeLsum': 0.02274151487234012}\n",
            "\n",
            "\n",
            "Evaluating model: Goud/DziriBERT-summarization-goud\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4749/4749 [48:19<00:00,  1.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE scores for Goud/DziriBERT-summarization-goud:\n",
            "{'rouge1': 0.018970946146984042, 'rouge2': 0.001262554089762682, 'rougeL': 0.018998986508380316, 'rougeLsum': 0.018859523893034066}\n",
            "\n",
            "\n",
            "Evaluating model: Goud/DarijaBERT-summarization-goud\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            " 23%|â–ˆâ–ˆâ–       | 1077/4749 [11:28<38:57,  1.57it/s]"
          ]
        }
      ],
      "source": [
        "# Evaluate each model\n",
        "for model_name in models:\n",
        "    print(f\"Evaluating model: {model_name}\")\n",
        "\n",
        "    if \"AraBERT\" in model_name or \"DziriBERT\" in model_name or \"DarijaBERT\" in model_name:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        tokenizer.model_max_length = 1024\n",
        "        model = EncoderDecoderModel.from_pretrained(model_name)\n",
        "        model.config.max_position_embeddings = 1024\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    rouge_scores = evaluation(tokenizer, model, dataset)\n",
        "    print(f\"ROUGE scores for {model_name}:\")\n",
        "    print(rouge_scores)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH1Qcxd3PR-K"
      },
      "source": [
        "# Activity: Native Language Article Summarization\n",
        "\n",
        "**Task:** Fetch an article in your native language and assess the summarization and headline generation capabilities of ChatGPT.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. **Select an Article:** Choose a relevant and recent article written in your native language. Ensure it is of short or medium length.\n",
        "\n",
        "2. **Summarize with ChatGPT:** Use ChatGPT to generate a summary of the selected article.\n",
        "\n",
        "3. **Evaluate Summarization Quality:**\n",
        "    - **Impression:** Share your impression of the summarization quality. Consider the following:\n",
        "        - **Accuracy:** Does the summary capture the main points and essence of the article?\n",
        "        - **Clarity:** Is the summary clear and easy to understand?\n",
        "        - **Coverage:** Does the summary include all critical information from the article?\n",
        "\n",
        "4. **Provide Feedback:** Offer constructive feedback on the summarization. Highlight any discrepancies or areas for improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNFLl70zPR-K",
        "outputId": "7ac8d86f-f530-4405-8996-150983c5bf8c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<iframe\n",
              "\tsrc=\"https://forms.gle/sggLJWMFQ4JQCmHL8\",\n",
              "  width=\"80%\"\n",
              "\theight=\"320px\" >\n",
              "\tLoading...\n",
              "</iframe>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title Generate Quiz Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/sggLJWMFQ4JQCmHL8\",\n",
        "  width=\"80%\"\n",
        "\theight=\"320px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXgW_EWSPR-K"
      },
      "source": [
        "# Section 2: Summarization using GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0mH6ieSPR-K"
      },
      "source": [
        "In this section, we will utilize both OpenAI's Large and Small language models to perform the same task of abstractive text summarization. We will then evaluate their performance and compare the results with those obtained from the models discussed in Section 1.\n",
        "\n",
        "Unlike traditional fine-tuning approaches that involve updating model weights, the initial step in adapting a GPT-based model for a specific task is prompt engineering, which does not require weight updates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5qHU4lEPR-K"
      },
      "source": [
        "<div style=\"display: flex; align-items: flex-start;\">\n",
        "    <figure style=\"margin-right: 10px; text-align: center;\">\n",
        "        <a href=\"https://arxiv.org/pdf/2005.14165\" target=\"_blank\">\n",
        "            <img src=\"https://github.com/stevenkolawole/indaba-low-resource-nlp-prac/blob/main/content/traditional-finetuning.png?raw=1\" width=\"80%\" />\n",
        "        </a>\n",
        "        <figcaption><a href=\"https://arxiv.org/pdf/2005.14165\" target=\"_blank\">Traditional Fine-Tuning</a></figcaption>\n",
        "    </figure>\n",
        "    <figure style= \"text-align: center;\">\n",
        "        <a href=\"https://arxiv.org/pdf/2005.14165\" target=\"_blank\">\n",
        "            <img src=\"https://github.com/stevenkolawole/indaba-low-resource-nlp-prac/blob/main/content/prompting.png?raw=1\" width=\"80%\" />\n",
        "        </a>\n",
        "        <figcaption><a href=\"https://arxiv.org/pdf/2005.14165\" target=\"_blank\">Prompting</a></figcaption>\n",
        "    </figure>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGBTE4UuPR-K"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yf9I37FGPR-K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from rouge_metric import PyRouge\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import random\n",
        "import openai\n",
        "import time\n",
        "from openai import OpenAI\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vxxstq3PR-K"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awh2SUuXPR-L"
      },
      "outputs": [],
      "source": [
        "def evaluate_rouge(hypotheses, references):\n",
        "    these_refs = [[ref.strip().lower()] for ref in references]\n",
        "    rouge = PyRouge(rouge_n=(1, 2), rouge_l=True)\n",
        "    scores = rouge.evaluate(hypotheses, these_refs)\n",
        "    print(scores)\n",
        "\n",
        "def substring_after_colon(input_string):\n",
        "    colon_index = input_string.find(':')\n",
        "    if colon_index != -1:\n",
        "        return input_string[colon_index + 1:]\n",
        "    else:\n",
        "        return input_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnQnZR3CPR-L"
      },
      "outputs": [],
      "source": [
        "# Define dataset and paths\n",
        "DATASET = \"Goud\"\n",
        "MAX_TRAIN = 0\n",
        "model_name = \"gpt-4o-mini\"\n",
        "\n",
        "output_filename = f\"./{DATASET}_{model_name}_test_generated_{MAX_TRAIN}.csv\"\n",
        "\n",
        "# Load dataset\n",
        "goud_data = dataset\n",
        "train_source = goud_data[\"train\"][\"article\"]\n",
        "train_target = goud_data[\"train\"][\"headline\"]\n",
        "test_source = goud_data[\"test\"][\"article\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flHCWszSPR-L"
      },
      "source": [
        "## Function to summarize news articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee4p9nJ4PR-L"
      },
      "outputs": [],
      "source": [
        "key = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAh6RPXpPR-L"
      },
      "outputs": [],
      "source": [
        "def summarize_news_article(MAX_TRAIN=20):\n",
        "\n",
        "    client = OpenAI(api_key=key)\n",
        "    rewritten_prompt_count = 0\n",
        "    line_count = 0\n",
        "    wait_time = 1\n",
        "    df_lines = []\n",
        "    tokens_consumption = 0\n",
        "    existing_len = 0\n",
        "    if os.path.exists(output_filename):\n",
        "        existing_df = pd.read_csv(output_filename)\n",
        "        existing_len = existing_df.shape[0]\n",
        "        rewritten_prompt_count = existing_len\n",
        "        line_count = existing_len\n",
        "        df_lines = existing_df.to_dict('records')\n",
        "\n",
        "    for data in tqdm(test_source[existing_len:], desc=f\"Lines processed from {existing_len}-th line\"):\n",
        "        news_article = data.strip()\n",
        "        line_count += 1\n",
        "        made_error = True\n",
        "        num_error = 0\n",
        "        while made_error:\n",
        "            messages = [{\"role\": \"system\", \"content\": \"You are asked to summarize a news article written in Modern Standard Arabic and Moroccan Darija, and write that summary as a clickbait headline, in Moroccan Darija only.\\n\"}]\n",
        "            if MAX_TRAIN - num_error > 0:\n",
        "                for _ in range(MAX_TRAIN - num_error):\n",
        "                    idx = random.choice(range(len(train_source)))\n",
        "                    train_src = train_source[idx]\n",
        "                    train_tgt = train_target[idx]\n",
        "                    messages.append({\"role\": \"user\", \"content\": f\"Summarize the following news article into a headline in Moroccan Darija only:\\n\\\"{train_src}\\\"\"})\n",
        "                    messages.append({\"role\": \"assistant\", \"content\": f\"Absolutely! Here is the headline summarizing your news article:\\n\\\"{train_tgt}\\\"\"})\n",
        "            messages.append({\"role\": \"user\", \"content\": f\"Summarize the following news article into a headline in Moroccan Darija only:\\n\\\"{news_article}\\\"\"})\n",
        "            try:\n",
        "                response = client.chat.completions.create(\n",
        "                    messages=messages,\n",
        "                    model=model_name,\n",
        "                )\n",
        "                headline = response.choices[0].message.content\n",
        "                df_lines.append({\"article\": news_article,\"generated_headline\": headline,\"prompt_messages\":messages})\n",
        "\n",
        "                rewritten_prompt_count += 1\n",
        "                made_error = False\n",
        "            except Exception as e:\n",
        "                if isinstance(e, openai.RateLimitError):\n",
        "                    print(\"Rate limit error\")\n",
        "                    print(f\"Wait for {wait_time} seconds because all calls failed: \", flush=True)\n",
        "                    time.sleep(wait_time)\n",
        "                    wait_time *= 2\n",
        "                else:\n",
        "                    print(e)\n",
        "                    num_error += 1\n",
        "                    print(\"May be too long, reducing context to:\", MAX_TRAIN - num_error)\n",
        "            #time.sleep(1)\n",
        "\n",
        "    df = pd.DataFrame.from_dict(df_lines)\n",
        "    df.to_csv(output_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQBh6s0OPR-L"
      },
      "source": [
        "## Execute the summarization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_X4eMBBPR-L",
        "outputId": "db1d8d40-feed-4d8e-e0e6-20eb83647fc5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Lines processed from 0-th line: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9497/9497 [2:34:22<00:00,  1.03it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 9263.82370686531 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#record cell running time\n",
        "import time\n",
        "start_time = time.time()\n",
        "summarize_news_article(0)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrOv9fb3PR-L"
      },
      "source": [
        "## Load Generated output and evaluate ROUGE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHtGkyMsPR-L"
      },
      "source": [
        "In the \"generated_responses\" folder you will find the gpt responses corresponding to 0,1,5,20 shot prompts.\n",
        "\n",
        "Evaluate the generated headline summaries by running ROUGE evaluation and add the results to the table of results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2Ad0a1jPR-L"
      },
      "source": [
        "### ROUGE Metric Results: 0 Shot\n",
        "\n",
        "| Metric   | Recall (r)        | Precision (p)     | F1-Score (f)      |\n",
        "|----------|-------------------|-------------------|-------------------|\n",
        "| ROUGE-1  | 0.1228            | 0.1069            | 0.1143            |\n",
        "| ROUGE-2  | 0.0282            | 0.0235            | 0.0256            |\n",
        "| ROUGE-L  | 0.1128            | 0.0980            | 0.1049            |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJ8hyKQCPR-L",
        "outputId": "6a05e8e4-874d-41ff-def5-028a61b2523a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'rouge-1': {'r': 0.11884854492859614, 'p': 0.12977184865821972, 'f': 0.12407023545586798}, 'rouge-2': {'r': 0.0325784137233658, 'p': 0.03389945881811894, 'f': 0.03322581039836068}, 'rouge-l': {'r': 0.11128523451125509, 'p': 0.12142149826298465, 'f': 0.11613260817866634}}\n"
          ]
        }
      ],
      "source": [
        "shot_count = 0\n",
        "hypotheses = pd.read_csv(f\".\\generated_responses\\{model_name}\\Goud_{model_name}_test_generated_{str(shot_count)}.csv\", encoding = \"UTF-8\")[\"generated_headline\"].tolist()\n",
        "hypotheses = [substring_after_colon(hypo).replace(\"\\\"\", \"\").strip() for hypo in hypotheses]\n",
        "references = goud_data[\"test\"][\"headline\"]\n",
        "evaluate_rouge(hypotheses, references)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-Pc2PTjPR-L"
      },
      "source": [
        "### ROUGE Metric Results: 20 Shot\n",
        "Pick one or more of the files that have the previously generated N-shot GPT responses, present in the \"generated_responses\" folder, run the evaluation and then populate the table below\n",
        "\n",
        "| Metric   | Recall (r)        | Precision (p)     | F1-Score (f)      |\n",
        "|----------|-------------------|-------------------|-------------------|\n",
        "| ROUGE-1  |             |             |             |\n",
        "| ROUGE-2  |            |             |            |\n",
        "| ROUGE-L  |             |             |           |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwmRpTerPR-L",
        "outputId": "951e34f1-f0c6-4652-e9c6-a8dcb30e5405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'rouge-1': {'r': 0.13750397869020445, 'p': 0.1306205269799418, 'f': 0.13397389480280544}, 'rouge-2': {'r': 0.03387859852289136, 'p': 0.03194819653833152, 'f': 0.032885092553751126}, 'rouge-l': {'r': 0.1264128630910928, 'p': 0.11986076103165903, 'f': 0.12304965282629712}}\n"
          ]
        }
      ],
      "source": [
        "shot_count = 20\n",
        "model_name  = \"gpt4\"  #\"C:\\Users\\salamaaya\\OneDrive - Microsoft\\Desktop\\DLI\\Indaba2024-practical\\indaba-low-resource-nlp-prac\\generated_responses\\gpt4\\Goud_test_generated_5.csv\"\n",
        "hypotheses = pd.read_csv(f\".\\generated_responses\\{model_name}\\Goud_test_generated_{str(shot_count)}.csv\", encoding = \"UTF-8\")[\"generated_headline\"].tolist()\n",
        "hypotheses = [substring_after_colon(hypo).replace(\"\\\"\", \"\").strip() for hypo in hypotheses]\n",
        "references = goud_data[\"test\"][\"headline\"]\n",
        "evaluate_rouge(hypotheses, references)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkDCrKpbPR-L"
      },
      "outputs": [],
      "source": [
        "# @title Generate Quiz Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/zbJoTSz3nfYq1VrY6\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s1NaD5bPR-L"
      },
      "source": [
        "## Conclusion\n",
        "**Summary:**\n",
        "\n",
        "[Summary of the main points/takeaways from the prac.]\n",
        "\n",
        "**Next Steps:**\n",
        "\n",
        "[Next steps for people who have completed the prac, like optional reading (e.g. blogs, papers, courses, youtube videos). This could also link to other pracs.]\n",
        "\n",
        "**Appendix:**\n",
        "\n",
        "[Anything (probably math heavy stuff) we don't have space for in the main practical sections.]\n",
        "\n",
        "**References:**\n",
        "\n",
        "[References for any content used in the notebook.]\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2022)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e761XjLuPR-L"
      },
      "source": [
        "## Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yebMVSHHPR-L",
        "outputId": "64d5953f-3f67-415e-edd8-d91f825902f1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<iframe\n",
              "\tsrc=\"https://forms.gle/WUpRupqfhFtbLXtN6\",\n",
              "  width=\"80%\"\n",
              "\theight=\"1200px\" >\n",
              "\tLoading...\n",
              "</iframe>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/WUpRupqfhFtbLXtN6\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaCrNDBwPR-L"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WILOYJH4gCnD"
      ],
      "gpuType": "T4",
      "name": "Indaba_2022_Prac_Template.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "vscode": {
      "interpreter": {
        "hash": "145833166d986a8417df3c7acb65d917d84b716b5a452e57fcacdc66f1a168c9"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
