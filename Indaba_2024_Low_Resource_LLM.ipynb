{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Low-resource NLP**\n",
        "\n",
        "<img src=\"./content/lr_llm_header.png\" width=\"60%\" allign =\"center\"/>\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2024/blob/main/practicals/Indaba_2024_Prac_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [Change colab link to point to prac.]\n",
        "\n",
        "Â© Deep Learning Indaba 2024. Apache License 2.0.\n",
        "\n",
        "**Authors:**\n",
        "- Ali Zaidi\n",
        "- Aya Salama\n",
        "- Khalil Mrini\n",
        "- Steven Kolawole \n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "Low-resource NLP (Natural Language Processing) refers to the study and development of NLP models and systems for languages, tasks, or domains that have limited data and resources available. These can include languages with fewer digital text corpora, limited computational tools, or less-developed linguistic research.\n",
        "\n",
        "**Key Challenges in Low-Resource NLP**\n",
        "\n",
        "1. **Data Scarcity:**\n",
        "   - **Limited Training Data:** Many languages lack large annotated corpora necessary for training NLP models.\n",
        "   - **Lack of Pre-trained Models:** Popular NLP models like BERT, GPT, and others are often not available for low-resource languages.\n",
        "\n",
        "2. **Linguistic Diversity:**\n",
        "   - **Morphological Complexity:** Some languages have complex grammatical structures and morphological richness.\n",
        "   - **Dialectal Variations:** A lack of standardized versions can complicate NLP tasks.\n",
        "\n",
        "3. **Resource Limitations:**\n",
        "   - **Computational Constraints:** Low-resource scenarios often involve limited access to computational power and storage.\n",
        "   - **Expertise and Tools:** Fewer linguistic experts and fewer NLP tools are tailored for these languages.\n",
        "\n",
        "**Topics:**\n",
        "\n",
        "Content: [Natural Language Processing, Low-resource, Large Language Models, Parameter Efficient Finetuning, Adaptation]  \n",
        "Level: [Intermediate]\n",
        "\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "- Exploring data scarcity challenges\n",
        "- Exploring Compute resource limitations and addressing them with Parameter efficient finetuning\n",
        "- Comparing Performance between Small (BERT) and Large (GPT) Language models on low-resource languages/tasks\n",
        "  \n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "[Knowledge required for this prac. You can link a relevant parallel track session, blogs, papers, courses, topics etc.]\n",
        "_ link resources on LLMs, Bert, Masakhane papers on low resouce conversations\n",
        "\n",
        "**Outline:**\n",
        "\n",
        "[Points that link to each section. Auto-generate following the instructions [here](https://stackoverflow.com/questions/67458990/how-to-automatically-generate-a-table-of-contents-in-colab-notebook).]\n",
        "\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "[Tasks just before starting.]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Storyline: working on a task with scarce data which is summarization in Moroccan Darija.\n",
        "this task pose resources constrainst as the Moroccan Darija can be considered a low resource dialect of Arabic, we will set this task in a coumputational resource poor environment so our training should be able to run on a commodity GPU \n",
        "we will be using parameter efficient fine tuning technique (LORA) to optimize the training procedure in order to make it feasible "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation and Imports [should download any needed resources]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install arabert\n",
        "!pip install accelerate -U\n",
        "!pip install transformers[torch]\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset, concatenate_datasets, load_metric\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PeftConfig, PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from tqdm import tqdm\n",
        "from huggingface_hub import login\n",
        "from dotenv import load_dotenv\n",
        "import wandb\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /home/alizaidi/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malizaidi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "load_dotenv()\n",
        "login(token=os.getenv(\"HF_HUB_TOKEN\"))\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#download the datset [done]\n",
        "#download the model checkpoints\n",
        "#download the GPT outputs [done]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this practical we are interested in generating headlines for news articles featured on the news website [Goud.ma](www.Gound.ma).\n",
        "[TODO]This repo holds the training code for Goud.ma: a News Dataset for Summarization in Moroccan Darija\n",
        "\n",
        "We will frame this as a summarization task where the input is the body of a news article and the output is an appropriate headline. The [Goud dataset](https://github.com/issam9/goud-summarization-dataset) contains 158k articles and their headlines. All headlines are in Moroccan Darija, while articles may be in Moroccan Darija, in Modern Standard Arabic, or a mix of both (code-switched Moroccan Darija).\n",
        "\n",
        "**Data Fields**\n",
        "- *article*: a string containing the body of the news article\n",
        "- *headline*: a string containing the article's headline\n",
        "- *categories*: a list of string of article categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What we will do:\n",
        "<img src=\"./content/DLI_LR_llm_prac_1.png\" width=\"40%\" />\n",
        "\n",
        "**Resource folders**:\n",
        "\n",
        "The resources you'll need to run this practical are part of the repo, you'll find them in the parent folder:\n",
        "\n",
        "- *Data* folder: this folder has the Goud-sum dataset that we will be utilizing in the pratical\n",
        "- *genrated_responses* folder: this folder has pregenrated summaries that will be utilized in Section2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Metric: ROUGE\n",
        "\n",
        "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate the quality of summaries by comparing them to reference (or ground truth) summaries. ROUGE is widely used in Natural Language Processing (NLP) tasks, particularly for evaluating the performance of text summarization models.\n",
        "\n",
        "![ROUGE-Base](https://i0.wp.com/blog.uptrain.ai/wp-content/uploads/2024/01/rouge-n.webp?resize=700%2C228&ssl=1)\n",
        "\n",
        "### Key ROUGE Variants\n",
        "\n",
        "1. **ROUGE-N**: Measures the overlap of n-grams between the candidate summary and the reference summary.\n",
        "\n",
        "![ROUGE-1](https://clementbm.github.io/assets/2021-12-23/rouge-unigrams.png)\n",
        "\n",
        "*caption:*\n",
        "$ROUGE_1 = \\frac{7}{10} = 0.7$\n",
        "\n",
        "   - **ROUGE-1**: Overlap of unigrams (1-gram).\n",
        "   - **ROUGE-2**: Overlap of bigrams (2-grams).\n",
        "   - **ROUGE-L**: Measures the longest common subsequence (LCS) between the candidate and reference summaries.\n",
        "\n",
        "2. **ROUGE-L**: Measures the longest common subsequence (LCS) between the candidate summary and the reference summary. Unlike ROUGE-N, ROUGE-L considers sentence-level structure similarity by identifying the longest co-occurring sequence of words in both summaries.\n",
        "\n",
        "\n",
        "\n",
        "3. **ROUGE-W**: A weighted version of ROUGE-L that gives more importance to the contiguous LCS.\n",
        "\n",
        "4. **ROUGE-S**: Measures the overlap of skip-bigrams, which are pairs of words in their order of appearance that can have any number of gaps between them.\n",
        "\n",
        "### How ROUGE is Computed\n",
        "\n",
        "ROUGE metrics can be calculated in terms of three measures:\n",
        "\n",
        "- **Recall**: The ratio of overlapping units (n-grams, LCS, or skip-bigrams) between the candidate summary and the reference summary to the total units in the reference summary. It answers, \"How much of the reference summary is captured by the candidate summary?\"\n",
        "\n",
        "- **Precision**: The ratio of overlapping units between the candidate summary and the reference summary to the total units in the candidate summary. It answers, \"How much of the candidate summary is relevant to the reference summary?\"\n",
        "\n",
        "- **F1-Score**: The harmonic mean of Precision and Recall. This gives a balanced measure that considers both precision and recall.\n",
        "\n",
        "### Importance of ROUGE\n",
        "\n",
        "ROUGE is essential for summarization tasks because it provides a standardized way to evaluate and compare different summarization models. Higher ROUGE scores generally indicate that the candidate summary is more similar to the reference summary, meaning the model is likely performing well.\n",
        "\n",
        "#### NOTE: Caveat\n",
        "<div style=\"display: flex; justify-content: space-between;\">\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*8ZNpaag-Nr2GLs3A-sz0aQ.png\" alt=\"limitation 1\" width=\"250\"/>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*CLIKeyKYiR6sNA4yjIkCWg.png\" alt=\"limitation 2\" width=\"250\"/>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*667HMbjSLJhwR_xqBau3JQ.png\" alt=\"limitation 3\" width=\"250\"/>\n",
        "</div>\n",
        "\n",
        "While ROUGE and other evaluation metrics (e.g., BLEU, METEOR, etc) serve as valuable tools for quick and straightforward evaluation of language models, they have certain limitations that render them less than ideal. To begin with, they fall short when it comes to assessing the fluency, coherence, and overall meaning of passages. They are also relatively insensitive to word order. ROUGE primarily measures lexical overlap and may not fully capture the semantic meaning or quality of a summary. For these reasons, researchers are still trying to find improved metrics.\n",
        "\n",
        "Therefore, these metrics are not shoe-in replacements for human evaluation, but are best used in conjunction with human evaluations for a more comprehensive assessment of summary quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "# Load the dataset from disk\n",
        "dataset = load_from_disk(\"./data/Goud-sum/Goud-sum\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['article', 'headline', 'categories'],\n",
            "        num_rows: 139288\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['article', 'headline', 'categories'],\n",
            "        num_rows: 9497\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['article', 'headline', 'categories'],\n",
            "        num_rows: 9497\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'article': 'Ù…Ù†ÙŠØ± Ø§Ù„Ø¹Ù„Ù…ÙŠ Ù…Ù† Ù…Ø±Ø§ÙƒØ´: ØªØ­ÙˆÙ„ ÙØ¶Ø§Ø¡ Ù…Ù‚Ø± Ø§Ù„ØºØ±ÙØ© Ø§Ù„ÙÙ„Ø§Ø­ÙŠØ© Ø¨Ù…Ø¯ÙŠÙ†Ø© Ù…Ø±Ø§ÙƒØ´ØŒ Ø§Ù„Ø°ÙŠ ÙŠØ­ØªØ¶Ù† ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø§Ù”Ø«Ù†Ø§Ø¡ØŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø±ÙŠÙ”ÙŠØ³ ÙˆØ§Ù”Ø¹Ø¶Ø§Ø¡ Ø§Ù„Ù…ÙƒØªØ¨ Ø§Ù„Ù…Ø³ÙŠØ± Ù„Ù„ØºØ±ÙØ© Ø§Ù„ÙÙ„Ø§Ø­ÙŠØ© Ø¨Ø¬Ù‡Ø© Ù…Ø±Ø§ÙƒØ´ Ø§Ù“Ø³ÙÙŠØŒ Ø§Ù•Ù„Ù‰ Ø­Ù„Ø¨Ø© Ù„Ù„Ø§Ø´ØªØ¨Ø§ÙƒØ§Øª ÙˆØ§Ù„Ù…Ù„Ø§Ø³Ù†Ø§ØªØŒ Ø¨Ø¹Ø¯ Ø§Ø´ØªØ¯Ø§Ø¯ Ø§Ù„Ø®Ù„Ø§Ù Ø¨ÙŠÙ† Ø§Ù„Ø¨Ø±Ù„Ù…Ø§Ù†ÙŠÙŠÙ† Ø­Ù…ÙŠØ¯ Ø§Ù„Ø¹ÙƒØ±ÙˆØ¯ ÙˆØ¹Ù…Ø± Ø®ÙÙŠÙØŒ Ø§Ù„Ù„Ø°ÙŠÙ† ÙŠÙ†ØªÙ…ÙŠØ§Ù† Ø§Ù•Ù„Ù‰ Ø­Ø²Ø¨ Ø§Ù„ØªØ¬Ù…Ø¹ Ø§Ù„ÙˆØ·Ù†ÙŠ Ù„Ù„Ø§Ù”Ø­Ø±Ø§Ø±ØŒ Ù…Ø§ ÙƒØ§Ø¯ ÙŠØ¹ØµÙ Ø¨Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ Ø¨Ø¹Ø¯ Ø§Ù†Ø·Ù„Ø§Ù‚ Ø´Ø±Ø§Ø±Ø© Ø§Ù„Ø§Ø´ØªØ¨Ø§Ùƒ Ø¨Ø§Ù„Ø§Ù”ÙŠØ§Ø¯ÙŠ Ø§Ù„ØªÙŠ Ø§Ù”Ø¬Ù‡Ø¶Øª ÙÙŠ Ù…Ù‡Ø¯Ù‡Ø§ Ø¨ØªØ¯Ø®Ù„ Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø§Ø¶Ø±ÙŠÙ†. ÙˆØ­Ø³Ø¨ Ø´Ù‡ÙˆØ¯ Ø¹ÙŠØ§Ù†ØŒ ÙØ§Ù•Ù† Ø¹Ù…Ø± Ø®ÙÙŠÙØŒ Ø§Ù„Ø°ÙŠ ÙŠØ´ØºÙ„ Ø±ÙŠÙ”ÙŠØ³ Ø¬Ù…Ø§Ø¹Ø© Ø§Ù”ÙƒÙØ§ÙŠØŒ ÙˆÙ…Ø¯Ø¹Ù… Ø§Ù„Ø­Ø¨ÙŠØ¨ Ø¨Ù† Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„Ù…Ù†Ø³Ù‚ Ø§Ù„Ø§Ù‚Ù„ÙŠÙ…ÙŠ Ù„Ø­Ø²Ø¨ Ø§Ù„Ø§Ù”ØµØ§Ù„Ø© ÙˆØ§Ù„Ù…Ø¹Ø§ØµØ± Ø§Ù„Ø°ÙŠ ÙŠØªØ¬Ù‡ Ù„ØªÙˆÙ„ÙŠ Ø±ÙŠÙ”Ø§Ø³Ø© Ø§Ù„ØºØ±ÙØ© Ù„ÙˆÙ„Ø§ÙŠØ© ØªØ§Ù†ÙŠØ©ØŒ Ø±ÙØ¶ Ø¯Ø®ÙˆÙ„ Ø­Ù…ÙŠØ¯ Ø§Ù„Ø¹ÙƒØ±ÙˆØ¯ Ù„Ù„Ù…Ù†Ø§ÙØ³Ø© Ø¹Ù„Ù‰ Ø±ÙŠÙ”Ø§Ø³Ø© Ø§Ù„ØºØ±ÙØ©ØŒ ÙˆØ§ØµÙØ§ Ø§Ù•ÙŠØ§Ù‡ Ø¨Ù€ â€œØ§Ù„Ø§Ù”Ù…ÙŠ Ø§Ù„Ø°ÙŠ Ù„Ø§ÙŠÙÙ‚Ù‡ Ø´ÙŠÙŠÙ”Ø§â€ØŒ Ù„ÙŠØ¯Ø®Ù„ Ø§Ù„Ø·Ø±ÙØ§Ù† ÙÙŠ Ù…Ù„Ø§Ø³Ù†Ø§Øª ÙƒÙ„Ø§Ù…ÙŠØ© Ù‚Ø¨Ù„ Ø§Ù”Ù† ÙŠØªØ­ÙˆÙ„ Ø§Ù„ØµØ±Ø§Ø¹ Ø§Ù•Ù„Ù‰ ØªØ´Ø§Ø¨Ùƒ Ø¨Ø§Ù„Ø§Ù”ÙŠØ¯ÙŠ. ',\n",
              " 'headline': 'Ø¨Ø±Ù„Ù…Ø§Ù†ÙŠÙŠÙ† Ù…Ù† Ø­Ø²Ø¨ Ø§Ù„Ø­Ù…Ø§Ù…Ø© Ù‚Ù„Ø¨ÙˆÙ‡Ø§ Ø¨ÙˆÙ†ÙŠØ§ Ù‚Ø¨Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø±Ø¦ÙŠØ³ ÙˆØ£Ø¹Ø¶Ø§Ø¡ ØºØ±ÙØ© Ø§Ù„ÙÙ„Ø§Ø­Ø© Ø¨Ø¬Ù‡Ø© Ù…Ø±Ø§ÙƒØ´ Ø¢Ø³ÙÙŠ (ØµÙˆØ±)',\n",
              " 'categories': \"['Ø¢Ø´ ÙˆØ§Ù‚Ø¹', 'Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©']\"}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section1: Efficiently Fine-Tune Seq2Seq Models with Low Rank Adaptation (LoRA)\n",
        "We are going to leverage Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), and [PEFT](https://github.com/huggingface/peft). \n",
        "\n",
        "You will learn how to:\n",
        "\n",
        "1. Setup Development Environment\n",
        "2. Load and prepare the dataset\n",
        "3. Fine-Tune Multilingual BERT with LoRA and bnb int-8\n",
        "4. Evaluate & run Inference\n",
        "5. Cost performance comparison\n",
        "\n",
        "### Quick intro to PEFT or Parameter Efficient Fine-tuning\n",
        "<div style=\"display: flex; justify-content: center; align-items: flex-start;\">    <figure style=\"text-align: center;\">\n",
        "        <a href=\"https://arxiv.org/abs/2303.15647#\" target=\"_blank\">\n",
        "            <img src=\"./content/PEFT_method.png\" width=\"90%\" />\n",
        "        </a>\n",
        "        <figcaption><a href=\"https://arxiv.org/abs/2303.15647#\" target=\"_blank\">PEFT Methods, from the paper \"Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning\"\n",
        "</a></figcaption>\n",
        "    </figure>\n",
        "</div>\n",
        "\n",
        "[PEFT](https://github.com/huggingface/peft), or Parameter Efficient Fine-tuning, is a new open-source library from Hugging Face to enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. PEFT currently includes techniques for:\n",
        "\n",
        "- LoRA:Â [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
        "- Prefix Tuning:Â [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
        "- P-Tuning:Â [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
        "- Prompt Tuning:Â [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Low-Rank Adaptation (LoRA)\n",
        "\n",
        "While large language models (LLMs) have shown remarkable performance across a wide range of NLP tasks, they require significant computation resources to train, fine-tune and deploy. In addition, many real-world use-cases require adapting avialable LLMs to their target task in order to achieve desired performance. \n",
        "\n",
        "While fine-tuning an entire LLM is cost prohibitive, even on small datasets. For example, fully fine-tuning the Llama7B model requires 112GB of VRAM, i.e. at least two 80GB A100 GPUs. Fortunately, parameter efficient fine-tuning methods such as LoRA allow users with meager resources to adapat an LLM to their target task efficiently and effectively.\n",
        "\n",
        "In this tutorial we explore QLoRA, which is a parameter-efficient fine-tuning technique that reduces the number of parameters fine-tuned during the adaptation process, and additionally introduces quantization to further lower the memory footprint of the adapted model. \n",
        "\n",
        "### How Does LoRA Work?\n",
        "\n",
        "The paper [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) takes inspiration from the conjecture that over-parameterized models span a low-rank intrinsic dimension. A low intrinsic dimension means the data can be effectively represented or approximated by a lower-dimensional space while retaining most of its essential information or structure. In other words, this means we can decompose the new weight matrix for the adapted task into lower-dimensional (smaller) matrices without losing significant information.\n",
        "\n",
        "Concretely, let us suppose $\\delta W$ is the weight update for an $A\\times B$ weight matrix. Then, a low-rank decompsoition of $\\delta W$ can be expressed as: $\\delta W = W_A W_B$, where $W_A$ is an $A\\times k$ matrix and $W_B$ is a $k\\times B$ matrix. Here, $k$ is the rank of the decomposition, and is typically much smaller than $A$ and $B$.\n",
        "\n",
        "![Image courtesty from Sebastian Raschka's Ligthning.AI tutorial on LoRA](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/lora-4-300x226@2x.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summarization Using MT5\n",
        "\n",
        "Prior to fine-tuning ouro model, we need to select the model we will use as our base model. In this case, we will use the [MT5](https://huggingface.co/google/mt5-small) model, which is a multilingual variant of the T5 model. The MT5 model is trained on a large multilingual corpus and is capable of performing a wide range of NLP tasks, including summarization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/alizaidi/micromamba/envs/peft/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we prepare our datasets for training. This requires tokenizing the input and output sequences, padding them to the desired length, and then converting them into PyTorch Dataset objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The maximum total input sequence length after tokenization.\n",
        "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
        "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"article\"], truncation=True), batched=True, remove_columns=[\"categories\", \"headline\"])\n",
        "input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max source length: 571\n"
          ]
        }
      ],
      "source": [
        "# take 85 percentile of max length for better utilization\n",
        "max_source_length = int(np.percentile(input_lenghts, 85))\n",
        "print(f\"Max source length: {max_source_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max target length: 50\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# The maximum total sequence length for target text after tokenization.\n",
        "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
        "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"headline\"], truncation=True), batched=True, remove_columns=[\"article\", \"categories\"])\n",
        "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
        "# take 90 percentile of max length for better utilization\n",
        "max_target_length = int(np.percentile(target_lenghts, 90))\n",
        "print(f\"Max target length: {max_target_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
          ]
        }
      ],
      "source": [
        "def preprocess_function(sample, padding=\"max_length\"):\n",
        "    # # add prefix to the input for t5\n",
        "    # inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
        "\n",
        "    # tokenize inputs\n",
        "    model_inputs = tokenizer(sample['article'], max_length=max_source_length, padding=padding, truncation=True)\n",
        "\n",
        "    # Tokenize targets with the `summary` keyword argument\n",
        "    labels = tokenizer(sample[\"headline\"], max_length=max_target_length, padding=padding, truncation=True)\n",
        "\n",
        "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
        "    # padding in the loss.\n",
        "    if padding == \"max_length\":\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"headline\", \"article\", \"categories\"])\n",
        "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9239cc1fc6694dd1b0edd5613e98d9e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/9497 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_dataset[\"test\"].save_to_disk(\"arabic-goud-data/eval\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally we need to define our configuration for LoRA. The primary parameters for LoRA are:\n",
        "\n",
        "* `r`: this is the rank of the decomposed matrices $A$ and $B$ to be learned during fine-tuning. A smaller number will save more GPU memory but might decrease performance.\n",
        "* `lora_alpha`: this is the weight of the low-rank loss in the total loss function, or the coefficient for the learned $\\Delta W$ factor. A larger number will typically result in a larger behavior change after fine-tuning.\n",
        "* `lora_dropout`: the dropout ratio for layers in the LoRA adapters $A$ and $B$.\n",
        "* `target_modules`: which modules to learn the low-rank decomposition for. This could be all linear layers, for example, or specific modules in the base network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 688,128 || all params: 300,864,896 || trainable%: 0.2287\n"
          ]
        }
      ],
      "source": [
        "lora_config = LoraConfig(\n",
        "  r=16,\n",
        "  lora_alpha=32,\n",
        "  target_modules=[\"q\", \"v\"],\n",
        "  lora_dropout=0.05,\n",
        "  bias=\"none\",\n",
        "  task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")\n",
        "# prepare int-8 model for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# add LoRA adaptor\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we want to ignore tokenizer pad token in the loss\n",
        "label_pad_token_id = -100\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=label_pad_token_id,\n",
        "    pad_to_multiple_of=8,\n",
        "    return_tensors='pt'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 10\n",
        "output_dir = \"lora-goud-mt5-small\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define training args\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"lora-mt5-goud\",\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=500,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=[\"tensorboard\", \"wandb\"],\n",
        ")\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset = tokenized_dataset[\"train\"],\n",
        "    eval_dataset = tokenized_dataset[\"validation\"].select(range(20)),\n",
        "    tokenizer = tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.7 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/alizaidi/dev/nlp/llms/indaba/indaba-low-resource-nlp-prac/wandb/run-20240816_012514-brfaca9p\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora-mt5-goud\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/alizaidi/huggingface\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/alizaidi/huggingface/runs/brfaca9p\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='174110' max='174110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [174110/174110 4:54:01, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>6.364100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>5.019700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>4.733600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>4.573900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>4.487000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>4.441000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>4.375900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>4.370600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>4.312700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>4.309900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>4.276800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>4.278400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>4.213300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>4.183900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>4.184700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>4.174400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>4.174500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>4.145700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>4.104400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>4.122800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>4.112100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>4.096600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>4.086700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>4.070700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>4.075500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>4.048700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>4.044400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>4.013700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>4.032200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>4.025100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>4.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>4.017300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>4.028800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>4.054900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>3.983700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>3.977800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>4.015500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>3.962000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>3.950300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>3.988000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>3.963800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>3.948300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>3.954600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>4.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>3.944200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>3.936600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>3.942800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>3.938300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>3.926900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>3.943500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>3.933100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>3.951000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>3.914700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>3.912500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>3.971900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>3.906500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>3.913700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>3.937000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29500</td>\n",
              "      <td>3.909400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>3.934100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30500</td>\n",
              "      <td>3.904400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>3.883700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31500</td>\n",
              "      <td>3.890200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>3.901500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32500</td>\n",
              "      <td>3.899800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>3.869100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33500</td>\n",
              "      <td>3.912900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34000</td>\n",
              "      <td>3.917400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34500</td>\n",
              "      <td>3.887100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35000</td>\n",
              "      <td>3.892100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35500</td>\n",
              "      <td>3.874800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36000</td>\n",
              "      <td>3.897100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36500</td>\n",
              "      <td>3.874900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37000</td>\n",
              "      <td>3.846300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37500</td>\n",
              "      <td>3.858400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38000</td>\n",
              "      <td>3.864700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38500</td>\n",
              "      <td>3.873700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39000</td>\n",
              "      <td>3.855400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39500</td>\n",
              "      <td>3.860700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40000</td>\n",
              "      <td>3.864000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40500</td>\n",
              "      <td>3.822500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41000</td>\n",
              "      <td>3.871400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41500</td>\n",
              "      <td>3.873800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42000</td>\n",
              "      <td>3.864700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42500</td>\n",
              "      <td>3.871200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43000</td>\n",
              "      <td>3.824300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43500</td>\n",
              "      <td>3.836300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44000</td>\n",
              "      <td>3.822100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44500</td>\n",
              "      <td>3.809800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45000</td>\n",
              "      <td>3.870400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45500</td>\n",
              "      <td>3.828100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46000</td>\n",
              "      <td>3.858900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46500</td>\n",
              "      <td>3.821300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47000</td>\n",
              "      <td>3.850200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47500</td>\n",
              "      <td>3.808800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48000</td>\n",
              "      <td>3.836400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48500</td>\n",
              "      <td>3.848400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49000</td>\n",
              "      <td>3.831800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49500</td>\n",
              "      <td>3.844700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50000</td>\n",
              "      <td>3.852400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50500</td>\n",
              "      <td>3.810800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51000</td>\n",
              "      <td>3.810000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51500</td>\n",
              "      <td>3.826900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52000</td>\n",
              "      <td>3.818600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52500</td>\n",
              "      <td>3.808900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53000</td>\n",
              "      <td>3.829400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53500</td>\n",
              "      <td>3.811100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54000</td>\n",
              "      <td>3.810500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54500</td>\n",
              "      <td>3.805000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55000</td>\n",
              "      <td>3.779800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55500</td>\n",
              "      <td>3.802900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56000</td>\n",
              "      <td>3.798900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56500</td>\n",
              "      <td>3.799100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57000</td>\n",
              "      <td>3.812200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57500</td>\n",
              "      <td>3.799300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58000</td>\n",
              "      <td>3.808400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58500</td>\n",
              "      <td>3.793900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59000</td>\n",
              "      <td>3.777100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59500</td>\n",
              "      <td>3.790800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60000</td>\n",
              "      <td>3.783400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60500</td>\n",
              "      <td>3.804400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61000</td>\n",
              "      <td>3.785300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61500</td>\n",
              "      <td>3.806300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62000</td>\n",
              "      <td>3.804200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62500</td>\n",
              "      <td>3.801200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63000</td>\n",
              "      <td>3.806000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63500</td>\n",
              "      <td>3.808200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64000</td>\n",
              "      <td>3.785500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64500</td>\n",
              "      <td>3.775000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65000</td>\n",
              "      <td>3.795900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65500</td>\n",
              "      <td>3.775200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66000</td>\n",
              "      <td>3.803400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66500</td>\n",
              "      <td>3.763200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67000</td>\n",
              "      <td>3.774300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67500</td>\n",
              "      <td>3.775800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68000</td>\n",
              "      <td>3.811800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68500</td>\n",
              "      <td>3.761400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69000</td>\n",
              "      <td>3.747300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69500</td>\n",
              "      <td>3.763600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70000</td>\n",
              "      <td>3.779900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70500</td>\n",
              "      <td>3.755300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71000</td>\n",
              "      <td>3.745700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71500</td>\n",
              "      <td>3.752600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72000</td>\n",
              "      <td>3.764000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72500</td>\n",
              "      <td>3.755300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73000</td>\n",
              "      <td>3.731200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73500</td>\n",
              "      <td>3.735600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74000</td>\n",
              "      <td>3.761100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74500</td>\n",
              "      <td>3.765700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75000</td>\n",
              "      <td>3.735500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75500</td>\n",
              "      <td>3.749600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76000</td>\n",
              "      <td>3.758200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76500</td>\n",
              "      <td>3.745700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77000</td>\n",
              "      <td>3.735900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77500</td>\n",
              "      <td>3.738800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78000</td>\n",
              "      <td>3.742200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78500</td>\n",
              "      <td>3.770900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79000</td>\n",
              "      <td>3.771400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79500</td>\n",
              "      <td>3.741600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80000</td>\n",
              "      <td>3.754100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80500</td>\n",
              "      <td>3.754300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81000</td>\n",
              "      <td>3.753500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81500</td>\n",
              "      <td>3.734200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82000</td>\n",
              "      <td>3.761200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82500</td>\n",
              "      <td>3.732100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83000</td>\n",
              "      <td>3.752900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83500</td>\n",
              "      <td>3.728700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84000</td>\n",
              "      <td>3.766300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84500</td>\n",
              "      <td>3.738100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85000</td>\n",
              "      <td>3.760300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85500</td>\n",
              "      <td>3.742100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86000</td>\n",
              "      <td>3.735100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86500</td>\n",
              "      <td>3.719900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87000</td>\n",
              "      <td>3.751800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87500</td>\n",
              "      <td>3.702000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88000</td>\n",
              "      <td>3.728800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88500</td>\n",
              "      <td>3.727000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89000</td>\n",
              "      <td>3.711700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89500</td>\n",
              "      <td>3.710300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90000</td>\n",
              "      <td>3.736100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90500</td>\n",
              "      <td>3.745400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91000</td>\n",
              "      <td>3.706500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91500</td>\n",
              "      <td>3.698400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92000</td>\n",
              "      <td>3.720200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92500</td>\n",
              "      <td>3.753100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93000</td>\n",
              "      <td>3.712200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93500</td>\n",
              "      <td>3.714400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94000</td>\n",
              "      <td>3.725300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94500</td>\n",
              "      <td>3.681700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95000</td>\n",
              "      <td>3.720100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95500</td>\n",
              "      <td>3.714700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96000</td>\n",
              "      <td>3.709200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96500</td>\n",
              "      <td>3.732100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97000</td>\n",
              "      <td>3.673100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97500</td>\n",
              "      <td>3.695300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98000</td>\n",
              "      <td>3.707700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98500</td>\n",
              "      <td>3.690500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99000</td>\n",
              "      <td>3.725000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99500</td>\n",
              "      <td>3.737600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100000</td>\n",
              "      <td>3.713200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100500</td>\n",
              "      <td>3.707300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101000</td>\n",
              "      <td>3.723000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101500</td>\n",
              "      <td>3.688400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102000</td>\n",
              "      <td>3.682600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102500</td>\n",
              "      <td>3.701200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103000</td>\n",
              "      <td>3.667900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103500</td>\n",
              "      <td>3.709200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104000</td>\n",
              "      <td>3.710600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104500</td>\n",
              "      <td>3.678600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105000</td>\n",
              "      <td>3.696300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105500</td>\n",
              "      <td>3.695500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106000</td>\n",
              "      <td>3.697400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106500</td>\n",
              "      <td>3.683600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107000</td>\n",
              "      <td>3.691600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107500</td>\n",
              "      <td>3.684500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108000</td>\n",
              "      <td>3.657400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108500</td>\n",
              "      <td>3.670300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109000</td>\n",
              "      <td>3.665800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109500</td>\n",
              "      <td>3.687900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110000</td>\n",
              "      <td>3.693500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110500</td>\n",
              "      <td>3.682900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111000</td>\n",
              "      <td>3.691300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111500</td>\n",
              "      <td>3.666200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112000</td>\n",
              "      <td>3.695800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112500</td>\n",
              "      <td>3.669500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113000</td>\n",
              "      <td>3.689800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113500</td>\n",
              "      <td>3.674900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114000</td>\n",
              "      <td>3.685900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114500</td>\n",
              "      <td>3.649200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115000</td>\n",
              "      <td>3.652300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115500</td>\n",
              "      <td>3.663400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116000</td>\n",
              "      <td>3.672100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116500</td>\n",
              "      <td>3.641500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117000</td>\n",
              "      <td>3.679800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117500</td>\n",
              "      <td>3.649500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118000</td>\n",
              "      <td>3.673100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118500</td>\n",
              "      <td>3.693000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119000</td>\n",
              "      <td>3.678800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119500</td>\n",
              "      <td>3.679000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120000</td>\n",
              "      <td>3.672200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120500</td>\n",
              "      <td>3.664500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121000</td>\n",
              "      <td>3.643500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121500</td>\n",
              "      <td>3.676300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122000</td>\n",
              "      <td>3.649900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122500</td>\n",
              "      <td>3.636700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123000</td>\n",
              "      <td>3.652600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123500</td>\n",
              "      <td>3.651200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124000</td>\n",
              "      <td>3.646900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124500</td>\n",
              "      <td>3.672800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125000</td>\n",
              "      <td>3.664500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125500</td>\n",
              "      <td>3.633600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126000</td>\n",
              "      <td>3.634400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126500</td>\n",
              "      <td>3.642600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127000</td>\n",
              "      <td>3.645700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127500</td>\n",
              "      <td>3.658100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128000</td>\n",
              "      <td>3.620000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128500</td>\n",
              "      <td>3.650900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129000</td>\n",
              "      <td>3.620500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129500</td>\n",
              "      <td>3.672300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130000</td>\n",
              "      <td>3.665000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130500</td>\n",
              "      <td>3.613000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131000</td>\n",
              "      <td>3.615800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131500</td>\n",
              "      <td>3.667500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132000</td>\n",
              "      <td>3.650500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132500</td>\n",
              "      <td>3.629600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133000</td>\n",
              "      <td>3.642500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133500</td>\n",
              "      <td>3.636100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134000</td>\n",
              "      <td>3.658000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134500</td>\n",
              "      <td>3.647200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135000</td>\n",
              "      <td>3.633500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135500</td>\n",
              "      <td>3.648700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136000</td>\n",
              "      <td>3.672900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136500</td>\n",
              "      <td>3.644700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137000</td>\n",
              "      <td>3.627500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137500</td>\n",
              "      <td>3.612800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138000</td>\n",
              "      <td>3.642700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138500</td>\n",
              "      <td>3.635900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139000</td>\n",
              "      <td>3.625600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139500</td>\n",
              "      <td>3.611400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140000</td>\n",
              "      <td>3.620800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140500</td>\n",
              "      <td>3.616100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141000</td>\n",
              "      <td>3.616600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141500</td>\n",
              "      <td>3.617400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142000</td>\n",
              "      <td>3.597200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142500</td>\n",
              "      <td>3.621700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143000</td>\n",
              "      <td>3.609700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143500</td>\n",
              "      <td>3.622200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144000</td>\n",
              "      <td>3.604300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144500</td>\n",
              "      <td>3.599500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145000</td>\n",
              "      <td>3.595100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145500</td>\n",
              "      <td>3.586200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146000</td>\n",
              "      <td>3.594400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146500</td>\n",
              "      <td>3.657200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147000</td>\n",
              "      <td>3.630300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147500</td>\n",
              "      <td>3.614800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148000</td>\n",
              "      <td>3.636200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148500</td>\n",
              "      <td>3.605800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149000</td>\n",
              "      <td>3.612300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149500</td>\n",
              "      <td>3.623400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150000</td>\n",
              "      <td>3.620400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150500</td>\n",
              "      <td>3.590200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151000</td>\n",
              "      <td>3.609600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151500</td>\n",
              "      <td>3.600400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152000</td>\n",
              "      <td>3.583000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152500</td>\n",
              "      <td>3.626200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153000</td>\n",
              "      <td>3.603100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153500</td>\n",
              "      <td>3.619900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154000</td>\n",
              "      <td>3.641200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154500</td>\n",
              "      <td>3.616100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155000</td>\n",
              "      <td>3.620600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155500</td>\n",
              "      <td>3.623900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156000</td>\n",
              "      <td>3.607400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156500</td>\n",
              "      <td>3.605200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157000</td>\n",
              "      <td>3.587200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157500</td>\n",
              "      <td>3.599800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158000</td>\n",
              "      <td>3.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158500</td>\n",
              "      <td>3.575400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159000</td>\n",
              "      <td>3.574400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159500</td>\n",
              "      <td>3.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160000</td>\n",
              "      <td>3.603000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160500</td>\n",
              "      <td>3.611400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161000</td>\n",
              "      <td>3.566600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161500</td>\n",
              "      <td>3.615100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162000</td>\n",
              "      <td>3.571300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162500</td>\n",
              "      <td>3.581900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163000</td>\n",
              "      <td>3.581500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163500</td>\n",
              "      <td>3.574000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164000</td>\n",
              "      <td>3.578700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164500</td>\n",
              "      <td>3.566000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165000</td>\n",
              "      <td>3.599700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165500</td>\n",
              "      <td>3.584300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166000</td>\n",
              "      <td>3.587500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166500</td>\n",
              "      <td>3.632200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167000</td>\n",
              "      <td>3.598800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167500</td>\n",
              "      <td>3.570100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168000</td>\n",
              "      <td>3.575700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168500</td>\n",
              "      <td>3.608400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169000</td>\n",
              "      <td>3.607100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169500</td>\n",
              "      <td>3.590300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170000</td>\n",
              "      <td>3.575900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170500</td>\n",
              "      <td>3.604300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171000</td>\n",
              "      <td>3.600400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171500</td>\n",
              "      <td>3.592300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172000</td>\n",
              "      <td>3.559900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172500</td>\n",
              "      <td>3.577000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173000</td>\n",
              "      <td>3.594700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173500</td>\n",
              "      <td>3.576800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174000</td>\n",
              "      <td>3.613900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=174110, training_loss=3.7825945418129274, metrics={'train_runtime': 17642.8655, 'train_samples_per_second': 78.949, 'train_steps_per_second': 9.869, 'total_flos': 8.31857984054231e+17, 'train_loss': 3.7825945418129274, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "peft_model_id=\"peft-lora-mt5-goud-results\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('peft-lora-mt5-goud-results/tokenizer_config.json',\n",
              " 'peft-lora-mt5-goud-results/special_tokens_map.json',\n",
              " 'peft-lora-mt5-goud-results/spiece.model',\n",
              " 'peft-lora-mt5-goud-results/added_tokens.json',\n",
              " 'peft-lora-mt5-goud-results/tokenizer.json')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.model.save_pretrained(peft_model_id)\n",
        "tokenizer.save_pretrained(peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff8860c96231496a83995df93aa5b870",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95d8f0fc17824720a34d4e75b778b1ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/2.77M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56e2bd214de94322ac57fa72945058aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b77fa9fe51ed4b2f9b73db57b14eeecb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4aec675d46d456d9826e38d3879f121",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/alizaidi/lora-mt5-goud/commit/fee52adbeda959a97d9b839e0b8f0a5f315b0713', commit_message='alizaidi/lora-mt5-goud-ar', commit_description='', oid='fee52adbeda959a97d9b839e0b8f0a5f315b0713', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.push_to_hub(\"alizaidi/lora-mt5-goud-ar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = PeftConfig.from_pretrained(peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/alizaidi/micromamba/envs/peft/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# load base LLM model and tokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  load_in_8bit=True,  device_map={\"\":0})\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ø¨Ø§Ù„ÙÙŠØ¯ÙŠÙˆ. Ø·Ø§Ù„Ø¨Ø© ÙƒÙ„ÙŠØ§Øª ÙØ§Ø³ Ø®Ø±Ø¬Ø§Øª Ù„Ù„Ø§Ø­ØªØ¬Ø§Ø¬ Ø¹Ù„Ù‰ ÙØªØ­ Ø§Ù„Ø£Ø­ÙŠØ§Ø¡ ÙˆØ§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª\n"
          ]
        }
      ],
      "source": [
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
        "\n",
        "text = dataset[\"test\"][0][\"article\"]\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=128)\n",
        "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Ø±ÙˆØ¨ÙˆØ±Ø·Ø§Ø¬.. Ø§Ù„Ø¨ÙŠØ§ØªØ© Ø¨Ø§Ù„Ù„ÙŠÙ„ Ø¹Ù„Ù‰ Ø¨Ø±Ø§ Ø´ÙƒÙ„ Ø§Ø­ØªØ¬Ø§Ø¬ÙŠ Ø¬Ø¯ÙŠØ¯ Ù„Ù„Ù…Ø·Ø§Ù„Ø¨Ø© Ø¨ÙØªØ­ Ø§Ù„Ø£Ø­ÙŠØ§Ø¡ Ø§Ù„Ø¬Ø§Ù…Ø¹ÙŠØ© Ø¨ÙØ§Ø³'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset[\"test\"][0][\"headline\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
        "    \"\"\"\n",
        "    split the dataset into smaller batches that we can process simultaneously\n",
        "    Yield successive batch-sized chunks from list_of_elements.\n",
        "    \"\"\"\n",
        "    for i in range(0, len(list_of_elements), batch_size):\n",
        "        yield list_of_elements[i : i + batch_size]\n",
        "\n",
        "\n",
        "def calculate_metric_on_test_ds(\n",
        "    dataset,\n",
        "    metric,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    batch_size=16,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    column_text=\"article\",\n",
        "    column_summary=\"highlights\",\n",
        "):\n",
        "    article_batches = list(\n",
        "        generate_batch_sized_chunks(dataset[column_text], batch_size)\n",
        "    )\n",
        "    target_batches = list(\n",
        "        generate_batch_sized_chunks(dataset[column_summary], batch_size)\n",
        "    )\n",
        "\n",
        "    for article_batch, target_batch in tqdm(\n",
        "        zip(article_batches, target_batches), total=len(article_batches)\n",
        "    ):\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            article_batch,\n",
        "            max_length=1024,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        summaries = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"].to(device),\n",
        "            attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "            length_penalty=0.8,\n",
        "            num_beams=8,\n",
        "            max_length=128,\n",
        "        )\n",
        "        \"\"\" parameter for length penalty ensures that the model does not generate sequences that are too long. \"\"\"\n",
        "\n",
        "        # Finally, we decode the generated texts,\n",
        "        # replace the  token, and add the decoded texts with the references to the metric.\n",
        "        decoded_summaries = [\n",
        "            tokenizer.decode(\n",
        "                s, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "            )\n",
        "            for s in summaries\n",
        "        ]\n",
        "\n",
        "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
        "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
        "\n",
        "    #  Finally compute and return the ROUGE scores.\n",
        "    score = metric.compute()\n",
        "    return score\n",
        "\n",
        "\n",
        "def evaluation(tokenizer, model, dataset):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # loading data\n",
        "    rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "    rouge_metric = load_metric(\"rouge\")\n",
        "    score = calculate_metric_on_test_ds(\n",
        "        dataset[\"test\"][0:10],\n",
        "        rouge_metric,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        batch_size=2,\n",
        "        column_text=\"article\",\n",
        "        column_summary=\"headline\",\n",
        "    )\n",
        "\n",
        "    rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
        "\n",
        "    return rouge_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_498243/3742215907.py:70: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  rouge_metric = load_metric(\"rouge\")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "  0%|                                                                    | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                | 1/5 [00:02<00:08,  2.03s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                    | 2/5 [00:04<00:06,  2.10s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 3/5 [00:06<00:04,  2.23s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 4/5 [00:10<00:02,  2.80s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:13<00:00,  2.80s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:13<00:00,  2.61s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'rouge1': 0.1, 'rouge2': 0.0, 'rougeL': 0.1, 'rougeLsum': 0.1}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "evaluation(tokenizer, model, dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation1: early checkpoint of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#load an early checkpoint\n",
        "#run evaluation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation2: final trained model AraBERT , DarijaBERT \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Activity: Native Language Article Summarization\n",
        "\n",
        "**Task:** Fetch an article in your native language and assess the summarization and headline generation capabilities of ChatGPT.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. **Select an Article:** Choose a relevant and recent article written in your native language. Ensure it is of short or medium length.\n",
        "\n",
        "2. **Summarize with ChatGPT:** Use ChatGPT to generate a summary of the selected article.\n",
        "\n",
        "3. **Evaluate Summarization Quality:**\n",
        "    - **Impression:** Share your impression of the summarization quality. Consider the following:\n",
        "        - **Accuracy:** Does the summary capture the main points and essence of the article?\n",
        "        - **Clarity:** Is the summary clear and easy to understand?\n",
        "        - **Coverage:** Does the summary include all critical information from the article?\n",
        "\n",
        "4. **Provide Feedback:** Offer constructive feedback on the summarization. Highlight any discrepancies or areas for improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<iframe\n",
              "\tsrc=\"https://forms.gle/sggLJWMFQ4JQCmHL8\",\n",
              "  width=\"80%\"\n",
              "\theight=\"320px\" >\n",
              "\tLoading...\n",
              "</iframe>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title Generate Quiz Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/sggLJWMFQ4JQCmHL8\",\n",
        "  width=\"80%\"\n",
        "\theight=\"320px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section2: Summarization using GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we will utilize both OpenAI's Large and Small language models to perform the same task of abstractive text summarization. We will then evaluate their performance and compare the results with those obtained from the models discussed in Section 1.\n",
        "\n",
        "Unlike traditional fine-tuning approaches that involve updating model weights, the initial step in adapting a GPT-based model for a specific task is prompt engineering, which does not require weight updates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"display: flex; align-items: flex-start;\">\n",
        "    <figure style=\"margin-right: 10px; text-align: center;\">\n",
        "        <a href=\"https://arxiv.org/pdf/2005.14165\" target=\"_blank\">\n",
        "            <img src=\"./content/traditional-finetuning.png\" width=\"80%\" />\n",
        "        </a>\n",
        "        <figcaption><a href=\"https://arxiv.org/pdf/2005.14165\" target=\"_blank\">Traditional Fine-Tuning</a></figcaption>\n",
        "    </figure>\n",
        "    <figure style= \"text-align: center;\">\n",
        "        <a href=\"https://arxiv.org/pdf/2005.14165\" target=\"_blank\">\n",
        "            <img src=\"./content/prompting.png\" width=\"80%\" />\n",
        "        </a>\n",
        "        <figcaption><a href=\"https://arxiv.org/pdf/2005.14165\" target=\"_blank\">Prompting</a></figcaption>\n",
        "    </figure>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from rouge_metric import PyRouge\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import random\n",
        "import openai\n",
        "import time\n",
        "from openai import OpenAI\n",
        "import os \n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_rouge(hypotheses, references):\n",
        "    these_refs = [[ref.strip().lower()] for ref in references]\n",
        "    rouge = PyRouge(rouge_n=(1, 2), rouge_l=True)\n",
        "    scores = rouge.evaluate(hypotheses, these_refs)\n",
        "    print(scores)\n",
        "\n",
        "def substring_after_colon(input_string):\n",
        "    colon_index = input_string.find(':')\n",
        "    if colon_index != -1:\n",
        "        return input_string[colon_index + 1:]\n",
        "    else:\n",
        "        return input_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define dataset and paths\n",
        "DATASET = \"Goud\"\n",
        "MAX_TRAIN = 0\n",
        "model_name = \"gpt-4o-mini\"\n",
        "\n",
        "output_filename = f\"./{DATASET}_{model_name}_test_generated_{MAX_TRAIN}.csv\"\n",
        "\n",
        "# Load dataset\n",
        "goud_data = dataset\n",
        "train_source = goud_data[\"train\"][\"article\"]\n",
        "train_target = goud_data[\"train\"][\"headline\"]\n",
        "test_source = goud_data[\"test\"][\"article\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function to summarize news articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_news_article(MAX_TRAIN=20):\n",
        "       \n",
        "    client = OpenAI(api_key=key)\n",
        "    rewritten_prompt_count = 0\n",
        "    line_count = 0\n",
        "    wait_time = 1\n",
        "    df_lines = []\n",
        "    tokens_consumption = 0\n",
        "    existing_len = 0\n",
        "    if os.path.exists(output_filename):\n",
        "        existing_df = pd.read_csv(output_filename)\n",
        "        existing_len = existing_df.shape[0]\n",
        "        rewritten_prompt_count = existing_len\n",
        "        line_count = existing_len\n",
        "        df_lines = existing_df.to_dict('records')\n",
        "    \n",
        "    for data in tqdm(test_source[existing_len:], desc=f\"Lines processed from {existing_len}-th line\"):\n",
        "        news_article = data.strip()\n",
        "        line_count += 1\n",
        "        made_error = True\n",
        "        num_error = 0\n",
        "        while made_error:\n",
        "            messages = [{\"role\": \"system\", \"content\": \"You are asked to summarize a news article written in Modern Standard Arabic and Moroccan Darija, and write that summary as a clickbait headline, in Moroccan Darija only.\\n\"}]\n",
        "            if MAX_TRAIN - num_error > 0:\n",
        "                for _ in range(MAX_TRAIN - num_error):\n",
        "                    idx = random.choice(range(len(train_source)))\n",
        "                    train_src = train_source[idx]\n",
        "                    train_tgt = train_target[idx]\n",
        "                    messages.append({\"role\": \"user\", \"content\": f\"Summarize the following news article into a headline in Moroccan Darija only:\\n\\\"{train_src}\\\"\"})\n",
        "                    messages.append({\"role\": \"assistant\", \"content\": f\"Absolutely! Here is the headline summarizing your news article:\\n\\\"{train_tgt}\\\"\"})\n",
        "            messages.append({\"role\": \"user\", \"content\": f\"Summarize the following news article into a headline in Moroccan Darija only:\\n\\\"{news_article}\\\"\"})\n",
        "            try:\n",
        "                response = client.chat.completions.create(\n",
        "                    messages=messages,\n",
        "                    model=model_name,\n",
        "                )\n",
        "                headline = response.choices[0].message.content\n",
        "                df_lines.append({\"article\": news_article,\"generated_headline\": headline,\"prompt_messages\":messages})\n",
        "\n",
        "                rewritten_prompt_count += 1\n",
        "                made_error = False\n",
        "            except Exception as e:\n",
        "                if isinstance(e, openai.RateLimitError):\n",
        "                    print(\"Rate limit error\")\n",
        "                    print(f\"Wait for {wait_time} seconds because all calls failed: \", flush=True)\n",
        "                    time.sleep(wait_time)\n",
        "                    wait_time *= 2\n",
        "                else:\n",
        "                    print(e)\n",
        "                    num_error += 1\n",
        "                    print(\"May be too long, reducing context to:\", MAX_TRAIN - num_error)\n",
        "            #time.sleep(1)\n",
        "    \n",
        "    df = pd.DataFrame.from_dict(df_lines)\n",
        "    df.to_csv(output_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute the summarization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Lines processed from 0-th line: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9497/9497 [2:34:22<00:00,  1.03it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 9263.82370686531 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#record cell running time\n",
        "import time\n",
        "start_time = time.time()\n",
        "summarize_news_article(0)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Generated output and evaluate ROUGE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the \"generated_responses\" folder you will find the gpt responses corresponding to 0,1,5,20 shot prompts.\n",
        "\n",
        "Evaluate the generated headline summaries by running ROUGE evaluation and add the results to the table of results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ROUGE Metric Results: 0 Shot\n",
        "\n",
        "| Metric   | Recall (r)        | Precision (p)     | F1-Score (f)      |\n",
        "|----------|-------------------|-------------------|-------------------|\n",
        "| ROUGE-1  | 0.1228            | 0.1069            | 0.1143            |\n",
        "| ROUGE-2  | 0.0282            | 0.0235            | 0.0256            |\n",
        "| ROUGE-L  | 0.1128            | 0.0980            | 0.1049            |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'rouge-1': {'r': 0.11884854492859614, 'p': 0.12977184865821972, 'f': 0.12407023545586798}, 'rouge-2': {'r': 0.0325784137233658, 'p': 0.03389945881811894, 'f': 0.03322581039836068}, 'rouge-l': {'r': 0.11128523451125509, 'p': 0.12142149826298465, 'f': 0.11613260817866634}}\n"
          ]
        }
      ],
      "source": [
        "shot_count = 0\n",
        "hypotheses = pd.read_csv(f\".\\generated_responses\\{model_name}\\Goud_{model_name}_test_generated_{str(shot_count)}.csv\", encoding = \"UTF-8\")[\"generated_headline\"].tolist()\n",
        "hypotheses = [substring_after_colon(hypo).replace(\"\\\"\", \"\").strip() for hypo in hypotheses]\n",
        "references = goud_data[\"test\"][\"headline\"]\n",
        "evaluate_rouge(hypotheses, references)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ROUGE Metric Results: 20 Shot\n",
        "Pick one or more of the files that have the previously generated N-shot GPT responses, present in the \"generated_responses\" folder, run the evaluation and then populate the table below\n",
        "\n",
        "| Metric   | Recall (r)        | Precision (p)     | F1-Score (f)      |\n",
        "|----------|-------------------|-------------------|-------------------|\n",
        "| ROUGE-1  |             |             |             |\n",
        "| ROUGE-2  |            |             |            |\n",
        "| ROUGE-L  |             |             |           |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'rouge-1': {'r': 0.13750397869020445, 'p': 0.1306205269799418, 'f': 0.13397389480280544}, 'rouge-2': {'r': 0.03387859852289136, 'p': 0.03194819653833152, 'f': 0.032885092553751126}, 'rouge-l': {'r': 0.1264128630910928, 'p': 0.11986076103165903, 'f': 0.12304965282629712}}\n"
          ]
        }
      ],
      "source": [
        "shot_count = 20\n",
        "model_name  = \"gpt4\"  #\"C:\\Users\\salamaaya\\OneDrive - Microsoft\\Desktop\\DLI\\Indaba2024-practical\\indaba-low-resource-nlp-prac\\generated_responses\\gpt4\\Goud_test_generated_5.csv\"\n",
        "hypotheses = pd.read_csv(f\".\\generated_responses\\{model_name}\\Goud_test_generated_{str(shot_count)}.csv\", encoding = \"UTF-8\")[\"generated_headline\"].tolist()\n",
        "hypotheses = [substring_after_colon(hypo).replace(\"\\\"\", \"\").strip() for hypo in hypotheses]\n",
        "references = goud_data[\"test\"][\"headline\"]\n",
        "evaluate_rouge(hypotheses, references)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Generate Quiz Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/zbJoTSz3nfYq1VrY6\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "**Summary:**\n",
        "\n",
        "[Summary of the main points/takeaways from the prac.]\n",
        "\n",
        "**Next Steps:**\n",
        "\n",
        "[Next steps for people who have completed the prac, like optional reading (e.g. blogs, papers, courses, youtube videos). This could also link to other pracs.]\n",
        "\n",
        "**Appendix:**\n",
        "\n",
        "[Anything (probably math heavy stuff) we don't have space for in the main practical sections.]\n",
        "\n",
        "**References:**\n",
        "\n",
        "[References for any content used in the notebook.]\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2022)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<iframe\n",
              "\tsrc=\"https://forms.gle/WUpRupqfhFtbLXtN6\",\n",
              "  width=\"80%\"\n",
              "\theight=\"1200px\" >\n",
              "\tLoading...\n",
              "</iframe>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/WUpRupqfhFtbLXtN6\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "WILOYJH4gCnD"
      ],
      "name": "Indaba_2022_Prac_Template.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "145833166d986a8417df3c7acb65d917d84b716b5a452e57fcacdc66f1a168c9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
